{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='top'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    " - start\n",
    " - load model information\n",
    " - [Variable_Importance](#Variable_Importance)\n",
    " - [Partial Dependence Plots](#PDP)\n",
    " - [Individual Conditional Expectation (ICE)](#ICE)\n",
    " - [Interactions](#interactions)\n",
    " - [LIME](#LIME)\n",
    " - [Shapley](#Shapley)\n",
    " \n",
    " \n",
    "Notes: \n",
    "\n",
    "Sources:\n",
    " - https://christophm.github.io/interpretable-ml-book/\n",
    "\n",
    "Copyright (C) 2019 Alan Chalk  \n",
    "Please do not distribute or publish without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start:_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import h2o\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directories and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../RawData/\"\n",
    "dirPData = \"../PData/\"\n",
    "dirPOutput = \"../POutput/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Settings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "font = {'size'   : 22}\n",
    "plt.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_minus_MAE(y_true, y_pred):\n",
    "    return -np.round(np.mean(np.abs(y_pred - y_true)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_all (use the none one-hot version)\n",
    "#df_all = pd.read_hdf(dirPData + '02_df_all.h5', 'df_all')\n",
    "f_name = dirPData + '02_df.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_all = dict_['df_all']\n",
    "\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the variables information\n",
    "f_name = dirPData + '02_vars.pickle'\n",
    "with open(f_name, \"rb\") as f:\n",
    "    dict_ = pickle.load(f)\n",
    "    \n",
    "var_dep              = dict_['var_dep']\n",
    "vars_ind_numeric     = dict_['vars_ind_numeric']\n",
    "vars_ind_categorical = dict_['vars_ind_categorical']\n",
    "vars_ind_onehot      = dict_['vars_ind_onehot']\n",
    "\n",
    "del dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_train  = df_all['fold'].isin(range(6))\n",
    "idx_val    = df_all['fold'].isin([6, 7])\n",
    "idx_design = df_all['fold'].isin(range(8))\n",
    "idx_test   = df_all['fold'].isin([8, 9])\n",
    "\n",
    "y = df_all[var_dep].values.ravel()\n",
    "y_train = y[idx_train]\n",
    "y_val = y[idx_val]\n",
    "y_design = y[idx_design]\n",
    "y_test = y[idx_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ind = vars_ind_categorical + vars_ind_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I \"hope\" that sklearn label encoder is deterministic \n",
    "# since I did not save the LE generated when we trained the model\n",
    "from sklearn import preprocessing\n",
    "\n",
    "df_all_lgb = df_all.copy()\n",
    "\n",
    "for col in vars_ind_categorical:\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df_all[col].values)\n",
    "    df_all_lgb[col] = le.transform(df_all_lgb[col].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test     = df_all[idx_test][vars_ind]\n",
    "df_X_test_lgb = df_all_lgb[idx_test][vars_ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o_df_test = h2o.H2OFrame(df_all[idx_test][vars_ind + var_dep + ['fold']],\n",
    "                         destination_frame = 'df_all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load model information**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = dirPData + 'dict_predictions.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_predictions = dict_['df_predictions']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load h2o glm\n",
    "m_1a_path = dict_['m_1a_path']\n",
    "glm_bst = h2o.load_model(path = m_1a_path)\n",
    "\n",
    "# load h2o random forest\n",
    "m_2a_path = dict_['m_2a_path']\n",
    "rf_bst = h2o.load_model(path = m_2a_path)\n",
    "\n",
    "# load h2o random forest\n",
    "m_3a_path = dict_['m_3a_path']\n",
    "xgb_bst = h2o.load_model(path = m_3a_path)\n",
    "\n",
    "# load lightgbm \n",
    "m_3b_path = dict_['m_3b_path']\n",
    "lgb_bst = lgb.Booster(model_file=dirPData + '12b_lgb_bst.txt')\n",
    "\n",
    "# load catboost and calculated feature importances\n",
    "m_3c_path = dict_['m_3c_path']\n",
    "ctb_bst = CatBoostRegressor()\n",
    "ctb_bst.load_model(m_3c_path)\n",
    "ctb_bst_feature_importances_ = dict_['m_3c_feature_importances_']\n",
    "ctb_bst_feature_names_ = dict_['m_3c_feature_names_']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Variable_Importance'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable Importance\n",
    "\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable importance measures try to answer the question - which features are most important in the trained model.  Variable importance is typically calculated in one of three ways:\n",
    " - based on fitted model parameters (e.g. the standardised coefficients of a fitted linear model)\n",
    " - based on the structure of tree based models (such as how often a variable is split on, or the improvment to the loss during training)\n",
    " - based on how much the performance of the model deteriorates if a feature is removed (by permuting it so that it becomes meaningless) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h2o glm**\n",
    "\n",
    "> **How is variable importance calculated for GLM?  For GLM, the variable importance represents the coefficient magnitudes.**\n",
    "\n",
    "This is inconsistent with other h2o model types.  If would be nice if there was an option to create variable importance for glms based on permutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_bst.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h2o random forest**\n",
    "\n",
    "h2o does not seem to do permutation based variable importance, even for trees:\n",
    "\n",
    "> Variable importance is determined by calculating the relative influence of each variable: whether that variable was selected to split on during the tree building process, and how much the squared error (over all trees) improved (decreased) as a result.\n",
    "\n",
    " - http://docs.h2o.ai/h2o/latest-stable/h2o-docs/variable-importance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bst.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**h2o xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bst.varimp_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lightgbm**\n",
    "\n",
    "What is returned?\n",
    "\n",
    "From the docs:\n",
    " - importance_type : string, optional (default=\"split\")\n",
    " - How the importance is calculated.\n",
    " - If \"split\", result contains numbers of times the feature is used in a model.\n",
    " - If \"gain\", result contains total gains of splits which use the feature.\n",
    "\n",
    "\n",
    "[AC view] Variable importance based on split seems to be very likely to be useless. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(sorted(zip(lgb_bst.feature_importance(importance_type='gain'),\n",
    "                                 lgb_bst.feature_name())), \n",
    "                      columns=['var_imp','feature'])\n",
    "\n",
    "df_var_imp['var_imp'] = df_var_imp['var_imp'] / df_var_imp['var_imp'].max()\n",
    "\n",
    "df_var_imp.sort_values(by=\"var_imp\", ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "sns.barplot(x=\"var_imp\",\n",
    "            y=\"feature\",\n",
    "            data=df_var_imp[0:10])\n",
    "plt.title('lightgbm feature importance (gain)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(sorted(zip(lgb_bst.feature_importance(importance_type='split'),\n",
    "                                 lgb_bst.feature_name())), \n",
    "                      columns=['var_imp','feature'])\n",
    "\n",
    "df_var_imp['var_imp'] = df_var_imp['var_imp'] / df_var_imp['var_imp'].max()\n",
    "\n",
    "df_var_imp.sort_values(by=\"var_imp\", ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "sns.barplot(x=\"var_imp\",\n",
    "            y=\"feature\",\n",
    "            data=df_var_imp[0:10])\n",
    "plt.title('lightgbm feature importance (split)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**catboost**\n",
    "\n",
    "The docs state:\n",
    "\n",
    "> For each feature, PredictionValuesChange shows how much on average the prediction changes if the feature value changes. The bigger the value of the importance the bigger on average is the change to the prediction value, if this feature is changed.\n",
    "\n",
    "[AC comment] However, I do not know how this is defined or calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(sorted(zip(ctb_bst_feature_importances_,\n",
    "                                 ctb_bst_feature_names_)), \n",
    "                      columns=['var_imp','feature'])\n",
    "\n",
    "df_var_imp['var_imp'] = df_var_imp['var_imp'] / df_var_imp['var_imp'].max()\n",
    "\n",
    "df_var_imp.sort_values(by=\"var_imp\", ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "sns.barplot(x=\"var_imp\",\n",
    "            y=\"feature\",\n",
    "            data=df_var_imp[0:10])\n",
    "plt.title('catboost feature importance (prediction values change)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Permutation based feature importance**\n",
    " - http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/#example-1-feature-importance-for-classifiers\n",
    " \n",
    " We use test data for this since we have trained on all train and val data and we would get incorrect results if we used it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should just be able to import this function from mlxtend but it does not seem to work - so I have just copied the latest version from github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from mlxtend.evaluate import feature_importance_permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance_permutation(X, y, predict_method,\n",
    "                                   metric, num_rounds=1, seed=None):\n",
    "    \"\"\"Feature importance imputation via permutation importance\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : NumPy array, shape = [n_samples, n_features]\n",
    "        Dataset, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "    y : NumPy array, shape = [n_samples]\n",
    "        Target values.\n",
    "    predict_method : prediction function\n",
    "        A callable function that predicts the target values\n",
    "        from X.\n",
    "    metric : str, callable\n",
    "        The metric for evaluating the feature importance through\n",
    "        permutation. By default, the strings 'accuracy' is\n",
    "        recommended for classifiers and the string 'r2' is\n",
    "        recommended for regressors. Optionally, a custom\n",
    "        scoring function (e.g., `metric=scoring_func`) that\n",
    "        accepts two arguments, y_true and y_pred, which have\n",
    "        similar shape to the `y` array.\n",
    "    num_rounds : int (default=1)\n",
    "        Number of rounds the feature columns are permuted to\n",
    "        compute the permutation importance.\n",
    "    seed : int or None (default=None)\n",
    "        Random seed for permuting the feature columns.\n",
    "    Returns\n",
    "    ---------\n",
    "    mean_importance_vals, all_importance_vals : NumPy arrays.\n",
    "      The first array, mean_importance_vals has shape [n_features, ] and\n",
    "      contains the importance values for all features.\n",
    "      The shape of the second array is [n_features, num_rounds] and contains\n",
    "      the feature importance for each repetition. If num_rounds=1,\n",
    "      it contains the same values as the first array, mean_importance_vals.\n",
    "    Examples\n",
    "    -----------\n",
    "    For usage examples, please see\n",
    "    http://rasbt.github.io/mlxtend/user_guide/evaluate/feature_importance_permutation/\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(num_rounds, int):\n",
    "        raise ValueError('num_rounds must be an integer.')\n",
    "    if num_rounds < 1:\n",
    "        raise ValueError('num_rounds must be greater than 1.')\n",
    "\n",
    "    if not (metric in ('r2', 'accuracy') or hasattr(metric, '__call__')):\n",
    "        raise ValueError('metric must be either \"r2\", \"accuracy\", '\n",
    "                         'or a function with signature func(y_true, y_pred).')\n",
    "\n",
    "    if metric == 'r2':\n",
    "        def score_func(y_true, y_pred):\n",
    "            sum_of_squares = np.sum(np.square(y_true - y_pred))\n",
    "            res_sum_of_squares = np.sum(np.square(y_true - y_true.mean()))\n",
    "            r2_score = 1. - (sum_of_squares / res_sum_of_squares)\n",
    "            return r2_score\n",
    "\n",
    "    elif metric == 'accuracy':\n",
    "        def score_func(y_true, y_pred):\n",
    "            return np.mean(y_true == y_pred)\n",
    "\n",
    "    else:\n",
    "        score_func = metric\n",
    "\n",
    "    rng = np.random.RandomState(seed)\n",
    "\n",
    "    mean_importance_vals = np.zeros(X.shape[1])\n",
    "    all_importance_vals = np.zeros((X.shape[1], num_rounds))\n",
    "\n",
    "    baseline = score_func(y, predict_method(X))\n",
    "\n",
    "    for round_idx in range(num_rounds):\n",
    "        for col_idx in range(X.shape[1]):\n",
    "            # we take a copy of the column to be shuffled \n",
    "            # - so that we can put it back later!\n",
    "            save_col = X[:, col_idx].copy()\n",
    "            rng.shuffle(X[:, col_idx])\n",
    "            new_score = score_func(y, predict_method(X))\n",
    "            # put the column back!\n",
    "            X[:, col_idx] = save_col\n",
    "            importance = baseline - new_score\n",
    "            mean_importance_vals[col_idx] += importance\n",
    "            all_importance_vals[col_idx, round_idx] = importance\n",
    "    mean_importance_vals /= num_rounds\n",
    "\n",
    "    return mean_importance_vals, all_importance_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**lightgbm permutation var imp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_vals, _ = feature_importance_permutation(\n",
    "    X=df_X_test_lgb.values,\n",
    "    y=y_test,\n",
    "    predict_method=lgb_bst.predict, \n",
    "    metric = fn_minus_MAE, # the function normally uses r^2 where bigger is better\n",
    "    num_rounds=1,\n",
    "    seed=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(sorted(zip(imp_vals,\n",
    "                                 df_X_test.columns)), \n",
    "                      columns=['var_imp','feature'])\n",
    "\n",
    "df_var_imp['var_imp'] = df_var_imp['var_imp'] / df_var_imp['var_imp'].max()\n",
    "\n",
    "df_var_imp.sort_values(by=\"var_imp\", ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "sns.barplot(x=\"var_imp\",\n",
    "            y=\"feature\",\n",
    "            data=df_var_imp[0:10])\n",
    "plt.title('lightgbm feature importance (permutation)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**catboost permutation feature importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_vals, _ = feature_importance_permutation(\n",
    "    predict_method=ctb_bst.predict, \n",
    "    X=df_X_test.values,\n",
    "    y=y_test,\n",
    "    metric=fn_minus_MAE,\n",
    "    num_rounds=1,\n",
    "    seed=2019)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_var_imp = pd.DataFrame(sorted(zip(imp_vals,\n",
    "                                 df_X_test.columns)), \n",
    "                      columns=['var_imp','feature'])\n",
    "\n",
    "df_var_imp['var_imp'] = df_var_imp['var_imp'] / df_var_imp['var_imp'].max()\n",
    "\n",
    "df_var_imp.sort_values(by=\"var_imp\", ascending=False, inplace=True)\n",
    "\n",
    "fig = plt.figure(figsize=(17, 10))\n",
    "sns.barplot(x=\"var_imp\",\n",
    "            y=\"feature\",\n",
    "            data=df_var_imp[0:10])\n",
    "plt.title('catboost feature importance (permutation)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='PDP'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Dependence Plots\n",
    "\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# needs the one hot data\n",
    "#glm_bst.partial_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDP h2o xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_bst.partial_plot(data = h2o_df_test,\n",
    "                     cols = ['overall_qual'],\n",
    "                     nbins=20,\n",
    "                     plot=True, \n",
    "                     plot_stddev = True, \n",
    "                     figsize=(9, 6), \n",
    "                     save_to_file=dirPOutput + '20_xgb_pdp_overall_qual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PDPs don't seem to be implemented natively in lightgbm or catboost (which has a \"help wanted\" on this topic).  PDPs are not that difficult to program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PDP random forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_bst.partial_plot(data = h2o_df_test,\n",
    "                     cols = ['overall_qual'],\n",
    "                     nbins=20,\n",
    "                     plot=True, \n",
    "                     plot_stddev = True,\n",
    "                     figsize=(9, 6), \n",
    "                     save_to_file=dirPOutput + '20_rf_pdp_overall_qual')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ICE'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Individual Conditional Expectation\n",
    "\n",
    "[Top](#top)\n",
    "\n",
    "Sources:\n",
    " - https://github.com/AustinRochford/PyCEbox/blob/master/notebooks/PyCEBox%20Tutorial.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pycebox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycebox.ice import ice, ice_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df_ice\n",
    "#df_ice = ice(df_X_test_lgb.sample(100), 'gr_liv_area', lgb_bst.predict, num_grid_points=200)\n",
    "#df_ice = ice(df_X_test_lgb.sample(100), 'year_built', lgb_bst.predict, num_grid_points=10)\n",
    "#df_ice.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (data_ax, ice_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(16, 6))\n",
    "data_ax.scatter(df_X_test_lgb['year_built'],\n",
    "                y_test, \n",
    "                c='k', alpha=0.5);\n",
    "data_ax.set_xlabel('year_built');\n",
    "data_ax.set_ylabel('sale price');\n",
    "data_ax.set_title('actual data');\n",
    "\n",
    "\n",
    "ice_plot(df_ice, \n",
    "         # seems to be a bug in this class so cannot use frac_to_plot < 1\n",
    "         frac_to_plot=1,\n",
    "         #c='k', \n",
    "         alpha=0.25,\n",
    "         color_by='gr_liv_area',\n",
    "         #cmap=PuOr,\n",
    "         ax=ice_ax);\n",
    "\n",
    "ice_ax.set_xlabel('year_built');\n",
    "ice_ax.set_ylabel('sale price');\n",
    "ice_ax.set_title('ICE curves');\n",
    "\n",
    "#fig.savefig(dirPOutput + '20_lgb_ice_gr_liv_area')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='interactions'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactions\n",
    "\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friedman's h-statistic. Only general implementation I could find is for sklearn (sklearn-gbmi) - it could be adapted to be more general.  catboost has something built in and maybe other specific modules do too.\n",
    "\n",
    "In general these seem to be done in two ways\n",
    " - create a statistic which measures the difference between two one way measures of feature importance and a measure of the interaction feature importance\n",
    " - extract interactions from the structure of the trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**catboost interactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import Pool\n",
    "lst_ifi = ctb_bst.get_feature_importance(\n",
    "                               data=Pool(df_X_test, cat_features=vars_ind_categorical),\n",
    "                               type='Interaction',\n",
    "                               prettified=True,\n",
    "                               thread_count=-1,\n",
    "                               verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(lst_ifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ifi = pd.DataFrame(lst_ifi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ifi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ctb_bst_feature_names_\n",
    "var_1 = df_ifi.loc[:, 0].values\n",
    "var_2 = df_ifi.loc[:, 1].values\n",
    "df_ifi['var_1'] = [ctb_bst_feature_names_[x] for x in var_1]\n",
    "df_ifi['var_2'] = [ctb_bst_feature_names_[x] for x in var_2]\n",
    "df_ifi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ice = ice(df_X_test.sample(100), 'gr_liv_area', ctb_bst.predict, num_grid_points=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (data_ax, ice_ax) = plt.subplots(ncols=2, sharex=True, sharey=True, figsize=(16, 6))\n",
    "\n",
    "data_ax.scatter(df_X_test_lgb['gr_liv_area'],\n",
    "                y_test, \n",
    "                c='k', alpha=0.5);\n",
    "data_ax.set_xlabel('gr_liv_area');\n",
    "data_ax.set_ylabel('sale price');\n",
    "data_ax.set_title('actual data');\n",
    "\n",
    "ice_plot(df_ice, \n",
    "         # seems to be a bug in this class so cannot use frac_to_plot < 1\n",
    "         frac_to_plot=1,\n",
    "         #c='k', \n",
    "         alpha=0.25,\n",
    "         color_by='bsmtfin_sf_1',\n",
    "         #cmap=PuOr,\n",
    "         ax=ice_ax);\n",
    "\n",
    "ice_ax.set_xlabel('gr_liv_area');\n",
    "ice_ax.set_ylabel('sale price');\n",
    "ice_ax.set_title('ICE curves');\n",
    "\n",
    "fig.savefig(dirPOutput + '20_cat_ice_gr_liv_area_interaction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install eli5\n",
    "#from eli5 import show_weights, show_prediction\n",
    "#show_prediction(ctb_bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LIME'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIME\n",
    "\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ind_categorical_idx = [idx for idx, e in enumerate(df_X_test.columns) if e in vars_ind_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTabularExplainer(df_X_test_lgb.values, \n",
    "                                 feature_names=df_X_test.columns, \n",
    "                                 class_names=['sale_price'], \n",
    "                                 categorical_features=vars_ind_categorical_idx, \n",
    "                                 training_labels=y_test,\n",
    "                                 feature_selection='lasso_path',\n",
    "                                 random_state=2020,\n",
    "                                 verbose=True, \n",
    "                                 mode='regression')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lgb_bst.predict(df_X_test_lgb)\n",
    "plt.scatter(y_test_pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_min = np.argmin(y_test_pred)\n",
    "idx_qtile10 = np.argwhere(np.round(y_test_pred, -3) == np.round(np.quantile(y_test_pred, 0.1),-3))[0][0]\n",
    "idx_median = np.argwhere(y_test == np.median(y_test))[0][0]\n",
    "idx_max = np.argmax(y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = idx_qtile10\n",
    "exp = explainer.explain_instance(df_X_test_lgb.values[i],\n",
    "                                 lgb_bst.predict,\n",
    "                                 num_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_median)\n",
    "i = idx_median\n",
    "exp = explainer.explain_instance(df_X_test_lgb.values[i],\n",
    "                                 lgb_bst.predict,\n",
    "                                 num_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = idx_qtile90\n",
    "exp = explainer.explain_instance(df_X_test_lgb.values[i],\n",
    "                                 lgb_bst.predict,\n",
    "                                 num_features=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp.show_in_notebook(show_table=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Shapley\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapley values\n",
    "\n",
    "[Top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "\n",
    "# load JS visualization code to notebook\n",
    "shap.initjs()\n",
    "\n",
    "# explain the model's predictions using SHAP values\n",
    "# (same syntax works for LightGBM, CatBoost, and scikit-learn models)\n",
    "explainer = shap.TreeExplainer(lgb_bst)\n",
    "shap_values = explainer.shap_values(df_X_test_lgb.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\n",
    "shap.force_plot(explainer.expected_value, \n",
    "                shap_values[idx_qtile10,:], \n",
    "                df_X_test_lgb.iloc[idx_qtile10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
