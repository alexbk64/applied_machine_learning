{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ames housing: h2o interaction example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    " - 1. Start_. Packages, directories, functions\n",
    " - 2. Data preparation\n",
    " - 3. logistic regression without interactions\n",
    " - 4. Interactions:\n",
    "    - logistic regression testing to see if one or two interactions improves performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Start_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "import h2o\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directories and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../input/\"\n",
    "dirPData =   \"../PData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_MAE(actuals, predictions):\n",
    "    return np.round(np.mean(np.abs(predictions - actuals)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load df_all (use the none one-hot version)\n",
    "#df_all = pd.read_hdf(dirPData + '02_df_all.h5', 'df_all')\n",
    "f_name = dirPData + '02_df.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_all = dict_['df_all']\n",
    "\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the variables information\n",
    "f_name = dirPData + '02_vars.pickle'\n",
    "with open(f_name, \"rb\") as f:\n",
    "    dict_ = pickle.load(f)\n",
    "    \n",
    "var_dep = dict_['var_dep']\n",
    "vars_ind_numeric     = dict_['vars_ind_numeric']\n",
    "vars_ind_categorical = dict_['vars_ind_categorical']\n",
    "vars_ind_onehot      = dict_['vars_ind_onehot']\n",
    "\n",
    "del dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train examples 1601\n",
      "number of val examples 525\n",
      "number of design examples 2126\n",
      "number of test examples 539\n"
     ]
    }
   ],
   "source": [
    "idx_train  = df_all['fold'].isin(range(6))\n",
    "idx_val    = df_all['fold'].isin([6, 7])\n",
    "idx_design = df_all['fold'].isin(range(8))\n",
    "idx_test   = df_all['fold'].isin([8, 9])\n",
    "\n",
    "print(\"number of train examples\",    np.sum(idx_train == 1))\n",
    "print(\"number of val examples\",      np.sum(idx_val == 1))\n",
    "print(\"number of design examples\",   np.sum(idx_design == 1))\n",
    "\n",
    "print(\"number of test examples\",  np.sum(idx_test == 1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop some variables**\n",
    "\n",
    "Three or four students in the class came up with amazingly good results on the week 3 assignment.  The code below is based on the submission of Alexander Rostovtsev, who dropped the following variables.  He did this after exploratory data analysis suggested that they may not be much use.  It turns out that this helps avoid overfitting.\n",
    "\n",
    "Exactly why the LASSO did not drop them, is something I would like to investigate further - especially, whether or not the \"relaxed LASSO\" or some other form of intital variable selection would work.  \n",
    "\n",
    "Alexander also experimented with different spline points to achieve an even better result - but I have not included those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_toDrop = ['x3ssn_porch','enclosed_porch','screen_porch',\n",
    "               'pool_area','misc_val','half_bath','kitchen_abvgr',\n",
    "               'fireplaces','bsmtfin_sf_2', 'low_qual_fin_sf']\n",
    "\n",
    "vars_ind_categorical = list(set(vars_ind_categorical) - set(vars_toDrop))\n",
    "vars_ind_numeric     = list(set(vars_ind_numeric) - set(vars_toDrop))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardise before preparing basis functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise features (as in standard normal distribution mean 0, sd 1)\n",
    "for var in vars_ind_numeric:\n",
    "    x = df_all[var].values\n",
    "    x -= np.mean(x, axis=0)\n",
    "    x /= np.sqrt(np.mean(x ** 2, axis=0))\n",
    "    df_all[var] = x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Prepare basis functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do this only for truly continuous variables\n",
    "# (this is not necessarily \"right\" - but just quicker to code ...)\n",
    "# using >8 made sklearn crash - but h2o is fine with it\n",
    "vars_ind_tospline = df_all[vars_ind_numeric].columns[(df_all[vars_ind_numeric].nunique() > 8)].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_tosplines(x):\n",
    "    x = x.values\n",
    "    # hack: remove zeros to avoid issues where lots of values are zero\n",
    "    x_nonzero = x[x != 0]\n",
    "    ptiles = np.percentile(x_nonzero,[5, 10, 30, 50, 70, 90, 95] )\n",
    "    df_ptiles = pd.DataFrame({var: x})\n",
    "    for idx, ptile in enumerate(ptiles):\n",
    "        df_ptiles[var + '_' + str(idx)] = np.maximum(0, x - ptiles[idx])\n",
    "    return(df_ptiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now update df_all with splines / basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars_ind_tospline:\n",
    "    df_ptiles = fn_tosplines(df_all[var])\n",
    "    df_all.drop(columns=[var], inplace=True)\n",
    "    vars_ind_numeric.remove(var)\n",
    "    df_all = pd.concat([df_all, df_ptiles], axis=1, sort=False)\n",
    "    vars_ind_numeric.extend(df_ptiles.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ind = vars_ind_categorical + vars_ind_numeric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for convenience store dependent variable as y\n",
    "y = df_all[var_dep].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_212\"; OpenJDK Runtime Environment (build 1.8.0_212-8u212-b03-0ubuntu1.18.04.1-b03); OpenJDK 64-Bit Server VM (build 25.212-b03, mixed mode)\n",
      "  Starting server from /opt/conda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmpkdkzhol4\n",
      "  JVM stdout: /tmp/tmpkdkzhol4/h2o_jovyan_started_from_python.out\n",
      "  JVM stderr: /tmp/tmpkdkzhol4/h2o_jovyan_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (1 year, 2 months and 7 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 2 months and 7 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jovyan_tyjfu2</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>3.257 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>4</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    1 year, 2 months and 7 days !!!\n",
       "H2O cluster name:           H2O_from_python_jovyan_tyjfu2\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    3.257 Gb\n",
       "H2O cluster total cores:    4\n",
       "H2O cluster allowed cores:  4\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(port=54321)\n",
    "#h2o.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "h2o_df_all = h2o.H2OFrame(df_all[vars_ind + var_dep + ['fold']],\n",
    "                          destination_frame = 'df_all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "idx_h2o_train  = h2o.H2OFrame(idx_train.astype('int').values,  \n",
    "                              destination_frame = 'idx_h2o_train')\n",
    "idx_h2o_val    = h2o.H2OFrame(idx_val.astype('int').values  ,  \n",
    "                              destination_frame = 'idx_h2o_val')\n",
    "idx_h2o_design = h2o.H2OFrame(idx_design.astype('int').values, \n",
    "                              destination_frame = 'idx_h2o_design')\n",
    "idx_h2o_test   = h2o.H2OFrame(idx_test.astype('int').values,   \n",
    "                              destination_frame = 'idx_h2o_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define upfront the h2o frames - needed for stacked ensemble\n",
    "h2o_df_design = h2o_df_all[idx_h2o_design, :]\n",
    "h2o_df_train  = h2o_df_all[idx_h2o_train, :]\n",
    "h2o_df_val    = h2o_df_all[idx_h2o_val, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.  logistic regression without interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note on lambda search**\n",
    "\n",
    "I had all kinds of trouble with this - particularly when looking at low values of alpha - and it is not the first time.  It seems to be that the range of lambdas searched is not enough and also that the early stopping option will sometimes stop at the very beginning if there is no initial improvement.  I therefore change lambda_min_ratio to something much smaller and turn early stopping off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model=H2OGeneralizedLinearEstimator(alpha=0.20, \n",
    "                                    lambda_search=True,\n",
    "                                    lambda_min_ratio=1e-8,\n",
    "                                    nlambdas=200,\n",
    "                                    nfolds=20,\n",
    "                                    early_stopping=False,\n",
    "                                    family='gaussian',\n",
    "                                    link='identity',\n",
    "                                    # we already standardised above\n",
    "                                    standardize=False,\n",
    "                                    seed=2020)\n",
    "    \n",
    "model.train(x=vars_ind, \n",
    "            y='saleprice',\n",
    "            training_frame=h2o_df_all[idx_h2o_train, :])\n",
    "\n",
    "# Predict the model on train and val\n",
    "model_pred_train = model.predict(h2o_df_all[idx_h2o_train, :])\n",
    "model_pred_val   = model.predict(h2o_df_all[idx_h2o_val, :])\n",
    "\n",
    "model_pred_train = model_pred_train.as_data_frame().values.ravel()\n",
    "model_pred_val   = model_pred_val.as_data_frame().values.ravel()\n",
    "\n",
    "# Calculate train and cal mae and mse\n",
    "mae_train = fn_MAE(y[idx_train], model_pred_train)\n",
    "mae_val   = fn_MAE(y[idx_val], model_pred_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12249.0 13620.0 1371.0\n"
     ]
    }
   ],
   "source": [
    "mae_diff = mae_val - mae_train\n",
    "print(mae_train, mae_val, mae_diff)\n",
    "#12018 13652 1634"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Choose variables to interact**\n",
    "\n",
    "We need to choose some variables to test for interactions. How can we do this??  Well in week 6 we will discover a way to do this automatically, but that is not required for Part 1 of the  assignment.  All we need to do is to try a few interactions.\n",
    "\n",
    "If we knew what the variables mean (which you do not for this assigment) we could use our own thought process - for example maybe in Ames, the increase of price with gr_living_area will depend on the neighborhood (I think this is quite likely)\n",
    "\n",
    "\n",
    "So for the assignment - some ideas:\n",
    " - Simply try a few variables at random\n",
    " - Look at the features with largest coefficients in the model - as these are important.  If they do interact with other variables, it could be a useful addition to the model.  Ofcourse, just because they have large (standardised) coefficients, does not mean they will interact.  But I have seen people take the 5 or 10 features with largest coefficients and then try all possible combinations of them (55), for interactions.  This could be done in a for loop - using train-val.\n",
    " \n",
    "Part 1 of the assignment is not meant to be hard, if you just try a few interactions and leave some comments in your function on your results - i.e. \n",
    " - does a model fitted with them have a lower validation error\n",
    " - did you include them in your final model\n",
    "that will be sufficient\n",
    "\n",
    "See the example below where I test the ['bsmtfin_sf_1', 'gr_liv_area'] interaction.\n",
    "\n",
    "If you did want to see the largest few coefficients, you can get them with (if you en model name is \"model\"):\n",
    "     > model.std_coef_plot(num_of_features=10)\n",
    "     \n",
    "     \n",
    "Below we use the argument interactions.  This creates all possible two way interactions of whatever features you include.  If you wanted to define specific interactions pairs you can use the interaction_pairs argument:\n",
    "\n",
    " > interaction_pairs = [(\"CRSDepTime\", \"UniqueCarrier\"),\n",
    "                     (\"CRSDepTime\", \"Origin\"),\n",
    "                     (\"UniqueCarrier\", \"Origin\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bsmtfin_sf_1 \n",
    "#neighborhood\n",
    "#gr_liv_area\n",
    "#model.varimp()\n",
    "\n",
    "model=H2OGeneralizedLinearEstimator(alpha=0.20, \n",
    "                                    lambda_search=True,\n",
    "                                    lambda_min_ratio=1e-8,\n",
    "                                    nlambdas=200,\n",
    "                                    nfolds=20,\n",
    "                                    early_stopping=False,\n",
    "                                    family='gaussian',\n",
    "                                    link='identity',\n",
    "                                    interactions=['neighborhood', 'gr_liv_area'],\n",
    "                                    # we already standardised above\n",
    "                                    standardize=False,\n",
    "                                    seed=2020)\n",
    "    \n",
    "model.train(x=vars_ind, \n",
    "            y='saleprice',\n",
    "            training_frame=h2o_df_all[idx_h2o_train, :])\n",
    "\n",
    "# Predict the model on train and val\n",
    "model_pred_train = model.predict(h2o_df_all[idx_h2o_train, :])\n",
    "model_pred_val   = model.predict(h2o_df_all[idx_h2o_val, :])\n",
    "\n",
    "model_pred_train = model_pred_train.as_data_frame().values.ravel()\n",
    "model_pred_val   = model_pred_val.as_data_frame().values.ravel()\n",
    "\n",
    "# Calculate train and cal mae and mse\n",
    "mae_train = fn_MAE(y[idx_train], model_pred_train)\n",
    "mae_val   = fn_MAE(y[idx_val], model_pred_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_diff = mae_val - mae_train\n",
    "print(mae_train, mae_val, mae_diff)\n",
    "\n",
    "# I could not get this result to be replicable\n",
    "#12115 13415 1300\n",
    "#12411 13912 1501\n",
    "#11716 13403 1687"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Can we see the interactions?**\n",
    "\n",
    "Given that we fitted the interaction - can we see what h2o did?\n",
    "\n",
    "Luckily, one of  interaction terms created appears as the largest standardised coefficient.\n",
    "\n",
    "In the plot below you can see that the in the neighbourhood Stonebridge, saleprice has a different relationship with living area than in other areas.  If we were looking at this properly, we would now plot two saleprice vs living area charts, one for Stonebridge and one for all other areas, and we would look to see if the relationship appeared different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.std_coef_plot(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**\n",
    "\n",
    "Starting point: \n",
    " - validation error:  13,652  overfitting: 1,635\n",
    "\n",
    "Interaction ['neighborhood', 'gr_liv_area']:\n",
    " - validation error:  13,403 overfitting: 1,687\n",
    "\n",
    "Adding this interaction to the model does not consistently improve performance on validation data.  In the run shown here it does.  But different runs gave different answers and I could not consistently get this improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test error of the final model\n",
    "\n",
    "Please note - many of you used the test data repeatedly in the week 3 assignment.  I can't blame you given what I was asking for.  However, this means we have just fitted our model to our test data and once again, we have no idea of how it will generalise to data (on houses) it has never seen.  \n",
    "\n",
    "I do try to fit models on train-val and only occassionaly look at test.\n",
    "\n",
    "As previously discussed, this cannot be done on Kaggle - since even if you repeatedly submit, you are not seeing all the test data, since the public leaderboard reports only on 50% of the test data.  Last year (on a much smaller dataset) one team came second, right up to the last day, but when the final scores we released they dropped far down - they had overfitted to the public leaderboard.  Actually you will probably not be able to do this - because the data is much bigger this year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**design-test performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=H2OGeneralizedLinearEstimator(alpha=0.20, \n",
    "                                    lambda_search=True,\n",
    "                                    lambda_min_ratio=1e-7,\n",
    "                                    nlambdas=150,\n",
    "                                    nfolds=20,\n",
    "                                    early_stopping=False,\n",
    "                                    family='gaussian',\n",
    "                                    link='identity',\n",
    "                                    # we already standardised above\n",
    "                                    standardize=False,\n",
    "                                    seed=2020)\n",
    "    \n",
    "model.train(x=vars_ind, \n",
    "            y='saleprice',\n",
    "            training_frame=h2o_df_all[idx_h2o_design, :])\n",
    "\n",
    "# Predict the model on train and val\n",
    "model_pred_design = model.predict(h2o_df_all[idx_h2o_design, :])\n",
    "model_pred_test   = model.predict(h2o_df_all[idx_h2o_test, :])\n",
    "\n",
    "model_pred_design = model_pred_design.as_data_frame().values.ravel()\n",
    "model_pred_test   = model_pred_test.as_data_frame().values.ravel()\n",
    "\n",
    "# Calculate train and cal mae and mse\n",
    "mae_design = fn_MAE(y[idx_design], model_pred_design)\n",
    "mae_test   = fn_MAE(y[idx_test], model_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_diff = mae_test - mae_design\n",
    "print(mae_design, mae_test, mae_diff)\n",
    "\n",
    "# 12853 13312 459"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
