{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NQdisWkfQKua"
   },
   "source": [
    "# 01a ReadData_Discussion\n",
    "\n",
    "Contents\n",
    " - Import packages\n",
    " - Review of memory requirements and issues\n",
    " \n",
    "Notes\n",
    " - \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import packages and set directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Me0jS9aqQKuc"
   },
   "outputs": [],
   "source": [
    "# system commands\n",
    "import os\n",
    "\n",
    "# load and save data\n",
    "import pickle\n",
    "\n",
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# graphs\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 692,
     "status": "ok",
     "timestamp": 1562668971599,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "u_Yfp7j9QKug",
    "outputId": "cba319aa-f0aa-41a2-9049-fc6ce9300d07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.16.4\n",
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)\n",
    "print(pd.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 512,
     "status": "ok",
     "timestamp": 1562668973607,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "8o7pMYRMQKun",
    "outputId": "d1ea90cb-0ec9-45fd-f5e3-1b9a77b53e94"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Projects/final_assignment/PCode\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../input/\"\n",
    "dirPData   = \"../PData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Subsets of data**\n",
    "\n",
    "Even if we can store the whole data (see the details below), copying it to h2o and then running various models seems to take something like 16 gig. Also full runs can take a long time, so it is better to run initial experiments on a subset of the data. \n",
    "\n",
    "Therefore subsets with 250k records (train_250k) and 1m records (train_1m) were previously created (01a ReadData_Downsample_Test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Memory issues**\n",
    "\n",
    "When running Docker on mac, the default maximum memory is set to 2gig.  This can be seen (and changed) in the Docker Dashboard.  The size of the Pandas dataframe required to stored the data when using default pd.read_csv is around 1.5 gig, BUT the memory usage whilst reading this in, peaks at close to 4 gig.  This can be seen by running the command \"docker stats\" from the terminal command line and watching as the data is read in.\n",
    "\n",
    "Without enough memory available to your container you will just get an error - something like \"kernel died and needs to restart\", or something like that.  The error will not make it clear that you ran out of memory.  \n",
    "\n",
    "So I began to look at memory usage of the data frame.  I should mention that at this point, I had missings in many of the numerics.\n",
    "\n",
    "Having read in the data, I then looked at the dtypes.  Strings were stored as objects.  Numbers were stored as float64 or int64.  Both of these are much larger than needed.  I therefore printed the dict of dtypes, using \"df_train.dtypes.to_dict()\" and manually changed the dtypes as necessary.\n",
    "\n",
    "Changing just dtype Object to category reduced the memory of df_train to 1.5 gig to 0.831 gig.  However, this doing this is problematic since, the creation of categorical separately for test data could create different categoricals.\n",
    "\n",
    "Most of the integers have a range -127 to 127 and so can be stored with int8.  a04 goes up to 240 and so can be stored with a uint8 or int16.  Unfortunately (as discussed at the start of Week 4), NA's will cause int's to be upcast to floats - since NA is a float.  Downcasting those we can (because there are no missings) to int8, takes  us down to 0.5 gig. At this stage I went back to the original data and replaced all missing numbers with -99 (having checked that this is not otherwise in those fields).\n",
    "\n",
    "After this - size is 0.203 gig.  (Using object type instead of category, size is 0.77 gig)\n",
    "\n",
    "Datasets here are smaller since limited to 250k or 1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_cat = {\n",
    " 'target': 'int8',\n",
    " 'a01': \"category\",\n",
    " 'a02': \"category\",\n",
    " 'a03': \"category\",\n",
    " 'a10': \"category\",\n",
    " 'a11': \"int8\",\n",
    " 'a12': \"category\",\n",
    " 'a13': \"category\",\n",
    " 'a16': \"category\",\n",
    " 'a17': \"category\",\n",
    " 'a18': \"category\",\n",
    " 'a19': \"category\",\n",
    " 'a20': \"category\",\n",
    " 'b02': \"category\",\n",
    " 'b03': \"category\",\n",
    " 'b04': \"category\",\n",
    " 'b07': \"category\",\n",
    " 'c02': \"category\",\n",
    " 'c04': \"category\",\n",
    " 'c05': \"category\",\n",
    " 'c06': \"category\",\n",
    " 'c07': \"category\",\n",
    " 'c08': \"category\",\n",
    " 'c09': \"category\",\n",
    " 'e01': \"category\",\n",
    " 'e03': \"category\",\n",
    " 'e11': \"category\",\n",
    " 'e13': \"category\",\n",
    " 'e14': \"category\",\n",
    " 'e21': \"category\",\n",
    " 'e22': \"category\",\n",
    " 'e24': \"category\",\n",
    " 'e25': \"category\",\n",
    " 'e17': \"category\",\n",
    " 'e18': \"category\",\n",
    " 'e19': \"category\",\n",
    " 'e20': \"category\",\n",
    " 'f03': \"category\",\n",
    " 'f04': \"category\",\n",
    " 'f05': \"category\",\n",
    " 'f07': \"category\",\n",
    " 'f09': \"category\",\n",
    " 'f27': \"category\",\n",
    " 'f29': \"category\",\n",
    " 'f30': \"category\",\n",
    " 'f33': \"category\",\n",
    " 'f34': \"category\",\n",
    " 'f10': \"category\",\n",
    " 'a04': 'int16',\n",
    " 'a05': 'int8',\n",
    " 'a06': 'int8',\n",
    " 'a07': 'int8',\n",
    " 'a08': 'int8',\n",
    " 'a09': 'int8',\n",
    " 'a14': 'int8',\n",
    " 'a15': 'int8',\n",
    " 'b01': 'int8',\n",
    " 'b05': 'int8',\n",
    " 'b06': 'int8',\n",
    " 'c01': 'int8',\n",
    " 'c03': 'int8',\n",
    " 'd01': 'int8',\n",
    " 'd02': 'int8',\n",
    " 'd03': 'int8',\n",
    " 'e02': 'int8',\n",
    " 'e04': 'int8',\n",
    " 'e05': 'int8',\n",
    " 'e06': 'int8',\n",
    " 'e07': 'int8',\n",
    " 'e08': 'int8',\n",
    " 'e09': 'int8',\n",
    " 'e12': 'int8',\n",
    " 'e15': 'int8',\n",
    " 'e16': 'int8',\n",
    " 'e23': 'int8',\n",
    " 'f01': 'int8',\n",
    " 'f02': 'int8',\n",
    " 'f06': 'int8',\n",
    " 'f08': 'int8',\n",
    " 'f11': 'int8',\n",
    " 'f13': 'int8',\n",
    " 'f15': 'int8',\n",
    " 'f16': 'int8',\n",
    " 'f17': 'int8',\n",
    " 'f18': 'int8',\n",
    " 'f19': 'int8',\n",
    " 'f20': 'int8',\n",
    " 'f21': 'int8',\n",
    " 'f22': 'int8',\n",
    " 'f23': 'int8',\n",
    " 'f24': 'int8',\n",
    " 'f25': 'int8',\n",
    " 'f26': 'int8',\n",
    " 'f28': 'int8',\n",
    " 'f31': 'int8',\n",
    " 'f32': 'int8',\n",
    " 'unique_id': 'int64'}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtypes_object = {\n",
    " 'a01': \"O\",\n",
    " 'a02': \"O\",\n",
    " 'a03': \"O\",\n",
    " 'a10': \"O\",\n",
    " 'a11': \"int8\",\n",
    " 'a12': \"O\",\n",
    " 'a13': \"O\",\n",
    " 'a16': \"O\",\n",
    " 'a17': \"O\",\n",
    " 'a18': \"O\",\n",
    " 'a19': \"O\",\n",
    " 'a20': \"O\",\n",
    " 'b02': \"O\",\n",
    " 'b03': \"O\",\n",
    " 'b04': \"O\",\n",
    " 'b07': \"O\",\n",
    " 'c02': \"O\",\n",
    " 'c04': \"O\",\n",
    " 'c05': \"O\",\n",
    " 'c06': \"O\",\n",
    " 'c07': \"O\",\n",
    " 'c08': \"O\",\n",
    " 'c09': \"O\",\n",
    " 'e01': \"O\",\n",
    " 'e03': \"O\",\n",
    " 'e11': \"O\",\n",
    " 'e13': \"O\",\n",
    " 'e14': \"O\",\n",
    " 'e21': \"O\",\n",
    " 'e22': \"O\",\n",
    " 'e24': \"O\",\n",
    " 'e25': \"O\",\n",
    " 'e17': \"O\",\n",
    " 'e18': \"O\",\n",
    " 'e19': \"O\",\n",
    " 'e20': \"O\",\n",
    " 'f03': \"O\",\n",
    " 'f04': \"O\",\n",
    " 'f05': \"O\",\n",
    " 'f07': \"O\",\n",
    " 'f09': \"O\",\n",
    " 'f27': \"O\",\n",
    " 'f29': \"O\",\n",
    " 'f30': \"O\",\n",
    " 'f33': \"O\",\n",
    " 'f34': \"O\",\n",
    " 'f10': \"O\",\n",
    " 'a04': 'int16',\n",
    " 'a05': 'int8',\n",
    " 'a06': 'int8',\n",
    " 'a07': 'int8',\n",
    " 'a08': 'int8',\n",
    " 'a09': 'int8',\n",
    " 'a14': 'int8',\n",
    " 'a15': 'int8',\n",
    " 'b01': 'int8',\n",
    " 'b05': 'int8',\n",
    " 'b06': 'int8',\n",
    " 'c01': 'int8',\n",
    " 'c03': 'int8',\n",
    " 'd01': 'int8',\n",
    " 'd02': 'int8',\n",
    " 'd03': 'int8',\n",
    " 'e02': 'int8',\n",
    " 'e04': 'int8',\n",
    " 'e05': 'int8',\n",
    " 'e06': 'int8',\n",
    " 'e07': 'int8',\n",
    " 'e08': 'int8',\n",
    " 'e09': 'int8',\n",
    " 'e12': 'int8',\n",
    " 'e15': 'int8',\n",
    " 'e16': 'int8',\n",
    " 'e23': 'int8',\n",
    " 'f01': 'int8',\n",
    " 'f02': 'int8',\n",
    " 'f06': 'int8',\n",
    " 'f08': 'int8',\n",
    " 'f11': 'int8',\n",
    " 'f13': 'int8',\n",
    " 'f15': 'int8',\n",
    " 'f16': 'int8',\n",
    " 'f17': 'int8',\n",
    " 'f18': 'int8',\n",
    " 'f19': 'int8',\n",
    " 'f20': 'int8',\n",
    " 'f21': 'int8',\n",
    " 'f22': 'int8',\n",
    " 'f23': 'int8',\n",
    " 'f24': 'int8',\n",
    " 'f25': 'int8',\n",
    " 'f26': 'int8',\n",
    " 'f28': 'int8',\n",
    " 'f31': 'int8',\n",
    " 'f32': 'int8',\n",
    " 'unique_id': 'int64'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Eg8cGx3UQKur"
   },
   "outputs": [],
   "source": [
    "df_train_250k = pd.read_csv(dirRawData + 'train_250k.csv',\n",
    "                       na_values = 'NA',\n",
    "                       dtype=dtypes_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_250k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_250k.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1m = pd.read_csv(dirRawData + 'train_1m.csv',\n",
    "                          na_values = 'NA',\n",
    "                          dtype=dtypes_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_1m.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much memory is df_train using?\n",
    "mem_used = df_train_250k.memory_usage().sum() / (2<<29)\n",
    "print(\"df_train_250k is using {:.3f} gig\".format(mem_used))\n",
    "\n",
    "mem_used = df_train_1m.memory_usage().sum() / (2<<29)\n",
    "print(\"df_train_1m is using {:.3f} gig\".format(mem_used))\n",
    "\n",
    "del mem_used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('df_train_250k: ', df_train_250k.shape)\n",
    "print('df_train_1m: ', df_train_1m.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 346
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 949,
     "status": "ok",
     "timestamp": 1562668994111,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "by7yabbAQKu2",
    "outputId": "bb24b223-d4eb-4c1e-9c77-90527384d8ec"
   },
   "outputs": [],
   "source": [
    "df_train_250k.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j8AMlER_VMrl"
   },
   "source": [
    "**Checking the cardinality**\n",
    "\n",
    "For my own work, I decided to start of with anything above cardinality of 30 to be treated as an hccv. This is not necessarily optimal and you should feel to ingnore this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1562669002673,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "00Zk0pMKQKu7",
    "outputId": "befda2d9-a124-4803-d27d-21f1d1dfe71b"
   },
   "outputs": [],
   "source": [
    "cols_numeric = list(df_train_1m.select_dtypes(include=[np.number]).columns.values)\n",
    "cols_notNumeric = [var for var in df_train_1m.columns if var not in cols_numeric]\n",
    "srs_card = df_train_1m[cols_notNumeric].nunique()\n",
    "print(srs_card.min())\n",
    "print(srs_card.max())\n",
    "srs_card[srs_card>30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_ind_hccv = ['e17', 'e18', 'e19', 'f10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read in the test data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving disk space by reading in strings as factors is great - but how do we get the test data now to have the same factor levels?\n",
    "\n",
    "I will read in factors as objects, and then manually convert each object to a factor with the same index for levels and the categories in df_train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(dirRawData + 'test_k1.csv',\n",
    "                      na_values = 'NA',\n",
    "                      dtype=dtypes_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the categorical vars - this includes the hccv\n",
    "vars_ind_categorical = list(df_train_1m.columns[df_train_1m.dtypes == 'category'])\n",
    "\n",
    "# Check how the index of the levels of factors is stored, and print them out\n",
    "for var in vars_ind_categorical:\n",
    "    print(df_train_1m[var].cat.categories)\n",
    "    \n",
    "# In this version notebook ,the first printed line shows the issue, code C is last\n",
    "# in the index:\n",
    "# Index(['A', 'B', 'D', 'F', 'G', 'H', 'C'], dtype='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now for each relevant variable in df_test, replace it with a categorical\n",
    "for var in vars_ind_categorical:\n",
    "    var_levels = df_train_1m[var].cat.categories.values\n",
    "    df_test[var] = pd.Categorical(df_test[var], categories=var_levels, ordered=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that the index for a01 is stored in the same order as in df_train.  It is.\n",
    "df_test['a01'].cat.categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A0b8oBZWQKvB"
   },
   "source": [
    "**Define variable lists**\n",
    "\n",
    " - var_dep: the target feature\n",
    " - vars_notToUse:  the variables that will not be included in the experiments\n",
    " - vars_ind: the independent variables\n",
    " - vars_ind_numeric: the numeric independent variables\n",
    " - vars_ind_hccv: Three high cardinality categorical variables\n",
    " - vars_ind_categorical: the categorical independent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ozlPX04mQKvC"
   },
   "outputs": [],
   "source": [
    "vars_all = df_train_1m.columns.values\n",
    "var_dep = ['target']\n",
    "\n",
    "vars_notToUse = ['id'] \n",
    "vars_ind = [var for var in vars_all if var not in (vars_notToUse + var_dep)]\n",
    "\n",
    "vars_ind_numeric = [var for var in vars_ind if var not in vars_ind_categorical]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 497,
     "status": "ok",
     "timestamp": 1562669040577,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "2nK6P-wTQKvF",
    "outputId": "8c6abf8c-8c86-457d-8f19-86595fbb65f5"
   },
   "outputs": [],
   "source": [
    "srs_missing = df_train_1m.isnull().sum(axis=0)\n",
    "srs_missing[srs_missing > 0] \n",
    "# there are no missing data - but REMEMBER, numeric value -99 is missing!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jvmeLdr1bWt-"
   },
   "source": [
    "**Correlation of the numerical variables**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l1D-9ldcvp0k"
   },
   "source": [
    "Most of the numerical variables do not have high correlation between each other. However, there are some set of variables e.g. 'v_num_98' with 'v_num_79' which have high correlation (white squares)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5036,
     "status": "ok",
     "timestamp": 1562669456633,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "ytHM3lSXbV9c",
    "outputId": "e7fd9315-054b-424b-fc7e-6a9eca569260"
   },
   "outputs": [],
   "source": [
    "corr = df_train_1m.loc[1:10000, vars_ind_numeric].corr()\n",
    "ax = plt.subplots(figsize=(22, 22))\n",
    "sns.heatmap(corr, \n",
    "        xticklabels=corr.columns,\n",
    "        yticklabels=corr.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz01-tJSbdpv"
   },
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried two ways of storing the data.  hd5 file stores and pickle.\n",
    "\n",
    "We have already been using pickle, a traditional Python way to store any object.  Objects are convered to a stream of bytes and sent to a file.\n",
    "\n",
    "hd5 filestores:  See https://www.pythonforthelab.com/blog/how-to-use-hdf5-files-in-python/\n",
    "\n",
    "It is efficient for storing homoegeneous blocks of data.  It allows retrieval of specific bits of the data stored in it - unlike pickle where you have to load the whole object even if you only want part of the data.  This means that hd5 is very good for storing huge files.\n",
    "\n",
    "By the way, our data table is not homogeneous (since it has categories and integers).  Also the categories are not a type it is comfortable with.  So we should not expect good performance. \n",
    "\n",
    "The pickle output is smaller (259m vs 295m) and took much less time to store (1.69s vs 7.95s).\n",
    "\n",
    "For the moment I will continue to use pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#store = pd.HDFStore(dirPData + '01_df_o.h5')\n",
    "#df_train.to_hdf(store, 'df_train', format=\"table\")\n",
    "#df_test.to_hdf(store, 'df_test', format=\"table\")\n",
    "#store.close()\n",
    "\n",
    "#CPU times: user 5.37 s, sys: 971 ms, total: 6.34 s\n",
    "#Wall time: 7.95 s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_ = {'df_train': df_train_1m,\n",
    "         'df_test': df_test}\n",
    "\n",
    "f_name = dirPData + '01_df_1m.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(dict_, f)\n",
    "del f_name, dict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "dict_ = {'df_train': df_train_250k,\n",
    "         'df_test': df_test}\n",
    "\n",
    "f_name = dirPData + '01_df_250k.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(dict_, f)\n",
    "del f_name, dict_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XhNo40KqQKvb"
   },
   "outputs": [],
   "source": [
    "# store variable names to pickle\n",
    "dict_ = {'vars_ind_numeric': vars_ind_numeric,\n",
    "         'vars_ind_categorical': vars_ind_categorical,\n",
    "         'vars_ind_hccv': vars_ind_hccv,\n",
    "         'vars_notToUse': vars_notToUse,\n",
    "         'var_dep': var_dep}\n",
    "\n",
    "f_name = dirPData + '01_vars.pickle'\n",
    "with open(f_name, \"wb\") as f:\n",
    "    pickle.dump(dict_, f)\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yTghNE3RQKvi"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "*BriefEDA.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
