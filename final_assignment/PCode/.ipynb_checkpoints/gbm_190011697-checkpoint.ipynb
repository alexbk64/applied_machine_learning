{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TNQvOgFJPwMR"
   },
   "source": [
    "# Final assignment: Part 2\n",
    "- xgboost: train: 0.899, test 0.868"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aDp4OfVkp5Ec"
   },
   "source": [
    "**Import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Jnfy7xrPwMT"
   },
   "outputs": [],
   "source": [
    "### Import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import h2o\n",
    "from h2o.estimators import H2OXGBoostEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch\n",
    "\n",
    "#category encoders\n",
    "from category_encoders import LeaveOneOutEncoder\n",
    "\n",
    "#needed for fn_computeRatiosOfNumerics()\n",
    "from itertools import permutations\n",
    "\n",
    "#in order to compute AUCROC via scikitlearn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "#stops the output of warnings when running models on test data which have different factor levels for categorical \n",
    "#data than on the train data. I am aware this is not best practice, but makes the output more readable\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#plotting\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 542,
     "status": "ok",
     "timestamp": 1562674400140,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "lJ98I6jPPwMY",
    "outputId": "cb69b446-ac40-42c5-fdc9-29d67fc42c74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Projects/final_assignment/PCode\n"
     ]
    }
   ],
   "source": [
    "#### Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../input/\"\n",
    "dirPData = \"../PData/\"\n",
    "dirPOutput = \"../POutput/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Functions\n",
    "def fn_MAE(actuals, predictions):\n",
    "    return (np.mean(np.abs(predictions - actuals)))\n",
    "\n",
    "def fn_RMSE(actuals, predictions):\n",
    "    return (np.sqrt(np.mean((predictions - actuals)**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F3jUBL1RPwMf"
   },
   "outputs": [],
   "source": [
    "#### Load data via pickle\n",
    "f_name = dirPData + '01_df_250k.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_train = dict_['df_train']\n",
    "df_test  = dict_['df_test']\n",
    "\n",
    "del f_name, dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DEFINE VARIABLES\n",
    "vars_all = df_train.columns.values\n",
    "var_dep = ['target']\n",
    "\n",
    "vars_notToUse = ['unique_id'] \n",
    "vars_ind = [var for var in vars_all if var not in (vars_notToUse + var_dep)]\n",
    "\n",
    "\n",
    "# find the categorical vars - this includes the hccv\n",
    "vars_ind_categorical = list(df_train.columns[df_train.dtypes == 'category'])\n",
    "vars_ind_numeric = [var for var in vars_ind if var not in vars_ind_categorical]\n",
    "\n",
    "## GET HCCV VARS\n",
    "## Note: I experimented with changing the threshold below, but it doesn't really affect results much.\n",
    "## For ex. setting it to 15 only includes an additional 3 variables, and the resulting val AUC changes \n",
    "## from 0.873212 (th_card = 30) to 0.873174 (th_card = 30) (which is in fact marginally worse). I leave it at 30.\n",
    "th_card = 30\n",
    "srs_card = df_train[vars_ind_categorical].nunique()\n",
    "vars_ind_hccv = srs_card[srs_card>th_card].index.values.tolist()  #stores names of categorical variables with cardinality higher than threshold\n",
    "\n",
    "# for convenience store dependent variable as y\n",
    "y = df_train[var_dep].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set index for train, val, design, test data\n",
    "#### Create folds to seperate train data into train, val, design, test\n",
    "rng = np.random.RandomState(2020)\n",
    "fold = rng.randint(0, 10, df_train.shape[0])\n",
    "df_train['fold'] = fold\n",
    "\n",
    "#get indices for each subset\n",
    "idx_train  = df_train['fold'].isin(range(8))\n",
    "idx_val    = df_train['fold'].isin([7, 8])\n",
    "idx_design = df_train['fold'].isin(range(9))\n",
    "\n",
    "#drop fold column\n",
    "df_train.drop(columns='fold', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train[vars_ind_numeric].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Start and connect the H2O JVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 . connected.\n",
      "Warning: Your H2O cluster version is too old (1 year, 2 months and 9 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>18 mins 21 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 2 months and 9 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jovyan_c5l5bg</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>12.47 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>16</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>locked, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://localhost:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         18 mins 21 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    1 year, 2 months and 9 days !!!\n",
       "H2O cluster name:           H2O_from_python_jovyan_c5l5bg\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    12.47 Gb\n",
       "H2O cluster total cores:    16\n",
       "H2O cluster allowed cores:  16\n",
       "H2O cluster status:         locked, healthy\n",
       "H2O connection url:         http://localhost:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ### Connect to H2O cluster\n",
    "h2o.init(port=54321) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [key]\n",
       "Index: []"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.ls() #check if h2o cluster is empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simple xgboost model: no data manipulation**\n",
    "- The purpose of running this model is to get some idea of the basic performance on the unaltered dataset, and to see the effect of allowing H2O to treat missings.\n",
    "- The only data manipulation performed is to turn the -99 values back into NaNs so that H2O knows they are missing. I will then manually meanimpute the missings and compare the performance of the same model with the same parameters, but on the manipulated data.\n",
    "- I will then use leave-one-out encoding to deal with HCCVs and again note the difference in performance. \n",
    "- <b>A note on splines and interactions</b>: I am not creating splines nor interactions here due to the nature of tree-based algorithms, where these non-linear relationships are already captured. I found a good explanation of the reason for this at: https://stats.stackexchange.com/questions/157665/including-interaction-terms-in-random-forest?rq=1. The main idea is that tree-based models consider variables sequentially, and therefore interactions that are useful for prediction will be picked up (assuming a large enough forest).\n",
    "- Note that no hyper-parameter tuning is done here, I just use what I believe to be sensible starting points.\n",
    "- Note also that according to XGBoost docs, missings are interpreted as being missing for a reason (i.e. the fact that they are missing contains some information), and split decisions for every node are made by treating the missing values as separate categories on which the tree can be split. This seems counterintuitive in this context, as I am assuming values are missing because data was not available or for another similar reason. Will investigate the effect of this as described above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Deal with missings***\n",
    "- Change all -99 vals to NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get subset dataframe (only cols which are in vars_ind_numeric)\n",
    "#get only values != -99 -> this will mean that the missings (-99s) will be returned as NaN\n",
    "df_temp=df_train[vars_ind_numeric][df_train[vars_ind_numeric]!=-99].copy()  #make a working copy\n",
    "df_train[vars_ind_numeric] = df_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run xgboost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Send data to h2o\n",
    "h2o_df_train_simple = h2o.H2OFrame(df_train[vars_ind + var_dep],\n",
    "                         destination_frame = 'df_train_simple')\n",
    "#### Set target type to enum as we are running a classification model\n",
    "h2o_df_train_simple[var_dep] = h2o_df_train_simple[var_dep].asfactor()\n",
    "### Ensure all numeric vars are loaded as numerics,\n",
    "### for some reason H2o is converting b06 to enum when loading to cluster, \n",
    "### but doesn't do the same for b06 in test data. Just force all to be numerics\n",
    "h2o_df_train_simple[vars_ind_numeric] = h2o_df_train_simple[vars_ind_numeric].asnumeric()\n",
    "# h2o_df_train[var_dep].types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'target': 'enum'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o_df_train_simple[var_dep].types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a04': 'real',\n",
       " 'a05': 'real',\n",
       " 'a06': 'real',\n",
       " 'a07': 'real',\n",
       " 'a08': 'real',\n",
       " 'a09': 'real',\n",
       " 'a11': 'real',\n",
       " 'a14': 'real',\n",
       " 'a15': 'real',\n",
       " 'b01': 'real',\n",
       " 'b05': 'real',\n",
       " 'b06': 'real',\n",
       " 'c01': 'real',\n",
       " 'c03': 'real',\n",
       " 'd01': 'real',\n",
       " 'd02': 'real',\n",
       " 'd03': 'real',\n",
       " 'e02': 'real',\n",
       " 'e04': 'real',\n",
       " 'e05': 'real',\n",
       " 'e06': 'real',\n",
       " 'e07': 'real',\n",
       " 'e08': 'real',\n",
       " 'e09': 'real',\n",
       " 'e12': 'real',\n",
       " 'e15': 'real',\n",
       " 'e16': 'real',\n",
       " 'e23': 'real',\n",
       " 'f01': 'real',\n",
       " 'f02': 'real',\n",
       " 'f06': 'real',\n",
       " 'f08': 'real',\n",
       " 'f11': 'real',\n",
       " 'f13': 'real',\n",
       " 'f15': 'real',\n",
       " 'f16': 'real',\n",
       " 'f17': 'real',\n",
       " 'f18': 'real',\n",
       " 'f19': 'real',\n",
       " 'f20': 'real',\n",
       " 'f21': 'real',\n",
       " 'f22': 'real',\n",
       " 'f23': 'real',\n",
       " 'f24': 'real',\n",
       " 'f25': 'real',\n",
       " 'f26': 'real',\n",
       " 'f28': 'real',\n",
       " 'f31': 'real',\n",
       " 'f32': 'real'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o_df_train_simple[vars_ind_numeric].types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "idx_h2o_train  = h2o.H2OFrame(idx_train.astype('int').values, destination_frame='idx_train')\n",
    "idx_h2o_val    = h2o.H2OFrame(idx_val.astype('int').values, destination_frame='idx_val')\n",
    "idx_h2o_design = h2o.H2OFrame(idx_design.astype('int').values, destination_frame='idx_design')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {\n",
    "      \"ntrees\" : 200\n",
    "    , \"max_depth\" : 6\n",
    "    , \"learn_rate\" : 0.02\n",
    "    , \"sample_rate\" : 0.7\n",
    "    , \"col_sample_rate_per_tree\" : 0.9\n",
    "    , \"min_rows\" : 10\n",
    "    , \"seed\": 2020\n",
    "    #, \"feature_fraction_seed\": 2019\n",
    "    , \"stopping_metric\": 'mae'\n",
    "    , \"stopping_rounds\": 20\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = H2OXGBoostEstimator(**param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "model.train(x=vars_ind, \n",
    "            y=var_dep[0],\n",
    "            training_frame=h2o_df_train_simple[idx_h2o_train, :],\n",
    "            validation_frame=h2o_df_train_simple[idx_h2o_val, :]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 0.41970637413425504\n",
      "val error 0.420960690107112\n",
      "validation AUCROC computed via scikitlearn: 0.7910770607630329\n"
     ]
    }
   ],
   "source": [
    "h2o.no_progress()\n",
    "\n",
    "pred_train = model.predict(h2o_df_train_simple[idx_h2o_train, :])\n",
    "pred_val   = model.predict(h2o_df_train_simple[idx_h2o_val, :])\n",
    "\n",
    "pred_train = pred_train['p1'].as_data_frame().values.ravel()\n",
    "pred_val   = pred_val['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "print('train error', fn_MAE(y[idx_train], pred_train))\n",
    "print('val error',   fn_MAE(y[idx_val],   pred_val))\n",
    "\n",
    "print('validation AUCROC computed via scikitlearn:', roc_auc_score(y[idx_val], pred_val))\n",
    "\n",
    "\n",
    "h2o.show_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Now manually mean-impute missings\n",
    "#use fillna: computing the mean of each column and filling NaNs with this mean value.\n",
    "df_temp.fillna(df_temp.mean(), inplace=True)\n",
    "df_train_filled = df_train.copy()\n",
    "df_train_filled[vars_ind_numeric] = df_temp\n",
    "# df_train_filled[vars_ind_numeric]\n",
    "# Send back to h2o\n",
    "h2o_df_train_simple_filled = h2o.H2OFrame(df_train_filled[vars_ind + var_dep],\n",
    "                                          destination_frame = 'df_train_simple_filled')\n",
    "#### Set target type to enum as we are running a classification model\n",
    "h2o_df_train_simple_filled[var_dep] = h2o_df_train_simple_filled[var_dep].asfactor()\n",
    "### Ensure all numeric vars are loaded as numerics,\n",
    "### for some reason H2o is converting b06 to enum when loading to cluster, \n",
    "### but doesn't do the same for b06 in test data. Just force all to be numerics\n",
    "h2o_df_train_simple_filled[vars_ind_numeric] = h2o_df_train_simple_filled[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# train model again, now using dataset with filled missings\n",
    "model.train(x=vars_ind, \n",
    "            y=var_dep[0],\n",
    "            training_frame=h2o_df_train_simple_filled[idx_h2o_train, :],\n",
    "            validation_frame=h2o_df_train_simple_filled[idx_h2o_val, :]\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train error 0.41987707815377423\n",
      "val error 0.4211867263675641\n",
      "validation AUCROC computed via scikitlearn: 0.7913571725023665\n"
     ]
    }
   ],
   "source": [
    "h2o.no_progress()\n",
    "\n",
    "pred_train_filled = model.predict(h2o_df_train_simple_filled[idx_h2o_train, :])\n",
    "pred_val_filled   = model.predict(h2o_df_train_simple_filled[idx_h2o_val, :])\n",
    "\n",
    "pred_train_filled = pred_train_filled['p1'].as_data_frame().values.ravel()\n",
    "pred_val_filled   = pred_val_filled['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "print('train error', fn_MAE(y[idx_train], pred_train_filled))\n",
    "print('val error',   fn_MAE(y[idx_val],   pred_val_filled))\n",
    "print('validation AUCROC computed via scikitlearn:', roc_auc_score(y[idx_val], pred_val_filled))\n",
    "\n",
    "\n",
    "h2o.show_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in train error (without - with mean-imputation):  -0.00017070401951918557\n",
      "Difference in val error: (without - with mean-imputation) -0.00022603626045208935\n",
      "Overfitting WITHOUT mean imputation: 0.0012543159728569675\n",
      "Overfitting WITH mean imputation: 0.0013096482137898713\n"
     ]
    }
   ],
   "source": [
    "print('Difference in train error (without - with mean-imputation): ', (fn_MAE(y[idx_train], pred_train) - fn_MAE(y[idx_train], pred_train_filled)))\n",
    "print('Difference in val error: (without - with mean-imputation)', (fn_MAE(y[idx_val], pred_val) - fn_MAE(y[idx_val], pred_val_filled)))\n",
    "print('Overfitting WITHOUT mean imputation:', (fn_MAE(y[idx_val], pred_val)-fn_MAE(y[idx_train], pred_train)))\n",
    "print('Overfitting WITH mean imputation:', (fn_MAE(y[idx_val], pred_val_filled)-fn_MAE(y[idx_train], pred_train_filled)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Judging from the above, it seems that manually performing mean-imputation for missing numerical features increases both training and validation error, and also reduces the amount of overfitting (the difference between val and train MAE). Note however that results supported the opposite conclusion when using a different seed (2019). Either way, the difference between the two seems to be very small. With that in mind, I believe there is a reason the XGBoost algorithm handles missing data the way it does, and that there might be value in learning from the missing data.\n",
    "- Given the above (and notably the fact that the difference between the two is very small and changes depending on the seed), I will move forward with leaving the data as missing, and allow XGBoost to learn from it. I will now investigate the effect of adding HCCV encoding, again using the same model and performing similar comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***HCCV***\n",
    "- Note that any modifications made to train data must also be made to test data (engineered colums etc). However, for now, we will not make any modifications to the train data. We will only do so if the inclusion of HCCV encoding is determined to be beneficial via testing on the train-validation subsets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HCCV ENCODING USING category_encoders\n",
    "df_temp = df_train.copy()\n",
    "enc = LeaveOneOutEncoder(cols=vars_ind_hccv, sigma=0.3)\n",
    "enc.fit(df_temp[idx_design], y[idx_design])\n",
    "df_temp = enc.transform(df_temp)  #encode hccvs in train data\n",
    "# df_temp[vars_ind_hccv].head()  #see effect of encoding\n",
    "\n",
    "# df_test['target'] = np.nan  #add NaN target column to test dataset in order for it to have same shape as df_train\n",
    "# df_test = enc.transform(df_test)  #encode hccvs in test data\n",
    "# df_test.drop(columns='target', inplace=True)  #drop target column from df_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Send data to H2O\n",
    "h2o_df_train_HCCV = h2o.H2OFrame(df_temp[vars_ind+var_dep], destination_frame='df_train_HCCV')\n",
    "#### Set target type to enum as we are running a classification model\n",
    "h2o_df_train_HCCV[var_dep] = h2o_df_train_HCCV[var_dep].asfactor()\n",
    "### Ensure all numeric vars are loaded as numerics,\n",
    "### for some reason H2o is converting b06 to enum when loading to cluster, \n",
    "### but doesn't do the same for b06 in test data. Just force all to be numerics\n",
    "h2o_df_train_HCCV[vars_ind_numeric] = h2o_df_train_HCCV[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.30611671324281187\n",
      "val error 0.30658869361883134\n",
      "validation AUCROC computed via scikitlearn: 0.8732122826275229\n"
     ]
    }
   ],
   "source": [
    "# train model again, now using dataset with filled missings\n",
    "model.train(x=vars_ind, \n",
    "            y=var_dep[0],\n",
    "            training_frame=h2o_df_train_HCCV[idx_h2o_train, :],\n",
    "            validation_frame=h2o_df_train_HCCV[idx_h2o_val, :]\n",
    "            )\n",
    "\n",
    "h2o.no_progress()\n",
    "\n",
    "pred_train_HCCV = model.predict(h2o_df_train_HCCV[idx_h2o_train, :])\n",
    "pred_val_HCCV   = model.predict(h2o_df_train_HCCV[idx_h2o_val, :])\n",
    "\n",
    "pred_train_HCCV = pred_train_HCCV['p1'].as_data_frame().values.ravel()\n",
    "pred_val_HCCV   = pred_val_HCCV['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "print('train error', fn_MAE(y[idx_train], pred_train_HCCV))\n",
    "print('val error',   fn_MAE(y[idx_val],   pred_val_HCCV))\n",
    "print('validation AUCROC computed via scikitlearn:', roc_auc_score(y[idx_val], pred_val_HCCV))\n",
    "\n",
    "\n",
    "h2o.show_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Difference in train error (without - with HCCV encoding):  0.11358966089144318\n",
      "Difference in val error: (without - with HCCV encoding) 0.11437199648828067\n",
      "Overfitting WITHOUT HCCV encoding: 0.0012543159728569675\n",
      "Overfitting WITH HCCV encoding: 0.0004719803760194763\n",
      "Difference in overfitting (without - with HCCV encoding):  0.0007823355968374912\n",
      "Difference in AUC (without - with HCCV encoding):  -0.08185511012515645\n"
     ]
    }
   ],
   "source": [
    "print('Difference in train error (without - with HCCV encoding): ', (fn_MAE(y[idx_train], pred_train) - fn_MAE(y[idx_train], pred_train_HCCV)))\n",
    "print('Difference in val error: (without - with HCCV encoding)', (fn_MAE(y[idx_val], pred_val) - fn_MAE(y[idx_val], pred_val_HCCV)))\n",
    "print('Overfitting WITHOUT HCCV encoding:', (fn_MAE(y[idx_val], pred_val)-fn_MAE(y[idx_train], pred_train)))\n",
    "print('Overfitting WITH HCCV encoding:', (fn_MAE(y[idx_val], pred_val_HCCV)-fn_MAE(y[idx_train], pred_train_HCCV)))\n",
    "print('Difference in overfitting (without - with HCCV encoding): ', (fn_MAE(y[idx_val], pred_val)-fn_MAE(y[idx_train], pred_train))-(fn_MAE(y[idx_val], pred_val_HCCV)-fn_MAE(y[idx_train], pred_train_HCCV)))\n",
    "print('Difference in AUC (without - with HCCV encoding): ', roc_auc_score(y[idx_val], pred_val_filled) - roc_auc_score(y[idx_val], pred_val_HCCV))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can see that both train and val error fall by ~11%, and AUC increases by ~8%. Clearly, HCCV encoding adds much value to the model. Judging by the smaller difference between train and val error, it seems overfitting is also reduced. I will therefore move forward with leave-one-out target encoding. I now have to ensure HCCV encoding is also performed on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Perform HCCV encoding over train and test data, as we need columns to match in both datasets\n",
    "df_train = df_temp\n",
    "\n",
    "df_test['target'] = np.nan  #add NaN target column to test dataset in order for it to have same shape as df_train\n",
    "df_test = enc.transform(df_test)  #encode hccvs in test data\n",
    "df_test.drop(columns='target', inplace=True)  #drop target column from df_test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Investigating how much data is really necessary**\n",
    "- Given the amount of time a single model takes to train, and that some of this is due to the number of examples being considered, I would like to investigate whether it is necessary to include all examples, or if we can get a fairly decent model by considering only a subset of the sample. I will do this by training a few simple models (same as above), each using an increasing number of samples. I will then see if the validation error levels off around a certain number of examples, indicating that there may not be much use in including more than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tried to estimate all models at once and store all relevant info, but experienced memory issues. \n",
    "# Decided it was simpler to just estimate the models individually and store only the errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating model on subset:  125000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.3054095056228175\n",
      "val error 0.30537073168574735\n",
      "validation AUCROC computed via scikitlearn: 0.8755172260791816\n",
      "Estimating model on subset:  150000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.3057016263214906\n",
      "val error 0.30597831607767007\n",
      "validation AUCROC computed via scikitlearn: 0.8753040946955355\n",
      "Estimating model on subset:  175000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.3055261477540275\n",
      "val error 0.305473071584544\n",
      "validation AUCROC computed via scikitlearn: 0.8748952015256654\n",
      "Estimating model on subset:  200000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.30603138502406807\n",
      "val error 0.3059856763448654\n",
      "validation AUCROC computed via scikitlearn: 0.874005924802103\n",
      "Estimating model on subset:  225000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.30626599694114126\n",
      "val error 0.30652960197172224\n",
      "validation AUCROC computed via scikitlearn: 0.8734621739103663\n",
      "Estimating model on subset:  250000\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "train error 0.30611671324281187\n",
      "val error 0.30658869361883134\n",
      "validation AUCROC computed via scikitlearn: 0.8732122826275229\n"
     ]
    }
   ],
   "source": [
    "### Will run five models, each with 25k more examples than the previous, starting at 125k. \n",
    "subset_errors = [] #will fill with tuples, first element train_mae, second element val_mae\n",
    "for i in range(125000, 275000, 25000):\n",
    "    print('Estimating model on subset: ', i)\n",
    "    ## Define current training dataset\n",
    "    df_temp = df_train.loc[0:i-1, :].copy()  #create a copy to work with, in order to avoid any issues due to working with slices copies\n",
    "    ## Separate it into train, val, design\n",
    "    rng = np.random.RandomState(2020)\n",
    "    fold = rng.randint(0, 10, df_temp.shape[0])\n",
    "    df_temp['fold'] = fold\n",
    "    #get indices for each subset\n",
    "    idx_train_  = df_temp['fold'].isin(range(8))\n",
    "    idx_val_    = df_temp['fold'].isin([7, 8])\n",
    "    idx_design_ = df_temp['fold'].isin(range(9))\n",
    "    #drop fold column\n",
    "    df_temp = df_temp.drop(columns='fold')\n",
    "    \n",
    "    ### Send data to h2o\n",
    "    h2o_df_temp = h2o.H2OFrame(df_temp[vars_ind + var_dep],\n",
    "                         destination_frame = 'df_temp')\n",
    "    ### Set target type to enum as we are running a classification model\n",
    "    h2o_df_temp[var_dep] = h2o_df_temp[var_dep].asfactor()\n",
    "    ### Ensure all numeric vars are loaded as numerics,\n",
    "    ### for some reason H2o is converting b06 to enum when loading to cluster, \n",
    "    ### but doesn't do the same for b06 in test data. Just force all to be numerics\n",
    "    h2o_df_temp[vars_ind_numeric] = h2o_df_temp[vars_ind_numeric].asnumeric()\n",
    "\n",
    "    ### Boolean masks\n",
    "    idx_h2o_train_  = h2o.H2OFrame(idx_train_.astype('int').values)\n",
    "    idx_h2o_val_    = h2o.H2OFrame(idx_val_.astype('int').values)\n",
    "    idx_h2o_design_ = h2o.H2OFrame(idx_design_.astype('int').values)\n",
    "    \n",
    "    ### Train model, now using current subset dataset\n",
    "    model.train(x=vars_ind, \n",
    "                y=var_dep[0],\n",
    "                training_frame=h2o_df_temp[idx_h2o_train_, :],\n",
    "                validation_frame=h2o_df_temp[idx_h2o_val_, :]\n",
    "                )\n",
    "\n",
    "    h2o.no_progress()\n",
    "\n",
    "    pred_train_temp = model.predict(h2o_df_temp[idx_h2o_train_, :])\n",
    "    pred_val_temp   = model.predict(h2o_df_temp[idx_h2o_val_, :])\n",
    "\n",
    "    pred_train_temp = pred_train_temp['p1'].as_data_frame().values.ravel()\n",
    "    pred_val_temp   = pred_val_temp['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "    ### store errors\n",
    "    train_error = fn_MAE(y[0:i][idx_train_], pred_train_temp)\n",
    "    val_error = fn_MAE(y[0:i][idx_val_],   pred_val_temp)\n",
    "    # append to list\n",
    "    subset_errors.append((train_error, val_error))\n",
    "    \n",
    "    print('train error', train_error)\n",
    "    print('val error',   val_error)\n",
    "    print('validation AUCROC computed via scikitlearn:', roc_auc_score(y[0:i][idx_val_], pred_val_temp))\n",
    "\n",
    "\n",
    "    h2o.show_progress()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate train and val MAEs into separate lists\n",
    "training_maes = [subset_errors[idx][0] for idx,elem in enumerate(subset_errors)]\n",
    "validation_maes = [subset_errors[idx][1] for idx,elem in enumerate(subset_errors)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max validation MAE:  0.30658869361883134 corresponding to:  250000 samples\n",
      "min validation MAE:  0.30537073168574735 corresponding to:  125000 samples\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(12, 6))\n",
    "ax1 = fig.add_subplot(1, 1, 1)\n",
    "rnge = np.arange(125000, 275000, 25000)\n",
    "ax1.plot(rnge,\n",
    "         training_maes,\n",
    "         'k-',\n",
    "         label='training'\n",
    "        )\n",
    "ax1.plot(rnge,\n",
    "         validation_maes,\n",
    "         'r-',\n",
    "         label='validation'\n",
    "        )\n",
    "ax1.set_xlabel('number of exaples in sample', fontsize=20)\n",
    "ax1.set_ylabel('mae', fontsize=20)\n",
    "_ = ax1.legend(fontsize=20)\n",
    "ax1.set_xticks(rnge.tolist())\n",
    "ax1.set_xticklabels(['125k', '150k', '175k','200k','225k','250k'])\n",
    "print('max validation MAE: ', np.max(validation_maes), 'corresponding to: ', rnge[np.argmax(validation_maes)], 'samples')\n",
    "print('min validation MAE: ', np.min(validation_maes), 'corresponding to: ', rnge[np.argmin(validation_maes)], 'samples')                                                                                             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is no <b>clear</b> linear trend between validation MAE and the number of examples considered. In fact, the lowest validation MAE is seen when modelling using the smallest sample (125k examples).. I strongly believe this to be a coincidence rather than an indication that increased sample sizes causes worse validation MAE. Nevertheless, given that the val MAE really doesn't change much over all sample sizes tried (stays between range 0.3053 to 0.3065), I feel confident in conducting further analysis using only a subset of 125k samples <b> in order to reduce computation time. I acknowledge that by doing this, I may be sacrificing more 'optimal' results.</b>\n",
    "- Once hyper-params are tuned and best model is found, will of course estimate over entire test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLM_model_python_1594936985348_67\n",
      "Key_Frame__upload_829c9ceba805da98740d8b57aa6c4915.hex\n",
      "Key_Frame__upload_86864b13f355dd085d9c32624e734ece.hex\n",
      "Key_Frame__upload_87625caa44045c7c6d89a67275554ad4.hex\n",
      "Key_Frame__upload_89a2d7e2e514b01a7530d4e7e25a4108.hex\n",
      "Key_Frame__upload_8b8f311e768f500ceb005b3267d7449b.hex\n",
      "Key_Frame__upload_91c2d5a8f432f268584baf9f4c514ede.hex\n",
      "Key_Frame__upload_96172655eedf44104d7cbee85a874a4c.hex\n",
      "Key_Frame__upload_96e36d3fe8e55fc3e9546a9324044ad5.hex\n",
      "Key_Frame__upload_9a4e2091e9e195432f97946b2661485e.hex\n",
      "Key_Frame__upload_9c310195f228fd448bd054dae7bc4187.hex\n",
      "Key_Frame__upload_9cfe9517700794c2361aa9e0015a40fc.hex\n",
      "Key_Frame__upload_9d4412883a8dbff43a242aa0a2dd4484.hex\n",
      "Key_Frame__upload_9dc4e39fe14ca72c192b4bb587534fae.hex\n",
      "Key_Frame__upload_a0fed4f03d7b4884d4c9cb5589954de8.hex\n",
      "Key_Frame__upload_a9b571d61c0822384f3262559aed4d5b.hex\n",
      "Key_Frame__upload_a9fc70f54e5af56fb052fcc323ff4169.hex\n",
      "Key_Frame__upload_aaa0a2dd364e0520a547accf559a442d.hex\n",
      "Key_Frame__upload_aef8d42180a10cc9635237eed3f44174.hex\n",
      "Key_Frame__upload_af01a900a0931175a1994ea313104382.hex\n",
      "Key_Frame__upload_b0407c85ad99ccddfb349bc65f514972.hex\n",
      "Key_Frame__upload_baabe686fe8e1c77f25c758c36f74e50.hex\n",
      "XGBoost_model_python_1594936985348_120\n",
      "XGBoost_model_python_1594936985348_175\n",
      "XGBoost_model_python_1594936985348_210\n",
      "XGBoost_model_python_1594936985348_255\n",
      "XGBoost_model_python_1594936985348_296\n",
      "XGBoost_model_python_1594936985348_333\n",
      "XGBoost_model_python_1594936985348_34\n",
      "XGBoost_model_python_1594936985348_370\n",
      "XGBoost_model_python_1594936985348_403\n",
      "_8611e549a0ebf2e214718239c6984ecf\n",
      "_8676997d37814bc72cebf35c5bc140cf\n",
      "_9730324608182e1dd3156a16436e4201\n",
      "_9f6023a13255d5bd71125b6380a74218\n",
      "_b002e3a3ebfe1ce5922e59ad586947b7\n",
      "_b26c6321175ab6bcad0e65e0b15d49f0\n",
      "_b55800c04c7747931b0524846d514068\n",
      "_b69631ea50113c2cb89b3af2838e4e00\n",
      "_bea456e6126d02db1c35ec2d741c465a\n",
      "df_temp\n",
      "df_test\n",
      "df_train\n",
      "df_train_HCCV\n",
      "df_train_simple\n",
      "df_train_simple_filled\n",
      "idx_design\n",
      "idx_train\n",
      "idx_val\n",
      "modelmetrics_GLM_model_python_1594936985348_67@-2928334357789783168_on_py_5_sid_b6f2@-7632859947708469993\n",
      "modelmetrics_XGBoost_model_python_1594936985348_120@-7060742383594992464_on_py_12_sid_a9dc@2438319666549496976\n",
      "modelmetrics_XGBoost_model_python_1594936985348_120@-7060742383594992464_on_py_13_sid_a9dc@-4201470979806428656\n",
      "modelmetrics_XGBoost_model_python_1594936985348_120@-7060742383594992464_on_py_14_sid_a9dc@2438319666549496976\n",
      "modelmetrics_XGBoost_model_python_1594936985348_120@-7060742383594992464_on_py_15_sid_a9dc@-4201470979806428656\n",
      "modelmetrics_XGBoost_model_python_1594936985348_175@-7710198897175782566_on_py_20_sid_a9dc@-1373735619770144292\n",
      "modelmetrics_XGBoost_model_python_1594936985348_175@-7710198897175782566_on_py_21_sid_a9dc@1457917522729171728\n",
      "modelmetrics_XGBoost_model_python_1594936985348_175@-7710198897175782566_on_py_22_sid_a9dc@-1373735619770144292\n",
      "modelmetrics_XGBoost_model_python_1594936985348_175@-7710198897175782566_on_py_23_sid_a9dc@1457917522729171728\n",
      "modelmetrics_XGBoost_model_python_1594936985348_210@-368851828921840722_on_py_28_sid_a9dc@7304020065225572160\n",
      "modelmetrics_XGBoost_model_python_1594936985348_210@-368851828921840722_on_py_29_sid_a9dc@1483406793309242964\n",
      "modelmetrics_XGBoost_model_python_1594936985348_210@-368851828921840722_on_py_30_sid_a9dc@7304020065225572160\n",
      "modelmetrics_XGBoost_model_python_1594936985348_210@-368851828921840722_on_py_31_sid_a9dc@1483406793309242964\n",
      "modelmetrics_XGBoost_model_python_1594936985348_255@-8074239407384355408_on_py_36_sid_a9dc@7626761394792148716\n",
      "modelmetrics_XGBoost_model_python_1594936985348_255@-8074239407384355408_on_py_37_sid_a9dc@4897354368456815616\n",
      "modelmetrics_XGBoost_model_python_1594936985348_255@-8074239407384355408_on_py_38_sid_a9dc@7626761394792148716\n",
      "modelmetrics_XGBoost_model_python_1594936985348_255@-8074239407384355408_on_py_39_sid_a9dc@4897354368456815616\n",
      "modelmetrics_XGBoost_model_python_1594936985348_296@-5825413353219577277_on_py_44_sid_a9dc@-5797755726268340468\n",
      "modelmetrics_XGBoost_model_python_1594936985348_296@-5825413353219577277_on_py_45_sid_a9dc@-4812539035619824184\n",
      "modelmetrics_XGBoost_model_python_1594936985348_296@-5825413353219577277_on_py_46_sid_a9dc@-5797755726268340468\n",
      "modelmetrics_XGBoost_model_python_1594936985348_296@-5825413353219577277_on_py_47_sid_a9dc@-4812539035619824184\n",
      "modelmetrics_XGBoost_model_python_1594936985348_333@-5262249851112885942_on_py_52_sid_a9dc@-4016596564057014420\n",
      "modelmetrics_XGBoost_model_python_1594936985348_333@-5262249851112885942_on_py_53_sid_a9dc@109380020753370132\n",
      "modelmetrics_XGBoost_model_python_1594936985348_333@-5262249851112885942_on_py_54_sid_a9dc@-4016596564057014420\n",
      "modelmetrics_XGBoost_model_python_1594936985348_333@-5262249851112885942_on_py_55_sid_a9dc@109380020753370132\n",
      "modelmetrics_XGBoost_model_python_1594936985348_34@-8556150266682310832_on_py_4_sid_a9dc@-1204879218380523272\n",
      "modelmetrics_XGBoost_model_python_1594936985348_34@-8556150266682310832_on_py_5_sid_a9dc@1205408707887763832\n",
      "modelmetrics_XGBoost_model_python_1594936985348_34@-8556150266682310832_on_py_6_sid_a9dc@-1204879218380523272\n",
      "modelmetrics_XGBoost_model_python_1594936985348_34@-8556150266682310832_on_py_7_sid_a9dc@1205408707887763832\n",
      "modelmetrics_XGBoost_model_python_1594936985348_370@1869054417654483567_on_py_60_sid_a9dc@1058822201050273860\n",
      "modelmetrics_XGBoost_model_python_1594936985348_370@1869054417654483567_on_py_61_sid_a9dc@-1579383166008273716\n",
      "modelmetrics_XGBoost_model_python_1594936985348_370@1869054417654483567_on_py_62_sid_a9dc@1058822201050273860\n",
      "modelmetrics_XGBoost_model_python_1594936985348_370@1869054417654483567_on_py_63_sid_a9dc@-1579383166008273716\n",
      "modelmetrics_XGBoost_model_python_1594936985348_403@7510902697361162129_on_py_68_sid_a9dc@-1373735619770144292\n",
      "modelmetrics_XGBoost_model_python_1594936985348_403@7510902697361162129_on_py_69_sid_a9dc@1457917522729171728\n",
      "modelmetrics_XGBoost_model_python_1594936985348_403@7510902697361162129_on_py_70_sid_a9dc@-1373735619770144292\n",
      "modelmetrics_XGBoost_model_python_1594936985348_403@7510902697361162129_on_py_71_sid_a9dc@1457917522729171728\n",
      "predictions_84d5_XGBoost_model_python_1594936985348_296_on_py_47_sid_a9dc\n",
      "predictions_85cf_XGBoost_model_python_1594936985348_403_on_py_71_sid_a9dc\n",
      "predictions_8725_XGBoost_model_python_1594936985348_296_on_py_46_sid_a9dc\n",
      "predictions_87ee_XGBoost_model_python_1594936985348_210_on_py_31_sid_a9dc\n",
      "predictions_88bd_XGBoost_model_python_1594936985348_255_on_py_39_sid_a9dc\n",
      "predictions_8a0e_XGBoost_model_python_1594936985348_120_on_py_15_sid_a9dc\n",
      "predictions_8d9a_GLM_model_python_1594936985348_67_on_df_test\n",
      "predictions_9786_XGBoost_model_python_1594936985348_255_on_py_38_sid_a9dc\n",
      "predictions_99d1_XGBoost_model_python_1594936985348_333_on_py_55_sid_a9dc\n",
      "predictions_9e9c_XGBoost_model_python_1594936985348_120_on_py_14_sid_a9dc\n",
      "predictions_9f10_XGBoost_model_python_1594936985348_175_on_py_22_sid_a9dc\n",
      "predictions_a5f1_XGBoost_model_python_1594936985348_175_on_py_23_sid_a9dc\n",
      "predictions_a89f_XGBoost_model_python_1594936985348_370_on_py_63_sid_a9dc\n",
      "predictions_abaa_XGBoost_model_python_1594936985348_370_on_py_62_sid_a9dc\n",
      "predictions_b557_XGBoost_model_python_1594936985348_403_on_py_70_sid_a9dc\n",
      "predictions_b82c_XGBoost_model_python_1594936985348_34_on_py_6_sid_a9dc\n",
      "predictions_b845_XGBoost_model_python_1594936985348_210_on_py_30_sid_a9dc\n",
      "predictions_bb05_XGBoost_model_python_1594936985348_333_on_py_54_sid_a9dc\n",
      "predictions_be1d_XGBoost_model_python_1594936985348_34_on_py_7_sid_a9dc\n",
      "py_10_sid_a9dc\n",
      "py_11_sid_a9dc\n",
      "py_18_sid_a9dc\n",
      "py_19_sid_a9dc\n",
      "py_2_sid_a9dc\n",
      "py_3_sid_a9dc\n",
      "py_66_sid_a9dc\n",
      "py_67_sid_a9dc\n"
     ]
    }
   ],
   "source": [
    "# h2o.ls()\n",
    "for key in h2o.ls()['key']:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Clear h2o cluster as won't need most of the many things saved there\n",
    "# h2o.remove_all()\n",
    "## Instead of removing all, can keep only what we need and remove everything else\n",
    "h2o_keys_to_keep = ['df_temp', 'df_train_HCCV', 'df_train_simple', 'df_train_simple_filled', 'idx_design', 'idx_train', 'idx_val']\n",
    "\n",
    "for key in h2o.ls()['key']:\n",
    "    if key not in h2o_keys_to_keep:\n",
    "        h2o.remove(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>df_temp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>df_train_HCCV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>df_train_simple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>df_train_simple_filled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>idx_design</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>idx_train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>idx_val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      key\n",
       "0                 df_temp\n",
       "1           df_train_HCCV\n",
       "2         df_train_simple\n",
       "3  df_train_simple_filled\n",
       "4              idx_design\n",
       "5               idx_train\n",
       "6                 idx_val"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h2o.ls()  #check what is left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Declare all necessary variables for 125k sample subset of df_train\n",
    "df_train_125k = df_train.loc[0:125000-1, :].copy()\n",
    "## Separate it into train, val, design\n",
    "rng = np.random.RandomState(2020)\n",
    "fold = rng.randint(0, 10, df_train_125k.shape[0])\n",
    "df_train_125k['fold'] = fold\n",
    "#get indices for each subset\n",
    "idx_train_  = df_train_125k['fold'].isin(range(8))\n",
    "idx_val_    = df_train_125k['fold'].isin([7, 8])\n",
    "idx_design_ = df_train_125k['fold'].isin(range(9))\n",
    "#drop fold column\n",
    "df_train_125k = df_train_125k.drop(columns='fold')\n",
    "\n",
    "### Send data to h2o\n",
    "h2o_df_train_125k= h2o.H2OFrame(df_train_125k[vars_ind + var_dep],\n",
    "                     destination_frame = 'df_train_125k')\n",
    "\n",
    "\n",
    "### Boolean masks\n",
    "idx_h2o_train_125k  = h2o.H2OFrame(idx_train_.astype('int').values, destination_frame='idx_train_125k')\n",
    "idx_h2o_val_125k    = h2o.H2OFrame(idx_val_.astype('int').values, destination_frame='idx_val_125k')\n",
    "idx_h2o_design_125k = h2o.H2OFrame(idx_design_.astype('int').values, destination_frame='idx_design_125k')\n",
    "\n",
    "#declare the target variable for convenience\n",
    "y_125k = df_train_125k[var_dep].values.ravel()\n",
    "\n",
    "### Set target type to enum\n",
    "h2o_df_train_125k[var_dep] = h2o_df_train_125k[var_dep].asfactor()\n",
    "### Set numerics type to numeric\n",
    "h2o_df_train_125k[vars_ind_numeric] = h2o_df_train_125k[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Define features to use***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = vars_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Hyper-parameter tuning using H2O Gridsearch***\n",
    "<br>\n",
    "Will use random grid search rather than cartesian in order to save some time, as estimation for single models is already somewhat slow. The basic idea of this is to randomly sample the search space instead of discretising it with a cartesian grid. According to a Aloïs Bissuel (https://medium.com/criteo-labs/hyper-parameter-optimization-algorithms-2fe447525903), one of the many advantages of random search is that if two hps are slightly correlated, the algorithm allows to find more precisely the optima of each parameter. It is also particularly effective when some hps are given a larger range of variation than others, which is definitely the case in the following grid-search. \n",
    "<br>\n",
    "<u><b>Hyper parameters being tuned (descriptions more-or-less paraphrased from docs):</b></u>\n",
    "- <b>min_rows:</b> minimum number of observations for a leaf. Defaults to 1.\n",
    "- <b>sample_rate:</b> Row sampling ratio of the training instance i.e. the ratio of examples to randomly sample from the training set. Note that setting to 0.63 is would be the expected sample_rate for standard bootstrapping. Default is 1 (i.e. randomly sample from entire set), and ranges between 0 and 1.\n",
    "- <b>learn_rate:</b> Specify the learning rate by which to shrink the feature weights after each boosting step, each time making the boosting process more conservative and preventing overfitting. Ranges from 0 to 1 and defaults to 0.3. Seems that range is NOT inclusive, as I get errors when running either 0 or 1. This makes sense as a value of 0 would mean no shrinkage at all after each boosting step, and a value of 1 would mean 100% shrinkage after a single boosting step.\n",
    "- <b>max_depth:</b> Specifies the maximum tree depth. The higher the value, the more complex the model (possibly implying more overfitting). Defaults to 6 and setting to 0 specifies no limit (can get insanely complicated trees)\n",
    "- <b>n_trees:</b> Number of trees to build, defaults to 50.\n",
    "- <b>col_sample_rate_per_tree: </b> The column subsampling rate per tree (sample <b>without</b> replacement. Defaults to 1, and ranges from 0 to 1.\n",
    "<br><br>\n",
    "<u><b>Non-tuned XGBoost parameters:</b></u>\n",
    "- <b>strategy:</b> whether to try all possible models (<code>Cartesian</code>) or pick randomly (<code>RandomDiscrete</code>). Since picked <code>RandomDiscrete</code>, need to give it other stopping criteria.\n",
    "- <b>max_runtime_secs:</b> timeout limit after which to early stop\n",
    "- <b>seed:</b> ensures same starting point for models, allows for reproducability\n",
    "- <b>score_tree_interval:</b> number of trees after which to score the model\n",
    "- <b>stopping_metric:</b> Metric by which to check if model has improved when attempting early stop. Using AUC as this is the relevant metric we are interested in for Kaggle sub.\n",
    "- <b>stopping_rounds:</b> stops if hasn't found improvement in <code>stopping_metric</code> for specified number of rounds\n",
    "- <b>distribution:</b> specify the distribution (loss function) - bernoulli because response colum is a 2-class categorical.\n",
    "- <b>categorical_encoding:</b> XGBoost defaults to one_hot_internal encoding. That is, it one hots on-the-fly, creating N+1 new cols for categorical features with N levels. Just set to auto.\n",
    "- <b>tree_method:</b> Specifies the tree-construction method (<i>exact greedy</i>, <i>approx. greedy</i>, <i>histogram optimized greedy </i> methods). <code>auto</code> allows the model to choose the best method - it will likely pick <code>exact</code> for small-medium datasets, and <code>approx</code> for large datasets. Note that <code>hist</code> method uses a \"fast histogram optimised approximate greedy method\", where only a subset of possible split values are considered. This should theoretically make the grid search algorithm quicker, however I am more interested in model performance in this case, and given that the the search is not too painfully long, I have opted to use <code>auto</code> and let H2O decide which is best.\n",
    "- <b>keep_cross_validation_models</b>: whether to keep cross-validated models, defaults to True. We set to false in order to save some memory in the H2O cluster, and we don't need them.\n",
    "- <b>keep_cross_validation_predictions:</b> same as above but for predictions\n",
    "- <b>keep_cross_validation_fold_assignment:</b> same as above but for fold assignments. Not needed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Grid Build progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# %%time\n",
    "### H2O GRIDSEARCH - hyper-parameter tuning\n",
    "## Will use random grid search rather than cartesian to save some time.\n",
    "# del search_criteria, xgboost_hyperparams, xgboost_params, xgboost_grid\n",
    "search_criteria = {\n",
    "     'strategy': 'RandomDiscrete'\n",
    "    ,'max_runtime_secs': 3600  #timeout limit\n",
    "    ,'max_models': 20  #max number of models to run\n",
    "    ,'seed': 2020  #ensure same models are run for reproducability\n",
    "    ,'stopping_metric': 'auc'  #determines metric by which to check if model has improved\n",
    "    ,'stopping_rounds': 3  #stops if hasn't found improvement in AUC for 3 rounds\n",
    "}\n",
    "\n",
    "min_rows = [2, 5, 10, 20, 40]\n",
    "sample_rate = [i/10 for i in range(5,10)]\n",
    "col_sample_rate_per_tree = [i/10 for i in np.arange(5,10,0.5)]\n",
    "learn_rate = [i/10 for i in np.arange(1,9.5,0.5)]  \n",
    "max_depth = list(np.arange(0,12,2))\n",
    "ntrees = [50, 100, 150, 200]\n",
    "\n",
    "\n",
    "xgboost_hyperparams = {\n",
    "     'min_rows': min_rows\n",
    "    ,'sample_rate': sample_rate\n",
    "    , 'col_sample_rate_per_tree': col_sample_rate_per_tree\n",
    "    , 'learn_rate': learn_rate\n",
    "    , 'max_depth': max_depth\n",
    "    , 'ntrees': ntrees\n",
    "}\n",
    "    \n",
    "    \n",
    "xgboost_params = {\n",
    "#     'ntrees':200\n",
    "#     , 'max_depth':6\n",
    "      'seed':2020\n",
    "    , 'distribution':'bernoulli'  #specify the distribution (loss function) - bernoulli because response colum is a 2-class categorical\n",
    "    , 'categorical_encoding':'AUTO'  #XGBoost defaults to one_hot_internal encoding. That is it one hots on-the-fly, creating N+1 new cols for categorical features with N levels\n",
    "    , 'keep_cross_validation_models':False\n",
    "    , 'keep_cross_validation_predictions':False\n",
    "    , 'keep_cross_validation_fold_assignment':False\n",
    "    , 'score_tree_interval': 20 #number of trees after which to score the model\n",
    "    , 'tree_method': 'auto'\n",
    "#     , 'tree_method': 'hist'\n",
    "#     , 'learn_rate':0.3\n",
    "}\n",
    "\n",
    "## Train and validate a random grid of XGBoost GBMs\n",
    "xgboost_grid = H2OGridSearch(\n",
    "    model=H2OXGBoostEstimator(**xgboost_params)\n",
    "    , grid_id='xgboost_grid'\n",
    "    , hyper_params=xgboost_hyperparams\n",
    "    , search_criteria=search_criteria\n",
    "#     , parallelism = 0 #adaptive parallelism, decided by H2O\n",
    ")\n",
    "\n",
    "xgboost_grid.train(x=features, \n",
    "                   y='target',\n",
    "                   training_frame=h2o_df_train_125k[idx_h2o_train_125k, :],\n",
    "                   validation_frame=h2o_df_train_125k[idx_h2o_val_125k, :],\n",
    "                   seed=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of running grid search tuning all variables in <code>xgboost_hyperparams</code>**\n",
    "- Note that the best model when tuning col_sample_rate_per_tree, learn_rate, max_depth, min_rows, ntrees, sample_rate has a validation AUC of 0.9275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     col_sample_rate_per_tree learn_rate max_depth min_rows ntrees  \\\n",
      "0                        0.95        0.7        10      2.0    150   \n",
      "1                        0.85        0.2         8      5.0    150   \n",
      "2                        0.55        0.4         8     10.0    150   \n",
      "3                         0.8        0.3        10     10.0    100   \n",
      "4                         0.5        0.3         6      2.0    200   \n",
      "5                        0.65       0.25        10     40.0     50   \n",
      "6                        0.75       0.55        10     40.0    150   \n",
      "7                        0.75        0.3         6      2.0     50   \n",
      "8                        0.75       0.55         6     40.0    150   \n",
      "9                        0.55       0.65        10     10.0    100   \n",
      "10                        0.9        0.8         4      2.0     50   \n",
      "11                       0.75       0.85         4     20.0     50   \n",
      "12                        0.5       0.85         8     40.0     50   \n",
      "13                        0.9       0.75         2     10.0    200   \n",
      "14                       0.65       0.85         2     40.0    100   \n",
      "15                        0.9       0.45         2     20.0    150   \n",
      "16                       0.65       0.75         2      2.0     50   \n",
      "17                        0.8       0.45         2      5.0     50   \n",
      "18                        0.5        0.2         2      2.0     50   \n",
      "19                        0.6        0.7         0     40.0    150   \n",
      "\n",
      "   sample_rate              model_ids                 auc  \n",
      "0          0.9  xgboost_grid_model_10  0.9275413705848685  \n",
      "1          0.6   xgboost_grid_model_5  0.9056480293010454  \n",
      "2          0.7  xgboost_grid_model_13  0.9045647633479011  \n",
      "3          0.6  xgboost_grid_model_15  0.9045137629555903  \n",
      "4          0.6  xgboost_grid_model_18  0.8987928417500314  \n",
      "5          0.8   xgboost_grid_model_7  0.8934433324895514  \n",
      "6          0.6   xgboost_grid_model_1  0.8902264070116681  \n",
      "7          0.6   xgboost_grid_model_2  0.8868164193364172  \n",
      "8          0.6  xgboost_grid_model_20  0.8859185545090035  \n",
      "9          0.5   xgboost_grid_model_8  0.8814659747867749  \n",
      "10         0.8   xgboost_grid_model_3  0.8809218280210719  \n",
      "11         0.6  xgboost_grid_model_19  0.8773348887436833  \n",
      "12         0.6   xgboost_grid_model_9  0.8772216070310762  \n",
      "13         0.5   xgboost_grid_model_6  0.8766733296709155  \n",
      "14         0.7  xgboost_grid_model_17  0.8755029190278995  \n",
      "15         0.6  xgboost_grid_model_11  0.8749515229731598  \n",
      "16         0.5  xgboost_grid_model_12  0.8703111448482007  \n",
      "17         0.5  xgboost_grid_model_14  0.8662710851046103  \n",
      "18         0.6   xgboost_grid_model_4  0.8622257041083453  \n",
      "19         0.6  xgboost_grid_model_16                 0.5  \n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Get the grid results, sorted by validation AUC\n",
    "xgboost_grid_performance = xgboost_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "xgboost_grid_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the same grid search, but leave learn rate as 0.3 i.e. don't optimise it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Grid Build progress: |████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### H2O GRIDSEARCH 2 - hyper-parameter tuning\n",
    "## Will use random grid search rather than cartesian to save some time.\n",
    "search_criteria2 = {\n",
    "     'strategy': 'RandomDiscrete'\n",
    "    ,'max_runtime_secs': 3600  #timeout limit\n",
    "    ,'max_models': 20  #max number of models to run\n",
    "    ,'seed': 2020  #ensure same models are run for reproducability\n",
    "    ,'stopping_metric': 'auc'  #determines metric by which to check if model has improved\n",
    "    ,'stopping_rounds': 3  #stops if hasn't found improvement in AUC for 3 rounds\n",
    "}\n",
    "\n",
    "min_rows2 = [2, 5, 10, 20, 40]\n",
    "sample_rate2 = [i/10 for i in range(5,10)]\n",
    "col_sample_rate_per_tree2 = [i/10 for i in np.arange(5,10,0.5)]\n",
    "# learn_rate2 = [i/10 for i in np.arange(1,9.5,0.5)]\n",
    "# learn_rate = [i/10 for i in np.arange(2,9,0.5)]\n",
    "max_depth2 = list(np.arange(0,12,2))\n",
    "ntrees2 = [50, 100, 150, 200]\n",
    "\n",
    "\n",
    "xgboost2_hyperparams = {\n",
    "     'min_rows': min_rows2\n",
    "    ,'sample_rate': sample_rate2\n",
    "    , 'col_sample_rate_per_tree': col_sample_rate_per_tree2\n",
    "#     , 'learn_rate': learn_rate2\n",
    "    , 'max_depth': max_depth2\n",
    "    , 'ntrees': ntrees2}\n",
    "    \n",
    "    \n",
    "xgboost2_params = {\n",
    "      'seed':2020\n",
    "    , 'distribution':'bernoulli'  #specify the distribution (loss function) - bernoulli because response colum is a 2-class categorical\n",
    "    , 'categorical_encoding':'AUTO'  #XGBoost defaults to one_hot_internal encoding. That is it one hots on-the-fly, creating N+1 new cols for categorical features with N levels\n",
    "    , 'keep_cross_validation_models':False\n",
    "    , 'keep_cross_validation_predictions':False\n",
    "    , 'keep_cross_validation_fold_assignment':False\n",
    "    , 'score_tree_interval': 20 #number of trees after which to score the model\n",
    "    , 'tree_method': 'auto'\n",
    "    , 'learn_rate': 0.3\n",
    "}\n",
    "\n",
    "## Train and validate a random grid of XGBoost GBMs\n",
    "xgboost2_grid = H2OGridSearch(\n",
    "    model=H2OXGBoostEstimator(**xgboost2_params)\n",
    "    , grid_id='xgboost2_grid'\n",
    "    , hyper_params=xgboost2_hyperparams\n",
    "    , search_criteria=search_criteria2\n",
    "#     , parallelism = 0 #adaptive parallelism, decided by H2O\n",
    ")\n",
    "\n",
    "xgboost2_grid.train(x=features, \n",
    "                   y='target',\n",
    "                   training_frame=h2o_df_train_125k[idx_h2o_train_125k, :],\n",
    "                   validation_frame=h2o_df_train_125k[idx_h2o_val_125k, :],\n",
    "                   seed=2020)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Results of running grid search tuning all variables in <code>xgboost2_hyperparams</code> (same as in first grid serach, but excluding <code>learn_rate</code>)**\n",
    "- Note that the best model when tuning only <code>col_sample_rate_per_tree</code>, <code>max_depth</code>, <code>min_rows</code>, <code>ntrees</code>, <code>sample_rate</code> (not <code>learn_rate</code>) has a validation AUC of 0.9329 i.e. HIGHER than when also tuning learn_rate\n",
    "- I came across this somewhat accidentally, but thought it was interesting to mention and possibly to investigate. Upon quick reflection I believe this might be one of those cases where the random gridsearch returns good models, but not necessarily the optimal ones (in the case of the one including <code>learn_rate</code> as a hyper-parameter). Had I explored more models (while varying <code>learn_rate</code>), I might have found one with a higher AUC. \n",
    "- This is supported by the fact that the model <code>xgboost2_grid_model_16</code> (auc: 0.9328576403707831), estimated with HPs <code>col_sample_rate_per_tree</code>, <code>max_depth</code>, <code>min_rows</code>, <code>ntrees</code>, <code>sample_rate</code> (and <code>learn_rate</code>=0.3), is supposedly the best model. However, looking at the models estimated in the first grid search, I don't find one with the same HP values (excluding <code>learn_rate</code>). I'm therefore curious to attempt to tune <code>learn_rate</code>, while keeping the other 'best' hp values constant. In doing this, I acknowledge that hyper-params are sensitive to each other, and that by changing the learning rate, the optimal values for other parameters might also change. However, for the sake of investigating the effect of learn rate on the same model, I will move forward with this model formulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     col_sample_rate_per_tree max_depth min_rows ntrees sample_rate  \\\n",
      "0                        0.65        10      2.0    150         0.9   \n",
      "1                         0.6        10      5.0    200         0.9   \n",
      "2                        0.95        10      5.0    200         0.7   \n",
      "3                        0.95         8      5.0    150         0.6   \n",
      "4                         0.7        10     40.0    150         0.6   \n",
      "5                        0.75         8      5.0    100         0.5   \n",
      "6                         0.5         6      2.0    100         0.9   \n",
      "7                         0.8        10     10.0     50         0.5   \n",
      "8                         0.9         6     10.0    150         0.6   \n",
      "9                        0.85         6     10.0    100         0.6   \n",
      "10                        0.6         6     10.0    100         0.7   \n",
      "11                       0.75         8     40.0     50         0.6   \n",
      "12                        0.9         4     40.0    150         0.7   \n",
      "13                       0.95         2      5.0    200         0.5   \n",
      "14                       0.75         2     20.0    150         0.7   \n",
      "15                       0.55         2     10.0     50         0.5   \n",
      "16                       0.55         2     20.0     50         0.5   \n",
      "17                       0.85         0      2.0    150         0.7   \n",
      "18                        0.6         0      2.0    150         0.9   \n",
      "19                       0.75         0     10.0    100         0.5   \n",
      "\n",
      "                 model_ids                 auc  \n",
      "0   xgboost2_grid_model_16  0.9328576403707831  \n",
      "1   xgboost2_grid_model_15  0.9304210461129477  \n",
      "2   xgboost2_grid_model_19   0.926034297340874  \n",
      "3   xgboost2_grid_model_10  0.9084445151674854  \n",
      "4    xgboost2_grid_model_3  0.8980871612888605  \n",
      "5    xgboost2_grid_model_5  0.8977045035290406  \n",
      "6   xgboost2_grid_model_12   0.895187238186867  \n",
      "7    xgboost2_grid_model_6  0.8950423641300965  \n",
      "8   xgboost2_grid_model_17   0.894723842308332  \n",
      "9    xgboost2_grid_model_1  0.8914069143086792  \n",
      "10  xgboost2_grid_model_20  0.8902676778429998  \n",
      "11   xgboost2_grid_model_2  0.8873073344226952  \n",
      "12  xgboost2_grid_model_11  0.8863639309086254  \n",
      "13  xgboost2_grid_model_13  0.8764047484598853  \n",
      "14   xgboost2_grid_model_8  0.8752534809196223  \n",
      "15  xgboost2_grid_model_18  0.8645713039878112  \n",
      "16   xgboost2_grid_model_7  0.8645504850256375  \n",
      "17   xgboost2_grid_model_9                 0.5  \n",
      "18  xgboost2_grid_model_14                 0.5  \n",
      "19   xgboost2_grid_model_4                 0.5  \n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### RESULTS OF GRID SEARCH NOT INCLUDING LEARN RATE AS A HP ######\n",
    "## Get the grid results, sorted by validation AUC (pasted above)\n",
    "xgboost2_grid_performance = xgboost2_grid.get_grid(sort_by='auc', decreasing=True)\n",
    "xgboost2_grid_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Here I perform another (simpler) search over the space, tuning only learn_rate and keeping the other model params constant at their 'best' values. I do this manually over a loop.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n",
      "xgboost Model Build progress: |███████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Learn rate must lie between 0 and 1\n",
    "learn_rates = np.arange(0.1,1,0.1).tolist()\n",
    "models = []\n",
    "for learn_rate in learn_rates:\n",
    "    xgboost3_params = {\n",
    "          'seed':2020\n",
    "        , 'distribution':'bernoulli'  \n",
    "        , 'categorical_encoding':'AUTO' \n",
    "        , 'keep_cross_validation_models':False\n",
    "        , 'keep_cross_validation_predictions':False\n",
    "        , 'keep_cross_validation_fold_assignment':False\n",
    "        , 'score_tree_interval': 20\n",
    "        , 'tree_method': 'auto'\n",
    "        ### keep best values of tuned hyper-parameters\n",
    "        , 'col_sample_rate_per_tree':0.65\n",
    "        , 'max_depth':10\n",
    "        , 'min_rows': 2.0\n",
    "        , 'ntrees': 150\n",
    "        , 'sample_rate': 0.9\n",
    "        , 'learn_rate': learn_rate\n",
    "    }\n",
    "    model=H2OXGBoostEstimator(**xgboost3_params)\n",
    "    model.train(x=features, \n",
    "                   y='target',\n",
    "                   training_frame=h2o_df_train_125k[idx_h2o_train_125k, :],\n",
    "                   validation_frame=h2o_df_train_125k[idx_h2o_val_125k, :])\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1012\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.09354557242221177\n",
      "RMSE: 0.3058522068290693\n",
      "LogLoss: 0.30579088876339366\n",
      "Mean Per-Class Error: 0.12359376051651738\n",
      "AUC: 0.9535929857191447\n",
      "pr_auc: 0.9441568924216154\n",
      "Gini: 0.9071859714382895\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.47670960628870623: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>43338.0</td>\n",
       "<td>7604.0</td>\n",
       "<td>0.1493</td>\n",
       "<td> (7604.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>4940.0</td>\n",
       "<td>44280.0</td>\n",
       "<td>0.1004</td>\n",
       "<td> (4940.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>48278.0</td>\n",
       "<td>51884.0</td>\n",
       "<td>0.1252</td>\n",
       "<td> (12544.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ------------------\n",
       "0      43338  7604   0.1493   (7604.0/50942.0)\n",
       "1      4940   44280  0.1004   (4940.0/49220.0)\n",
       "Total  48278  51884  0.1252   (12544.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4767096</td>\n",
       "<td>0.8759297</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3041665</td>\n",
       "<td>0.9207089</td>\n",
       "<td>273.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6346075</td>\n",
       "<td>0.8902032</td>\n",
       "<td>152.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5152352</td>\n",
       "<td>0.8764601</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9979674</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0270720</td>\n",
       "<td>1.0</td>\n",
       "<td>383.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9979674</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5152352</td>\n",
       "<td>0.7528353</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5088341</td>\n",
       "<td>0.8758025</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5120260</td>\n",
       "<td>0.8764062</td>\n",
       "<td>197.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.47671      0.87593   211\n",
       "max f2                       0.304167     0.920709  273\n",
       "max f0point5                 0.634607     0.890203  152\n",
       "max accuracy                 0.515235     0.87646   196\n",
       "max precision                0.997967     1         0\n",
       "max recall                   0.027072     1         383\n",
       "max specificity              0.997967     1         0\n",
       "max absolute_mcc             0.515235     0.752835  196\n",
       "max min_per_class_accuracy   0.508834     0.875803  198\n",
       "max mean_per_class_accuracy  0.512026     0.876406  197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.15 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100038</td>\n",
       "<td>0.9942631</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9967356</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9967356</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0203576</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200076</td>\n",
       "<td>0.9881935</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9912460</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9939908</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0407152</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300014</td>\n",
       "<td>0.9824462</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9852516</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9910796</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0610524</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9773281</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9798182</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9882636</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9726705</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9749773</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9856058</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9468073</td>\n",
       "<td>2.0321413</td>\n",
       "<td>2.0335637</td>\n",
       "<td>0.9986022</td>\n",
       "<td>0.9601004</td>\n",
       "<td>0.9993012</td>\n",
       "<td>0.9728544</td>\n",
       "<td>0.1016050</td>\n",
       "<td>0.2033726</td>\n",
       "<td>103.2141349</td>\n",
       "<td>103.3563706</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9153814</td>\n",
       "<td>2.0203573</td>\n",
       "<td>2.0291619</td>\n",
       "<td>0.9928115</td>\n",
       "<td>0.9315505</td>\n",
       "<td>0.9971381</td>\n",
       "<td>0.9590873</td>\n",
       "<td>0.1010158</td>\n",
       "<td>0.3043885</td>\n",
       "<td>102.0357286</td>\n",
       "<td>102.9161859</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.8776756</td>\n",
       "<td>1.9915066</td>\n",
       "<td>2.0197485</td>\n",
       "<td>0.9786342</td>\n",
       "<td>0.8972100</td>\n",
       "<td>0.9925124</td>\n",
       "<td>0.9436188</td>\n",
       "<td>0.0995733</td>\n",
       "<td>0.4039618</td>\n",
       "<td>99.1506649</td>\n",
       "<td>101.9748526</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.7695903</td>\n",
       "<td>1.8911389</td>\n",
       "<td>1.9768801</td>\n",
       "<td>0.9293131</td>\n",
       "<td>0.8265875</td>\n",
       "<td>0.9714466</td>\n",
       "<td>0.9046097</td>\n",
       "<td>0.1891101</td>\n",
       "<td>0.5930719</td>\n",
       "<td>89.1138940</td>\n",
       "<td>97.6880091</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.6341313</td>\n",
       "<td>1.6475339</td>\n",
       "<td>1.8945456</td>\n",
       "<td>0.8096046</td>\n",
       "<td>0.7030876</td>\n",
       "<td>0.9309871</td>\n",
       "<td>0.8542304</td>\n",
       "<td>0.1647501</td>\n",
       "<td>0.7578220</td>\n",
       "<td>64.7533913</td>\n",
       "<td>89.4545602</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4984577</td>\n",
       "<td>1.2480948</td>\n",
       "<td>1.7652580</td>\n",
       "<td>0.6133187</td>\n",
       "<td>0.5661625</td>\n",
       "<td>0.8674547</td>\n",
       "<td>0.7966180</td>\n",
       "<td>0.1248070</td>\n",
       "<td>0.8826290</td>\n",
       "<td>24.8094812</td>\n",
       "<td>76.5258025</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.3616055</td>\n",
       "<td>0.7509293</td>\n",
       "<td>1.5962060</td>\n",
       "<td>0.3690096</td>\n",
       "<td>0.4315864</td>\n",
       "<td>0.7843819</td>\n",
       "<td>0.7357804</td>\n",
       "<td>0.0750914</td>\n",
       "<td>0.9577204</td>\n",
       "<td>-24.9070743</td>\n",
       "<td>59.6206043</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.2201389</td>\n",
       "<td>0.3427537</td>\n",
       "<td>1.4171440</td>\n",
       "<td>0.1684305</td>\n",
       "<td>0.2903365</td>\n",
       "<td>0.6963901</td>\n",
       "<td>0.6721465</td>\n",
       "<td>0.0342747</td>\n",
       "<td>0.9919951</td>\n",
       "<td>-65.7246305</td>\n",
       "<td>41.7143976</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0951741</td>\n",
       "<td>0.0776123</td>\n",
       "<td>1.2497046</td>\n",
       "<td>0.0381390</td>\n",
       "<td>0.1548787</td>\n",
       "<td>0.6141097</td>\n",
       "<td>0.6074888</td>\n",
       "<td>0.0077611</td>\n",
       "<td>0.9997562</td>\n",
       "<td>-92.2387723</td>\n",
       "<td>24.9704603</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0164546</td>\n",
       "<td>0.0024381</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0011981</td>\n",
       "<td>0.0505560</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5456081</td>\n",
       "<td>0.0002438</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.7561918</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000174</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0048516</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4915281</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100038                   0.994263           2.03499     2.03499            1                0.996736    1                           0.996736            0.0203576       0.0203576                  103.499   103.499\n",
       "    2        0.0200076                   0.988193           2.03499     2.03499            1                0.991246    1                           0.993991            0.0203576       0.0407152                  103.499   103.499\n",
       "    3        0.0300014                   0.982446           2.03499     2.03499            1                0.985252    1                           0.99108             0.0203373       0.0610524                  103.499   103.499\n",
       "    4        0.0400052                   0.977328           2.03499     2.03499            1                0.979818    1                           0.988264            0.0203576       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.972671           2.03499     2.03499            1                0.974977    1                           0.985606            0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.946807           2.03214     2.03356            0.998602         0.9601      0.999301                    0.972854            0.101605        0.203373                   103.214   103.356\n",
       "    7        0.150007                    0.915381           2.02036     2.02916            0.992812         0.93155     0.997138                    0.959087            0.101016        0.304388                   102.036   102.916\n",
       "    8        0.200006                    0.877676           1.99151     2.01975            0.978634         0.89721     0.992512                    0.943619            0.0995733       0.403962                   99.1507   101.975\n",
       "    9        0.300004                    0.76959            1.89114     1.97688            0.929313         0.826588    0.971447                    0.90461             0.18911         0.593072                   89.1139   97.688\n",
       "    10       0.400002                    0.634131           1.64753     1.89455            0.809605         0.703088    0.930987                    0.85423             0.16475         0.757822                   64.7534   89.4546\n",
       "    11       0.5                         0.498458           1.24809     1.76526            0.613319         0.566163    0.867455                    0.796618            0.124807        0.882629                   24.8095   76.5258\n",
       "    12       0.599998                    0.361606           0.750929    1.59621            0.36901          0.431586    0.784382                    0.73578             0.0750914       0.95772                    -24.9071  59.6206\n",
       "    13       0.699996                    0.220139           0.342754    1.41714            0.168431         0.290337    0.69639                     0.672146            0.0342747       0.991995                   -65.7246  41.7144\n",
       "    14       0.799994                    0.0951741          0.0776123   1.2497             0.038139         0.154879    0.61411                     0.607489            0.00776107      0.999756                   -92.2388  24.9705\n",
       "    15       0.899992                    0.0164546          0.00243808  1.11112            0.00119808       0.050556    0.546009                    0.545608            0.000243803     1                          -99.7562  11.1121\n",
       "    16       1                           1.74175e-05        0           1                  0                0.00485161  0.491404                    0.491528            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.1178414352277755\n",
      "RMSE: 0.3432804032096436\n",
      "LogLoss: 0.36884989958813535\n",
      "Mean Per-Class Error: 0.16759240609042425\n",
      "AUC: 0.9168209170238907\n",
      "pr_auc: 0.9014740447343367\n",
      "Gini: 0.8336418340477814\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4262963807582855: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>9761.0</td>\n",
       "<td>2984.0</td>\n",
       "<td>0.2341</td>\n",
       "<td> (2984.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1383.0</td>\n",
       "<td>10907.0</td>\n",
       "<td>0.1125</td>\n",
       "<td> (1383.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11144.0</td>\n",
       "<td>13891.0</td>\n",
       "<td>0.1744</td>\n",
       "<td> (4367.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      9761   2984   0.2341   (2984.0/12745.0)\n",
       "1      1383   10907  0.1125   (1383.0/12290.0)\n",
       "Total  11144  13891  0.1744   (4367.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4262964</td>\n",
       "<td>0.8331996</td>\n",
       "<td>227.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2391395</td>\n",
       "<td>0.8953326</td>\n",
       "<td>297.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6370870</td>\n",
       "<td>0.8483507</td>\n",
       "<td>149.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5143919</td>\n",
       "<td>0.8325544</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9976654</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0010746</td>\n",
       "<td>1.0</td>\n",
       "<td>398.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9976654</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5143919</td>\n",
       "<td>0.6649706</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5053631</td>\n",
       "<td>0.8316987</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5143919</td>\n",
       "<td>0.8324076</td>\n",
       "<td>194.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.426296     0.8332    227\n",
       "max f2                       0.23914      0.895333  297\n",
       "max f0point5                 0.637087     0.848351  149\n",
       "max accuracy                 0.514392     0.832554  194\n",
       "max precision                0.997665     1         0\n",
       "max recall                   0.00107464   1         398\n",
       "max specificity              0.997665     1         0\n",
       "max absolute_mcc             0.514392     0.664971  194\n",
       "max min_per_class_accuracy   0.505363     0.831699  198\n",
       "max mean_per_class_accuracy  0.514392     0.832408  194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.01 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9943873</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0370220</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9967836</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9967836</td>\n",
       "<td>0.0204231</td>\n",
       "<td>0.0204231</td>\n",
       "<td>103.7021969</td>\n",
       "<td>103.7021969</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9883857</td>\n",
       "<td>2.0207258</td>\n",
       "<td>2.0288901</td>\n",
       "<td>0.992</td>\n",
       "<td>0.9913489</td>\n",
       "<td>0.9960080</td>\n",
       "<td>0.9940717</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.0406021</td>\n",
       "<td>102.0725793</td>\n",
       "<td>102.8890145</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9825132</td>\n",
       "<td>2.0289063</td>\n",
       "<td>2.0288956</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9852806</td>\n",
       "<td>0.9960106</td>\n",
       "<td>0.9911374</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0609439</td>\n",
       "<td>102.8906344</td>\n",
       "<td>102.8895552</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9772828</td>\n",
       "<td>2.0288739</td>\n",
       "<td>2.0288901</td>\n",
       "<td>0.996</td>\n",
       "<td>0.9796268</td>\n",
       "<td>0.9960080</td>\n",
       "<td>0.9882655</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0812042</td>\n",
       "<td>102.8873881</td>\n",
       "<td>102.8890145</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9720831</td>\n",
       "<td>2.0125777</td>\n",
       "<td>2.0256329</td>\n",
       "<td>0.988</td>\n",
       "<td>0.9747272</td>\n",
       "<td>0.9944089</td>\n",
       "<td>0.9855622</td>\n",
       "<td>0.0200976</td>\n",
       "<td>0.1013019</td>\n",
       "<td>101.2577705</td>\n",
       "<td>102.5632869</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9450651</td>\n",
       "<td>2.0044817</td>\n",
       "<td>2.0150573</td>\n",
       "<td>0.9840256</td>\n",
       "<td>0.9588159</td>\n",
       "<td>0.9892173</td>\n",
       "<td>0.9721890</td>\n",
       "<td>0.1002441</td>\n",
       "<td>0.2015460</td>\n",
       "<td>100.4481682</td>\n",
       "<td>101.5057275</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9117140</td>\n",
       "<td>1.9459092</td>\n",
       "<td>1.9920079</td>\n",
       "<td>0.9552716</td>\n",
       "<td>0.9287950</td>\n",
       "<td>0.9779020</td>\n",
       "<td>0.9577244</td>\n",
       "<td>0.0973149</td>\n",
       "<td>0.2988609</td>\n",
       "<td>94.5909165</td>\n",
       "<td>99.2007905</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.8700055</td>\n",
       "<td>1.8986152</td>\n",
       "<td>1.9686737</td>\n",
       "<td>0.9320544</td>\n",
       "<td>0.8921718</td>\n",
       "<td>0.9664470</td>\n",
       "<td>0.9413461</td>\n",
       "<td>0.0948739</td>\n",
       "<td>0.3937347</td>\n",
       "<td>89.8615201</td>\n",
       "<td>96.8673718</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.7636135</td>\n",
       "<td>1.7441594</td>\n",
       "<td>1.8938256</td>\n",
       "<td>0.8562300</td>\n",
       "<td>0.8203327</td>\n",
       "<td>0.9297031</td>\n",
       "<td>0.9010029</td>\n",
       "<td>0.1744508</td>\n",
       "<td>0.5681855</td>\n",
       "<td>74.4159386</td>\n",
       "<td>89.3825644</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6261272</td>\n",
       "<td>1.5373290</td>\n",
       "<td>1.8047193</td>\n",
       "<td>0.7546944</td>\n",
       "<td>0.6956474</td>\n",
       "<td>0.8859597</td>\n",
       "<td>0.8496743</td>\n",
       "<td>0.1537022</td>\n",
       "<td>0.7218877</td>\n",
       "<td>53.7329005</td>\n",
       "<td>80.4719284</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4950879</td>\n",
       "<td>1.1576207</td>\n",
       "<td>1.6752789</td>\n",
       "<td>0.5682907</td>\n",
       "<td>0.5596233</td>\n",
       "<td>0.8224157</td>\n",
       "<td>0.7916548</td>\n",
       "<td>0.1157852</td>\n",
       "<td>0.8376729</td>\n",
       "<td>15.7620712</td>\n",
       "<td>67.5278892</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3645480</td>\n",
       "<td>0.8260397</td>\n",
       "<td>1.5337673</td>\n",
       "<td>0.4055134</td>\n",
       "<td>0.4309747</td>\n",
       "<td>0.7529459</td>\n",
       "<td>0.7315535</td>\n",
       "<td>0.0825875</td>\n",
       "<td>0.9202604</td>\n",
       "<td>-17.3960328</td>\n",
       "<td>53.3767290</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.2236411</td>\n",
       "<td>0.5053898</td>\n",
       "<td>1.3868814</td>\n",
       "<td>0.2481023</td>\n",
       "<td>0.2937635</td>\n",
       "<td>0.6808377</td>\n",
       "<td>0.6690227</td>\n",
       "<td>0.0505289</td>\n",
       "<td>0.9707893</td>\n",
       "<td>-49.4610211</td>\n",
       "<td>38.6881369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0999433</td>\n",
       "<td>0.2131389</td>\n",
       "<td>1.2401343</td>\n",
       "<td>0.1046326</td>\n",
       "<td>0.1603799</td>\n",
       "<td>0.6087977</td>\n",
       "<td>0.6054297</td>\n",
       "<td>0.0213181</td>\n",
       "<td>0.9921074</td>\n",
       "<td>-78.6861120</td>\n",
       "<td>24.0134255</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0173507</td>\n",
       "<td>0.0716172</td>\n",
       "<td>1.1103221</td>\n",
       "<td>0.0351578</td>\n",
       "<td>0.0524907</td>\n",
       "<td>0.5450712</td>\n",
       "<td>0.5440029</td>\n",
       "<td>0.0071603</td>\n",
       "<td>0.9992677</td>\n",
       "<td>-92.8382767</td>\n",
       "<td>11.0322081</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000174</td>\n",
       "<td>0.0073216</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0035942</td>\n",
       "<td>0.0050480</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4900967</td>\n",
       "<td>0.0007323</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.2678435</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.994387           2.03702     2.03702            1                0.996784    1                           0.996784            0.0204231       0.0204231                  103.702   103.702\n",
       "    2        0.020012                    0.988386           2.02073     2.02889            0.992            0.991349    0.996008                    0.994072            0.020179        0.0406021                  102.073   102.889\n",
       "    3        0.0300379                   0.982513           2.02891     2.0289             0.996016         0.985281    0.996011                    0.991137            0.0203417       0.0609439                  102.891   102.89\n",
       "    4        0.040024                    0.977283           2.02887     2.02889            0.996            0.979627    0.996008                    0.988265            0.0202604       0.0812042                  102.887   102.889\n",
       "    5        0.05001                     0.972083           2.01258     2.02563            0.988            0.974727    0.994409                    0.985562            0.0200976       0.101302                   101.258   102.563\n",
       "    6        0.10002                     0.945065           2.00448     2.01506            0.984026         0.958816    0.989217                    0.972189            0.100244        0.201546                   100.448   101.506\n",
       "    7        0.15003                     0.911714           1.94591     1.99201            0.955272         0.928795    0.977902                    0.957724            0.0973149       0.298861                   94.5909   99.2008\n",
       "    8        0.2                         0.870005           1.89862     1.96867            0.932054         0.892172    0.966447                    0.941346            0.0948739       0.393735                   89.8615   96.8674\n",
       "    9        0.30002                     0.763614           1.74416     1.89383            0.85623          0.820333    0.929703                    0.901003            0.174451        0.568186                   74.4159   89.3826\n",
       "    10       0.4                         0.626127           1.53733     1.80472            0.754694         0.695647    0.88596                     0.849674            0.153702        0.721888                   53.7329   80.4719\n",
       "    11       0.50002                     0.495088           1.15762     1.67528            0.568291         0.559623    0.822416                    0.791655            0.115785        0.837673                   15.7621   67.5279\n",
       "    12       0.6                         0.364548           0.82604     1.53377            0.405513         0.430975    0.752946                    0.731553            0.0825875       0.92026                    -17.396   53.3767\n",
       "    13       0.69998                     0.223641           0.50539     1.38688            0.248102         0.293764    0.680838                    0.669023            0.0505289       0.970789                   -49.461   38.6881\n",
       "    14       0.8                         0.0999433          0.213139    1.24013            0.104633         0.16038     0.608798                    0.60543             0.0213181       0.992107                   -78.6861  24.0134\n",
       "    15       0.89998                     0.0173507          0.0716172   1.11032            0.0351578        0.0524907   0.545071                    0.544003            0.00716029      0.999268                   -92.8383  11.0322\n",
       "    16       1                           1.74175e-05        0.00732156  1                  0.00359425       0.00504798  0.490913                    0.490097            0.000732303     1                          -99.2678  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:34</td>\n",
       "<td> 0.047 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:38</td>\n",
       "<td> 3.991 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3674312</td>\n",
       "<td>0.4310445</td>\n",
       "<td>0.9053114</td>\n",
       "<td>0.9005776</td>\n",
       "<td>2.0309239</td>\n",
       "<td>0.1922486</td>\n",
       "<td>0.3759176</td>\n",
       "<td>0.4457231</td>\n",
       "<td>0.8921378</td>\n",
       "<td>0.8860970</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.2047134</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:42</td>\n",
       "<td> 7.860 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.3483080</td>\n",
       "<td>0.3829164</td>\n",
       "<td>0.9165286</td>\n",
       "<td>0.9137200</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1763443</td>\n",
       "<td>0.3621815</td>\n",
       "<td>0.4079378</td>\n",
       "<td>0.8990760</td>\n",
       "<td>0.8945904</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1967246</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:44</td>\n",
       "<td>10.316 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.3397867</td>\n",
       "<td>0.3633411</td>\n",
       "<td>0.9238699</td>\n",
       "<td>0.9200045</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1687666</td>\n",
       "<td>0.3573377</td>\n",
       "<td>0.3954406</td>\n",
       "<td>0.9032355</td>\n",
       "<td>0.8980413</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1920911</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:47</td>\n",
       "<td>12.725 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.3330877</td>\n",
       "<td>0.3501380</td>\n",
       "<td>0.9299594</td>\n",
       "<td>0.9249111</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1597911</td>\n",
       "<td>0.3540954</td>\n",
       "<td>0.3884278</td>\n",
       "<td>0.9062930</td>\n",
       "<td>0.9022174</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1886958</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:49</td>\n",
       "<td>15.184 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.3252526</td>\n",
       "<td>0.3368018</td>\n",
       "<td>0.9372519</td>\n",
       "<td>0.9302809</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1509954</td>\n",
       "<td>0.3507254</td>\n",
       "<td>0.3821018</td>\n",
       "<td>0.9096918</td>\n",
       "<td>0.9006001</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1853006</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:52</td>\n",
       "<td>17.537 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.3174997</td>\n",
       "<td>0.3241237</td>\n",
       "<td>0.9440517</td>\n",
       "<td>0.9362832</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1424692</td>\n",
       "<td>0.3477673</td>\n",
       "<td>0.3767624</td>\n",
       "<td>0.9125550</td>\n",
       "<td>0.9021820</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1803875</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:54</td>\n",
       "<td>19.929 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.3097176</td>\n",
       "<td>0.3118956</td>\n",
       "<td>0.9505316</td>\n",
       "<td>0.9405224</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1296500</td>\n",
       "<td>0.3447153</td>\n",
       "<td>0.3714671</td>\n",
       "<td>0.9154634</td>\n",
       "<td>0.9074377</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1755542</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:55</td>\n",
       "<td>21.489 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.3058522</td>\n",
       "<td>0.3057909</td>\n",
       "<td>0.9535930</td>\n",
       "<td>0.9441569</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1252371</td>\n",
       "<td>0.3432804</td>\n",
       "<td>0.3688499</td>\n",
       "<td>0.9168209</td>\n",
       "<td>0.9014740</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1744358</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:37:34  0.047 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:37:38  3.991 sec   20                 0.367431         0.431044            0.905311        0.900578           2.03092          0.192249                         0.375918           0.445723              0.892138          0.886097             2.02079            0.204713\n",
       "    2020-07-16 22:37:42  7.860 sec   40                 0.348308         0.382916            0.916529        0.91372            2.03499          0.176344                         0.362181           0.407938              0.899076          0.89459              2.03702            0.196725\n",
       "    2020-07-16 22:37:44  10.316 sec  60                 0.339787         0.363341            0.92387         0.920005           2.03499          0.168767                         0.357338           0.395441              0.903236          0.898041             2.03702            0.192091\n",
       "    2020-07-16 22:37:47  12.725 sec  80                 0.333088         0.350138            0.929959        0.924911           2.03499          0.159791                         0.354095           0.388428              0.906293          0.902217             2.03702            0.188696\n",
       "    2020-07-16 22:37:49  15.184 sec  100                0.325253         0.336802            0.937252        0.930281           2.03499          0.150995                         0.350725           0.382102              0.909692          0.9006               2.03702            0.185301\n",
       "    2020-07-16 22:37:52  17.537 sec  120                0.3175           0.324124            0.944052        0.936283           2.03499          0.142469                         0.347767           0.376762              0.912555          0.902182             2.03702            0.180387\n",
       "    2020-07-16 22:37:54  19.929 sec  140                0.309718         0.311896            0.950532        0.940522           2.03499          0.12965                          0.344715           0.371467              0.915463          0.907438             2.03702            0.175554\n",
       "    2020-07-16 22:37:55  21.489 sec  150                0.305852         0.305791            0.953593        0.944157           2.03499          0.125237                         0.34328            0.36885               0.916821          0.901474             2.03702            0.174436"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>176259.7812500</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4611761</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>27608.6015625</td>\n",
       "<td>0.1566359</td>\n",
       "<td>0.0722367</td></tr>\n",
       "<tr><td>e18</td>\n",
       "<td>10177.3300781</td>\n",
       "<td>0.0577405</td>\n",
       "<td>0.0266285</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>10037.8730469</td>\n",
       "<td>0.0569493</td>\n",
       "<td>0.0262637</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>7784.0205078</td>\n",
       "<td>0.0441622</td>\n",
       "<td>0.0203666</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>a12.D</td>\n",
       "<td>2.9107218</td>\n",
       "<td>0.0000165</td>\n",
       "<td>0.0000076</td></tr>\n",
       "<tr><td>f09.H</td>\n",
       "<td>2.3391256</td>\n",
       "<td>0.0000133</td>\n",
       "<td>0.0000061</td></tr>\n",
       "<tr><td>e20.40EE1</td>\n",
       "<td>2.0217814</td>\n",
       "<td>0.0000115</td>\n",
       "<td>0.0000053</td></tr>\n",
       "<tr><td>e20.F3D59</td>\n",
       "<td>1.6984112</td>\n",
       "<td>0.0000096</td>\n",
       "<td>0.0000044</td></tr>\n",
       "<tr><td>c06.C</td>\n",
       "<td>1.1313248</td>\n",
       "<td>0.0000064</td>\n",
       "<td>0.0000030</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         176259.78125           1.0                     0.46117608832771534\n",
       "e19         27608.6015625          0.15663585513782657     0.07223671096432953\n",
       "e18         10177.330078125        0.057740512361636666    0.02662854362897769\n",
       "f02         10037.873046875        0.05694931070314715     0.026263660343037093\n",
       "e12         7784.0205078125        0.044162204517728       0.02036655273141436\n",
       "---         ---                    ---                     ---\n",
       "a12.D       2.910721778869629      1.6513817038846626e-05  7.615777545334861e-06\n",
       "f09.H       2.339125633239746      1.327089831072706e-05   6.120220971535991e-06\n",
       "e20.40EE1   2.0217814445495605     1.1470463824540577e-05  5.289903637906188e-06\n",
       "e20.F3D59   1.698411226272583      9.635840996895501e-06   4.4438194586961e-06\n",
       "c06.C       1.1313247680664062     6.418507727873435e-06   2.9600622868418826e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1031\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.06454280456711209\n",
      "RMSE: 0.25405275941644895\n",
      "LogLoss: 0.23053650846408238\n",
      "Mean Per-Class Error: 0.07054182022560063\n",
      "AUC: 0.9828674389695217\n",
      "pr_auc: 0.9684933660463567\n",
      "Gini: 0.9657348779390433\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.49097249696129247: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>46788.0</td>\n",
       "<td>4154.0</td>\n",
       "<td>0.0815</td>\n",
       "<td> (4154.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>2942.0</td>\n",
       "<td>46278.0</td>\n",
       "<td>0.0598</td>\n",
       "<td> (2942.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>49730.0</td>\n",
       "<td>50432.0</td>\n",
       "<td>0.0708</td>\n",
       "<td> (7096.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      46788  4154   0.0815   (4154.0/50942.0)\n",
       "1      2942   46278  0.0598   (2942.0/49220.0)\n",
       "Total  49730  50432  0.0708   (7096.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4909725</td>\n",
       "<td>0.9287922</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.3829865</td>\n",
       "<td>0.9515038</td>\n",
       "<td>242.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6213083</td>\n",
       "<td>0.9396481</td>\n",
       "<td>160.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5070259</td>\n",
       "<td>0.9294044</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9991538</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0641181</td>\n",
       "<td>1.0</td>\n",
       "<td>364.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9991538</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5070259</td>\n",
       "<td>0.8588205</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5129671</td>\n",
       "<td>0.9291940</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5070259</td>\n",
       "<td>0.9294582</td>\n",
       "<td>199.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.490972     0.928792  204\n",
       "max f2                       0.382987     0.951504  242\n",
       "max f0point5                 0.621308     0.939648  160\n",
       "max accuracy                 0.507026     0.929404  199\n",
       "max precision                0.999154     1         0\n",
       "max recall                   0.0641181    1         364\n",
       "max specificity              0.999154     1         0\n",
       "max absolute_mcc             0.507026     0.85882   199\n",
       "max min_per_class_accuracy   0.512967     0.929194  197\n",
       "max mean_per_class_accuracy  0.507026     0.929458  199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.15 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100038</td>\n",
       "<td>0.9979125</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9988857</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9988857</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0203576</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200076</td>\n",
       "<td>0.9951941</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9965742</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9977300</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0407152</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300114</td>\n",
       "<td>0.9920387</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9936123</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9963574</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0610727</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9888048</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9904471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9948809</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9853792</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9870868</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9933218</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9662292</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9761648</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9847441</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9414490</td>\n",
       "<td>2.0337667</td>\n",
       "<td>2.0345795</td>\n",
       "<td>0.9994010</td>\n",
       "<td>0.9544966</td>\n",
       "<td>0.9998003</td>\n",
       "<td>0.9746623</td>\n",
       "<td>0.1016863</td>\n",
       "<td>0.3052011</td>\n",
       "<td>103.3766737</td>\n",
       "<td>103.4579458</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9096408</td>\n",
       "<td>2.0317350</td>\n",
       "<td>2.0338684</td>\n",
       "<td>0.9984026</td>\n",
       "<td>0.9260169</td>\n",
       "<td>0.9994509</td>\n",
       "<td>0.9625016</td>\n",
       "<td>0.1015847</td>\n",
       "<td>0.4067859</td>\n",
       "<td>103.1735002</td>\n",
       "<td>103.3868380</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.8159475</td>\n",
       "<td>2.0028844</td>\n",
       "<td>2.0235407</td>\n",
       "<td>0.9842252</td>\n",
       "<td>0.8664464</td>\n",
       "<td>0.9943759</td>\n",
       "<td>0.9304843</td>\n",
       "<td>0.2002844</td>\n",
       "<td>0.6070703</td>\n",
       "<td>100.2884365</td>\n",
       "<td>102.3540719</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.6814535</td>\n",
       "<td>1.8844342</td>\n",
       "<td>1.9887650</td>\n",
       "<td>0.9260184</td>\n",
       "<td>0.7523910</td>\n",
       "<td>0.9772869</td>\n",
       "<td>0.8859621</td>\n",
       "<td>0.1884397</td>\n",
       "<td>0.7955100</td>\n",
       "<td>88.4434214</td>\n",
       "<td>98.8764961</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4969136</td>\n",
       "<td>1.4112431</td>\n",
       "<td>1.8732629</td>\n",
       "<td>0.6934904</td>\n",
       "<td>0.5927319</td>\n",
       "<td>0.9205287</td>\n",
       "<td>0.8273172</td>\n",
       "<td>0.1411215</td>\n",
       "<td>0.9366315</td>\n",
       "<td>41.1243132</td>\n",
       "<td>87.3262901</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.3107564</td>\n",
       "<td>0.5355653</td>\n",
       "<td>1.6503170</td>\n",
       "<td>0.2631789</td>\n",
       "<td>0.4014191</td>\n",
       "<td>0.8109723</td>\n",
       "<td>0.7563354</td>\n",
       "<td>0.0535555</td>\n",
       "<td>0.9901869</td>\n",
       "<td>-46.4434653</td>\n",
       "<td>65.0317019</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.1642412</td>\n",
       "<td>0.0900059</td>\n",
       "<td>1.4274186</td>\n",
       "<td>0.0442292</td>\n",
       "<td>0.2344992</td>\n",
       "<td>0.7014391</td>\n",
       "<td>0.6817884</td>\n",
       "<td>0.0090004</td>\n",
       "<td>0.9991873</td>\n",
       "<td>-90.9994139</td>\n",
       "<td>42.7418604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0622871</td>\n",
       "<td>0.0081269</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0039936</td>\n",
       "<td>0.1096833</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6102762</td>\n",
       "<td>0.0008127</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.1873060</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0084095</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0306377</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5458726</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000003</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0022616</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4915072</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100038                   0.997913           2.03499     2.03499            1                0.998886    1                           0.998886            0.0203576       0.0203576                  103.499   103.499\n",
       "    2        0.0200076                   0.995194           2.03499     2.03499            1                0.996574    1                           0.99773             0.0203576       0.0407152                  103.499   103.499\n",
       "    3        0.0300114                   0.992039           2.03499     2.03499            1                0.993612    1                           0.996357            0.0203576       0.0610727                  103.499   103.499\n",
       "    4        0.0400052                   0.988805           2.03499     2.03499            1                0.990447    1                           0.994881            0.0203373       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.985379           2.03499     2.03499            1                0.987087    1                           0.993322            0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.966229           2.03499     2.03499            1                0.976165    1                           0.984744            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.941449           2.03377     2.03458            0.999401         0.954497    0.9998                      0.974662            0.101686        0.305201                   103.377   103.458\n",
       "    8        0.200006                    0.909641           2.03174     2.03387            0.998403         0.926017    0.999451                    0.962502            0.101585        0.406786                   103.174   103.387\n",
       "    9        0.300004                    0.815947           2.00288     2.02354            0.984225         0.866446    0.994376                    0.930484            0.200284        0.60707                    100.288   102.354\n",
       "    10       0.400002                    0.681454           1.88443     1.98876            0.926018         0.752391    0.977287                    0.885962            0.18844         0.79551                    88.4434   98.8765\n",
       "    11       0.5                         0.496914           1.41124     1.87326            0.69349          0.592732    0.920529                    0.827317            0.141121        0.936631                   41.1243   87.3263\n",
       "    12       0.599998                    0.310756           0.535565    1.65032            0.263179         0.401419    0.810972                    0.756335            0.0535555       0.990187                   -46.4435  65.0317\n",
       "    13       0.699996                    0.164241           0.0900059   1.42742            0.0442292        0.234499    0.701439                    0.681788            0.00900041      0.999187                   -90.9994  42.7419\n",
       "    14       0.799994                    0.0622871          0.00812694  1.25001            0.00399361       0.109683    0.61426                     0.610276            0.000812678     1                          -99.1873  25.0009\n",
       "    15       0.899992                    0.00840947         0           1.11112            0                0.0306377   0.546009                    0.545873            0               1                          -100      11.1121\n",
       "    16       1                           3.49102e-07        0           1                  0                0.00226158  0.491404                    0.491507            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10689832775185219\n",
      "RMSE: 0.32695309717427695\n",
      "LogLoss: 0.3422374073490319\n",
      "Mean Per-Class Error: 0.14512490898487285\n",
      "AUC: 0.9292629697952675\n",
      "pr_auc: 0.9110623938247933\n",
      "Gini: 0.8585259395905349\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.462499076598569: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10530.0</td>\n",
       "<td>2215.0</td>\n",
       "<td>0.1738</td>\n",
       "<td> (2215.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1462.0</td>\n",
       "<td>10828.0</td>\n",
       "<td>0.119</td>\n",
       "<td> (1462.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>11992.0</td>\n",
       "<td>13043.0</td>\n",
       "<td>0.1469</td>\n",
       "<td> (3677.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10530  2215   0.1738   (2215.0/12745.0)\n",
       "1      1462   10828  0.119    (1462.0/12290.0)\n",
       "Total  11992  13043  0.1469   (3677.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4624991</td>\n",
       "<td>0.8548534</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2679921</td>\n",
       "<td>0.8990609</td>\n",
       "<td>285.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6756943</td>\n",
       "<td>0.8656198</td>\n",
       "<td>141.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4960995</td>\n",
       "<td>0.8547234</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9991497</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0002175</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9991497</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4960995</td>\n",
       "<td>0.7096381</td>\n",
       "<td>204.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5095034</td>\n",
       "<td>0.8525631</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4960995</td>\n",
       "<td>0.8548751</td>\n",
       "<td>204.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.462499     0.854853  215\n",
       "max f2                       0.267992     0.899061  285\n",
       "max f0point5                 0.675694     0.86562   141\n",
       "max accuracy                 0.4961       0.854723  204\n",
       "max precision                0.99915      1         0\n",
       "max recall                   0.000217465  1         399\n",
       "max specificity              0.99915      1         0\n",
       "max absolute_mcc             0.4961       0.709638  204\n",
       "max min_per_class_accuracy   0.509503     0.852563  199\n",
       "max mean_per_class_accuracy  0.4961       0.854875  204"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.08 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9980369</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0370220</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9989154</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9989154</td>\n",
       "<td>0.0204231</td>\n",
       "<td>0.0204231</td>\n",
       "<td>103.7021969</td>\n",
       "<td>103.7021969</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9952886</td>\n",
       "<td>2.0288739</td>\n",
       "<td>2.0329561</td>\n",
       "<td>0.996</td>\n",
       "<td>0.9966926</td>\n",
       "<td>0.9980040</td>\n",
       "<td>0.9978062</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0406835</td>\n",
       "<td>102.8873881</td>\n",
       "<td>103.2956057</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9917600</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0343132</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9935424</td>\n",
       "<td>0.9986702</td>\n",
       "<td>0.9963831</td>\n",
       "<td>0.0204231</td>\n",
       "<td>0.0611066</td>\n",
       "<td>103.7021969</td>\n",
       "<td>103.4313163</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9882628</td>\n",
       "<td>2.0044296</td>\n",
       "<td>2.0268572</td>\n",
       "<td>0.984</td>\n",
       "<td>0.9900465</td>\n",
       "<td>0.9950100</td>\n",
       "<td>0.9948021</td>\n",
       "<td>0.0200163</td>\n",
       "<td>0.0811229</td>\n",
       "<td>100.4429618</td>\n",
       "<td>102.6857189</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9847450</td>\n",
       "<td>2.0044296</td>\n",
       "<td>2.0223788</td>\n",
       "<td>0.984</td>\n",
       "<td>0.9864973</td>\n",
       "<td>0.9928115</td>\n",
       "<td>0.9931438</td>\n",
       "<td>0.0200163</td>\n",
       "<td>0.1011391</td>\n",
       "<td>100.4429618</td>\n",
       "<td>102.2378840</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9643894</td>\n",
       "<td>1.9947196</td>\n",
       "<td>2.0085492</td>\n",
       "<td>0.9792332</td>\n",
       "<td>0.9747702</td>\n",
       "<td>0.9860224</td>\n",
       "<td>0.9839570</td>\n",
       "<td>0.0997559</td>\n",
       "<td>0.2008950</td>\n",
       "<td>99.4719596</td>\n",
       "<td>100.8549218</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9364547</td>\n",
       "<td>1.9524172</td>\n",
       "<td>1.9898386</td>\n",
       "<td>0.9584665</td>\n",
       "<td>0.9510492</td>\n",
       "<td>0.9768371</td>\n",
       "<td>0.9729877</td>\n",
       "<td>0.0976404</td>\n",
       "<td>0.2985354</td>\n",
       "<td>95.2417223</td>\n",
       "<td>98.9838553</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9009472</td>\n",
       "<td>1.9165267</td>\n",
       "<td>1.9715216</td>\n",
       "<td>0.9408473</td>\n",
       "<td>0.9198659</td>\n",
       "<td>0.9678450</td>\n",
       "<td>0.9597152</td>\n",
       "<td>0.0957689</td>\n",
       "<td>0.3943043</td>\n",
       "<td>91.6526665</td>\n",
       "<td>97.1521562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.7996785</td>\n",
       "<td>1.8214426</td>\n",
       "<td>1.9214886</td>\n",
       "<td>0.8941693</td>\n",
       "<td>0.8539130</td>\n",
       "<td>0.9432832</td>\n",
       "<td>0.9244431</td>\n",
       "<td>0.1821806</td>\n",
       "<td>0.5764849</td>\n",
       "<td>82.1442567</td>\n",
       "<td>92.1488570</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.6609208</td>\n",
       "<td>1.5999941</td>\n",
       "<td>1.8411310</td>\n",
       "<td>0.7854575</td>\n",
       "<td>0.7331934</td>\n",
       "<td>0.9038346</td>\n",
       "<td>0.8766402</td>\n",
       "<td>0.1599675</td>\n",
       "<td>0.7364524</td>\n",
       "<td>59.9994084</td>\n",
       "<td>84.1131001</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4971811</td>\n",
       "<td>1.2438525</td>\n",
       "<td>1.7216562</td>\n",
       "<td>0.6106230</td>\n",
       "<td>0.5801120</td>\n",
       "<td>0.8451829</td>\n",
       "<td>0.8173251</td>\n",
       "<td>0.1244101</td>\n",
       "<td>0.8608625</td>\n",
       "<td>24.3852472</td>\n",
       "<td>72.1656210</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.3307639</td>\n",
       "<td>0.7373320</td>\n",
       "<td>1.5576349</td>\n",
       "<td>0.3619656</td>\n",
       "<td>0.4132085</td>\n",
       "<td>0.7646628</td>\n",
       "<td>0.7499858</td>\n",
       "<td>0.0737185</td>\n",
       "<td>0.9345810</td>\n",
       "<td>-26.2668037</td>\n",
       "<td>55.7634934</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.1788659</td>\n",
       "<td>0.3792458</td>\n",
       "<td>1.3893224</td>\n",
       "<td>0.1861766</td>\n",
       "<td>0.2516127</td>\n",
       "<td>0.6820361</td>\n",
       "<td>0.6788018</td>\n",
       "<td>0.0379170</td>\n",
       "<td>0.9724980</td>\n",
       "<td>-62.0754200</td>\n",
       "<td>38.9322448</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0682901</td>\n",
       "<td>0.1952417</td>\n",
       "<td>1.2400325</td>\n",
       "<td>0.0958466</td>\n",
       "<td>0.1207267</td>\n",
       "<td>0.6087478</td>\n",
       "<td>0.6090285</td>\n",
       "<td>0.0195281</td>\n",
       "<td>0.9920260</td>\n",
       "<td>-80.4758278</td>\n",
       "<td>24.0032547</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0089846</td>\n",
       "<td>0.0699896</td>\n",
       "<td>1.1100509</td>\n",
       "<td>0.0343588</td>\n",
       "<td>0.0333615</td>\n",
       "<td>0.5449381</td>\n",
       "<td>0.5450769</td>\n",
       "<td>0.0069976</td>\n",
       "<td>0.9990236</td>\n",
       "<td>-93.0010432</td>\n",
       "<td>11.0050852</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000014</td>\n",
       "<td>0.0097621</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0047923</td>\n",
       "<td>0.0024248</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4908008</td>\n",
       "<td>0.0009764</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.0237914</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.998037           2.03702     2.03702            1                0.998915    1                           0.998915            0.0204231       0.0204231                  103.702   103.702\n",
       "    2        0.020012                    0.995289           2.02887     2.03296            0.996            0.996693    0.998004                    0.997806            0.0202604       0.0406835                  102.887   103.296\n",
       "    3        0.0300379                   0.99176            2.03702     2.03431            1                0.993542    0.99867                     0.996383            0.0204231       0.0611066                  103.702   103.431\n",
       "    4        0.040024                    0.988263           2.00443     2.02686            0.984            0.990047    0.99501                     0.994802            0.0200163       0.0811229                  100.443   102.686\n",
       "    5        0.05001                     0.984745           2.00443     2.02238            0.984            0.986497    0.992812                    0.993144            0.0200163       0.101139                   100.443   102.238\n",
       "    6        0.10002                     0.964389           1.99472     2.00855            0.979233         0.97477     0.986022                    0.983957            0.0997559       0.200895                   99.472    100.855\n",
       "    7        0.15003                     0.936455           1.95242     1.98984            0.958466         0.951049    0.976837                    0.972988            0.0976404       0.298535                   95.2417   98.9839\n",
       "    8        0.2                         0.900947           1.91653     1.97152            0.940847         0.919866    0.967845                    0.959715            0.0957689       0.394304                   91.6527   97.1522\n",
       "    9        0.30002                     0.799678           1.82144     1.92149            0.894169         0.853913    0.943283                    0.924443            0.182181        0.576485                   82.1443   92.1489\n",
       "    10       0.4                         0.660921           1.59999     1.84113            0.785457         0.733193    0.903835                    0.87664             0.159967        0.736452                   59.9994   84.1131\n",
       "    11       0.50002                     0.497181           1.24385     1.72166            0.610623         0.580112    0.845183                    0.817325            0.12441         0.860862                   24.3852   72.1656\n",
       "    12       0.6                         0.330764           0.737332    1.55763            0.361966         0.413208    0.764663                    0.749986            0.0737185       0.934581                   -26.2668  55.7635\n",
       "    13       0.69998                     0.178866           0.379246    1.38932            0.186177         0.251613    0.682036                    0.678802            0.037917        0.972498                   -62.0754  38.9322\n",
       "    14       0.8                         0.0682901          0.195242    1.24003            0.0958466        0.120727    0.608748                    0.609029            0.0195281       0.992026                   -80.4758  24.0033\n",
       "    15       0.89998                     0.00898464         0.0699896   1.11005            0.0343588        0.0333615   0.544938                    0.545077            0.00699756      0.999024                   -93.001   11.0051\n",
       "    16       1                           1.43574e-06        0.00976209  1                  0.00479233       0.00242481  0.490913                    0.490801            0.000976404     1                          -99.0238  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:37:56</td>\n",
       "<td> 0.051 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:00</td>\n",
       "<td> 3.310 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3509950</td>\n",
       "<td>0.3866388</td>\n",
       "<td>0.9124678</td>\n",
       "<td>0.9095944</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1855694</td>\n",
       "<td>0.3632448</td>\n",
       "<td>0.4092306</td>\n",
       "<td>0.8971865</td>\n",
       "<td>0.8927706</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1975634</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:02</td>\n",
       "<td> 5.381 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.3372822</td>\n",
       "<td>0.3568154</td>\n",
       "<td>0.9251009</td>\n",
       "<td>0.9185844</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1722709</td>\n",
       "<td>0.3557758</td>\n",
       "<td>0.3911585</td>\n",
       "<td>0.9042068</td>\n",
       "<td>0.8947384</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1910925</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:04</td>\n",
       "<td> 7.520 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.3221226</td>\n",
       "<td>0.3309317</td>\n",
       "<td>0.9391586</td>\n",
       "<td>0.9301031</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1488588</td>\n",
       "<td>0.3492999</td>\n",
       "<td>0.3793920</td>\n",
       "<td>0.9107117</td>\n",
       "<td>0.8994253</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1779908</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:06</td>\n",
       "<td> 9.834 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.3067779</td>\n",
       "<td>0.3061358</td>\n",
       "<td>0.9519408</td>\n",
       "<td>0.9423388</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1266448</td>\n",
       "<td>0.3437660</td>\n",
       "<td>0.3695974</td>\n",
       "<td>0.9159051</td>\n",
       "<td>0.9025654</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1716796</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:09</td>\n",
       "<td>12.134 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.2908329</td>\n",
       "<td>0.2819427</td>\n",
       "<td>0.9632166</td>\n",
       "<td>0.9512082</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1105709</td>\n",
       "<td>0.3381760</td>\n",
       "<td>0.3600964</td>\n",
       "<td>0.9207667</td>\n",
       "<td>0.9068827</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1624526</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:11</td>\n",
       "<td>14.499 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.2759516</td>\n",
       "<td>0.2604138</td>\n",
       "<td>0.9721992</td>\n",
       "<td>0.9596572</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0929794</td>\n",
       "<td>0.3332754</td>\n",
       "<td>0.3522831</td>\n",
       "<td>0.9246805</td>\n",
       "<td>0.9081233</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1551827</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:13</td>\n",
       "<td>16.947 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.2621348</td>\n",
       "<td>0.2413035</td>\n",
       "<td>0.9792453</td>\n",
       "<td>0.9648685</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0786426</td>\n",
       "<td>0.3289836</td>\n",
       "<td>0.3453679</td>\n",
       "<td>0.9278756</td>\n",
       "<td>0.9103620</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1496305</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:15</td>\n",
       "<td>18.536 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.2540528</td>\n",
       "<td>0.2305365</td>\n",
       "<td>0.9828674</td>\n",
       "<td>0.9684934</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0708452</td>\n",
       "<td>0.3269531</td>\n",
       "<td>0.3422374</td>\n",
       "<td>0.9292630</td>\n",
       "<td>0.9110624</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1468744</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:37:56  0.051 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:38:00  3.310 sec   20                 0.350995         0.386639            0.912468        0.909594           2.03499          0.185569                         0.363245           0.409231              0.897186          0.892771             2.03702            0.197563\n",
       "    2020-07-16 22:38:02  5.381 sec   40                 0.337282         0.356815            0.925101        0.918584           2.03499          0.172271                         0.355776           0.391158              0.904207          0.894738             2.03702            0.191092\n",
       "    2020-07-16 22:38:04  7.520 sec   60                 0.322123         0.330932            0.939159        0.930103           2.03499          0.148859                         0.3493             0.379392              0.910712          0.899425             2.03702            0.177991\n",
       "    2020-07-16 22:38:06  9.834 sec   80                 0.306778         0.306136            0.951941        0.942339           2.03499          0.126645                         0.343766           0.369597              0.915905          0.902565             2.03702            0.17168\n",
       "    2020-07-16 22:38:09  12.134 sec  100                0.290833         0.281943            0.963217        0.951208           2.03499          0.110571                         0.338176           0.360096              0.920767          0.906883             2.03702            0.162453\n",
       "    2020-07-16 22:38:11  14.499 sec  120                0.275952         0.260414            0.972199        0.959657           2.03499          0.0929794                        0.333275           0.352283              0.924681          0.908123             2.03702            0.155183\n",
       "    2020-07-16 22:38:13  16.947 sec  140                0.262135         0.241303            0.979245        0.964869           2.03499          0.0786426                        0.328984           0.345368              0.927876          0.910362             2.03702            0.149631\n",
       "    2020-07-16 22:38:15  18.536 sec  150                0.254053         0.230537            0.982867        0.968493           2.03499          0.0708452                        0.326953           0.342237              0.929263          0.911062             2.03702            0.146874"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>98385.8203125</td>\n",
       "<td>1.0</td>\n",
       "<td>0.4030239</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>15846.4169922</td>\n",
       "<td>0.1610640</td>\n",
       "<td>0.0649126</td></tr>\n",
       "<tr><td>e18</td>\n",
       "<td>7029.8276367</td>\n",
       "<td>0.0714516</td>\n",
       "<td>0.0287967</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>6948.0302734</td>\n",
       "<td>0.0706202</td>\n",
       "<td>0.0284616</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>5982.1201172</td>\n",
       "<td>0.0608027</td>\n",
       "<td>0.0245049</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>c08.C</td>\n",
       "<td>1.8748287</td>\n",
       "<td>0.0000191</td>\n",
       "<td>0.0000077</td></tr>\n",
       "<tr><td>e24.E</td>\n",
       "<td>1.7421736</td>\n",
       "<td>0.0000177</td>\n",
       "<td>0.0000071</td></tr>\n",
       "<tr><td>e13.D</td>\n",
       "<td>1.6543319</td>\n",
       "<td>0.0000168</td>\n",
       "<td>0.0000068</td></tr>\n",
       "<tr><td>e20.F3D59</td>\n",
       "<td>1.5619270</td>\n",
       "<td>0.0000159</td>\n",
       "<td>0.0000064</td></tr>\n",
       "<tr><td>f27.C</td>\n",
       "<td>0.7985026</td>\n",
       "<td>0.0000081</td>\n",
       "<td>0.0000033</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         98385.8203125          1.0                     0.4030238607228191\n",
       "e19         15846.4169921875       0.1610640328235816      0.0649126483321467\n",
       "e18         7029.82763671875       0.07145163413172868     0.028796713442723647\n",
       "f02         6948.0302734375        0.07062024030870175     0.028461641894386228\n",
       "e12         5982.1201171875        0.06080266544697871     0.024504924970679312\n",
       "---         ---                    ---                     ---\n",
       "c08.C       1.8748286962509155     1.905588315771472e-05   7.679975599705131e-06\n",
       "e24.E       1.7421735525131226     1.7707567482585474e-05  7.136572210841449e-06\n",
       "e13.D       1.654331922531128      1.6814739332116374e-05  6.776741162677378e-06\n",
       "e20.F3D59   1.5619269609451294     1.5875529176704798e-05  6.3982170598133254e-06\n",
       "f27.C       0.7985026240348816     8.116033606251603e-06   3.270955197747665e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1050\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.043129487942190146\n",
      "RMSE: 0.20767640198681733\n",
      "LogLoss: 0.17177115034187015\n",
      "Mean Per-Class Error: 0.03609642247413469\n",
      "AUC: 0.9947704804665793\n",
      "pr_auc: 0.9728693210451688\n",
      "Gini: 0.9895409609331587\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5153042009002284: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>49236.0</td>\n",
       "<td>1706.0</td>\n",
       "<td>0.0335</td>\n",
       "<td> (1706.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1905.0</td>\n",
       "<td>47315.0</td>\n",
       "<td>0.0387</td>\n",
       "<td> (1905.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>51141.0</td>\n",
       "<td>49021.0</td>\n",
       "<td>0.0361</td>\n",
       "<td> (3611.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      49236  1706   0.0335   (1706.0/50942.0)\n",
       "1      1905   47315  0.0387   (1905.0/49220.0)\n",
       "Total  51141  49021  0.0361   (3611.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5153042</td>\n",
       "<td>0.9632435</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4157188</td>\n",
       "<td>0.9727936</td>\n",
       "<td>228.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5932036</td>\n",
       "<td>0.9707790</td>\n",
       "<td>175.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5153042</td>\n",
       "<td>0.9639484</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9994679</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0838248</td>\n",
       "<td>1.0</td>\n",
       "<td>349.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9994679</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5153042</td>\n",
       "<td>0.9278779</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5082441</td>\n",
       "<td>0.9634498</td>\n",
       "<td>201.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5153042</td>\n",
       "<td>0.9639036</td>\n",
       "<td>199.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.515304     0.963243  199\n",
       "max f2                       0.415719     0.972794  228\n",
       "max f0point5                 0.593204     0.970779  175\n",
       "max accuracy                 0.515304     0.963948  199\n",
       "max precision                0.999468     1         0\n",
       "max recall                   0.0838248    1         349\n",
       "max specificity              0.999468     1         0\n",
       "max absolute_mcc             0.515304     0.927878  199\n",
       "max min_per_class_accuracy   0.508244     0.96345   201\n",
       "max mean_per_class_accuracy  0.515304     0.963904  199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100038</td>\n",
       "<td>0.9990887</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995373</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995373</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0203576</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200076</td>\n",
       "<td>0.9978544</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9985003</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9990188</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0407152</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300014</td>\n",
       "<td>0.9964125</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9971621</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9984003</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0610524</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9947280</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9956059</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9977015</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9928872</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9938444</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9969300</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9808923</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9873268</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9921288</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9634080</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9728318</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9856969</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9388907</td>\n",
       "<td>2.0345794</td>\n",
       "<td>2.0348842</td>\n",
       "<td>0.9998003</td>\n",
       "<td>0.9517614</td>\n",
       "<td>0.9999501</td>\n",
       "<td>0.9772134</td>\n",
       "<td>0.1017269</td>\n",
       "<td>0.4069890</td>\n",
       "<td>103.4579431</td>\n",
       "<td>103.4884196</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.8645827</td>\n",
       "<td>2.0319382</td>\n",
       "<td>2.0339022</td>\n",
       "<td>0.9985024</td>\n",
       "<td>0.9051089</td>\n",
       "<td>0.9994675</td>\n",
       "<td>0.9531794</td>\n",
       "<td>0.2031898</td>\n",
       "<td>0.6101788</td>\n",
       "<td>103.1938176</td>\n",
       "<td>103.3902222</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.7405021</td>\n",
       "<td>2.0045098</td>\n",
       "<td>2.0265543</td>\n",
       "<td>0.9850240</td>\n",
       "<td>0.8080529</td>\n",
       "<td>0.9958567</td>\n",
       "<td>0.9168987</td>\n",
       "<td>0.2004470</td>\n",
       "<td>0.8106258</td>\n",
       "<td>100.4509753</td>\n",
       "<td>102.6554288</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4798692</td>\n",
       "<td>1.6038516</td>\n",
       "<td>1.9420154</td>\n",
       "<td>0.7881390</td>\n",
       "<td>0.6247189</td>\n",
       "<td>0.9543140</td>\n",
       "<td>0.8584639</td>\n",
       "<td>0.1603820</td>\n",
       "<td>0.9710077</td>\n",
       "<td>60.3851611</td>\n",
       "<td>94.2015441</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.2446289</td>\n",
       "<td>0.2736747</td>\n",
       "<td>1.6639633</td>\n",
       "<td>0.1344848</td>\n",
       "<td>0.3482757</td>\n",
       "<td>0.8176781</td>\n",
       "<td>0.7734340</td>\n",
       "<td>0.0273669</td>\n",
       "<td>0.9983746</td>\n",
       "<td>-72.6325295</td>\n",
       "<td>66.3963278</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.1186674</td>\n",
       "<td>0.0154412</td>\n",
       "<td>1.4284635</td>\n",
       "<td>0.0075879</td>\n",
       "<td>0.1764732</td>\n",
       "<td>0.7019526</td>\n",
       "<td>0.6881551</td>\n",
       "<td>0.0015441</td>\n",
       "<td>0.9999187</td>\n",
       "<td>-98.4558814</td>\n",
       "<td>42.8463481</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0412688</td>\n",
       "<td>0.0008127</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0003994</td>\n",
       "<td>0.0765784</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6117089</td>\n",
       "<td>0.0000813</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9187306</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0047143</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0195171</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459106</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0011491</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4914301</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100038                   0.999089           2.03499      2.03499            1                0.999537    1                           0.999537            0.0203576       0.0203576                  103.499   103.499\n",
       "    2        0.0200076                   0.997854           2.03499      2.03499            1                0.9985      1                           0.999019            0.0203576       0.0407152                  103.499   103.499\n",
       "    3        0.0300014                   0.996412           2.03499      2.03499            1                0.997162    1                           0.9984              0.0203373       0.0610524                  103.499   103.499\n",
       "    4        0.0400052                   0.994728           2.03499      2.03499            1                0.995606    1                           0.997702            0.0203576       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.992887           2.03499      2.03499            1                0.993844    1                           0.99693             0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.980892           2.03499      2.03499            1                0.987327    1                           0.992129            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.963408           2.03499      2.03499            1                0.972832    1                           0.985697            0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.938891           2.03458      2.03488            0.9998           0.951761    0.99995                     0.977213            0.101727        0.406989                   103.458   103.488\n",
       "    9        0.300004                    0.864583           2.03194      2.0339             0.998502         0.905109    0.999468                    0.953179            0.20319         0.610179                   103.194   103.39\n",
       "    10       0.400002                    0.740502           2.00451      2.02655            0.985024         0.808053    0.995857                    0.916899            0.200447        0.810626                   100.451   102.655\n",
       "    11       0.5                         0.479869           1.60385      1.94202            0.788139         0.624719    0.954314                    0.858464            0.160382        0.971008                   60.3852   94.2015\n",
       "    12       0.599998                    0.244629           0.273675     1.66396            0.134485         0.348276    0.817678                    0.773434            0.0273669       0.998375                   -72.6325  66.3963\n",
       "    13       0.699996                    0.118667           0.0154412    1.42846            0.00758786       0.176473    0.701953                    0.688155            0.00154409      0.999919                   -98.4559  42.8463\n",
       "    14       0.799994                    0.0412688          0.000812694  1.25001            0.000399361      0.0765784   0.61426                     0.611709            8.12678e-05     1                          -99.9187  25.0009\n",
       "    15       0.899992                    0.00471432         0            1.11112            0                0.0195171   0.546009                    0.545911            0               1                          -100      11.1121\n",
       "    16       1                           1.74868e-08        0            1                  0                0.00114909  0.491404                    0.49143             0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10159886373701392\n",
      "RMSE: 0.31874576661818416\n",
      "LogLoss: 0.3323685434588642\n",
      "Mean Per-Class Error: 0.13449763001556803\n",
      "AUC: 0.9328576403707831\n",
      "pr_auc: 0.9076439690357748\n",
      "Gini: 0.8657152807415662\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.46539048105478287: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10791.0</td>\n",
       "<td>1954.0</td>\n",
       "<td>0.1533</td>\n",
       "<td> (1954.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1428.0</td>\n",
       "<td>10862.0</td>\n",
       "<td>0.1162</td>\n",
       "<td> (1428.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12219.0</td>\n",
       "<td>12816.0</td>\n",
       "<td>0.1351</td>\n",
       "<td> (3382.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10791  1954   0.1533   (1954.0/12745.0)\n",
       "1      1428   10862  0.1162   (1428.0/12290.0)\n",
       "Total  12219  12816  0.1351   (3382.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4653905</td>\n",
       "<td>0.8652912</td>\n",
       "<td>211.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.2136777</td>\n",
       "<td>0.9008615</td>\n",
       "<td>299.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6291746</td>\n",
       "<td>0.8739620</td>\n",
       "<td>158.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5165776</td>\n",
       "<td>0.8655882</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9995165</td>\n",
       "<td>0.9959184</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0001084</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9995165</td>\n",
       "<td>0.9999215</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5165776</td>\n",
       "<td>0.7310728</td>\n",
       "<td>194.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5083627</td>\n",
       "<td>0.8646054</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5165776</td>\n",
       "<td>0.8655024</td>\n",
       "<td>194.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.46539      0.865291  211\n",
       "max f2                       0.213678     0.900862  299\n",
       "max f0point5                 0.629175     0.873962  158\n",
       "max accuracy                 0.516578     0.865588  194\n",
       "max precision                0.999517     0.995918  0\n",
       "max recall                   0.00010837   1         399\n",
       "max specificity              0.999517     0.999922  0\n",
       "max absolute_mcc             0.516578     0.731073  194\n",
       "max min_per_class_accuracy   0.508363     0.864605  197\n",
       "max mean_per_class_accuracy  0.516578     0.865502  194"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.12 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9990793</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0370220</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995292</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995292</td>\n",
       "<td>0.0204231</td>\n",
       "<td>0.0204231</td>\n",
       "<td>103.7021969</td>\n",
       "<td>103.7021969</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9977173</td>\n",
       "<td>2.0125777</td>\n",
       "<td>2.0248242</td>\n",
       "<td>0.988</td>\n",
       "<td>0.9984345</td>\n",
       "<td>0.9940120</td>\n",
       "<td>0.9989829</td>\n",
       "<td>0.0200976</td>\n",
       "<td>0.0405207</td>\n",
       "<td>101.2577705</td>\n",
       "<td>102.4824233</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9963267</td>\n",
       "<td>2.0289063</td>\n",
       "<td>2.0261867</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9970847</td>\n",
       "<td>0.9946809</td>\n",
       "<td>0.9983493</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0608625</td>\n",
       "<td>102.8906344</td>\n",
       "<td>102.6186746</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400639</td>\n",
       "<td>0.9945512</td>\n",
       "<td>2.0207907</td>\n",
       "<td>2.0248364</td>\n",
       "<td>0.9920319</td>\n",
       "<td>0.9955183</td>\n",
       "<td>0.9940179</td>\n",
       "<td>0.9976409</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0811229</td>\n",
       "<td>102.0790718</td>\n",
       "<td>102.4836394</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9924599</td>\n",
       "<td>2.0042987</td>\n",
       "<td>2.0207518</td>\n",
       "<td>0.9839357</td>\n",
       "<td>0.9935410</td>\n",
       "<td>0.9920128</td>\n",
       "<td>0.9968255</td>\n",
       "<td>0.0199349</td>\n",
       "<td>0.1010578</td>\n",
       "<td>100.4298725</td>\n",
       "<td>102.0751826</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9791842</td>\n",
       "<td>1.9800765</td>\n",
       "<td>2.0004141</td>\n",
       "<td>0.9720447</td>\n",
       "<td>0.9863759</td>\n",
       "<td>0.9820288</td>\n",
       "<td>0.9916007</td>\n",
       "<td>0.0990236</td>\n",
       "<td>0.2000814</td>\n",
       "<td>98.0076467</td>\n",
       "<td>100.0414146</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9588494</td>\n",
       "<td>1.9524172</td>\n",
       "<td>1.9844152</td>\n",
       "<td>0.9584665</td>\n",
       "<td>0.9698716</td>\n",
       "<td>0.9741747</td>\n",
       "<td>0.9843577</td>\n",
       "<td>0.0976404</td>\n",
       "<td>0.2977217</td>\n",
       "<td>95.2417223</td>\n",
       "<td>98.4415172</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9292820</td>\n",
       "<td>1.9035001</td>\n",
       "<td>1.9641985</td>\n",
       "<td>0.9344524</td>\n",
       "<td>0.9449371</td>\n",
       "<td>0.9642500</td>\n",
       "<td>0.9745084</td>\n",
       "<td>0.0951180</td>\n",
       "<td>0.3928397</td>\n",
       "<td>90.3500145</td>\n",
       "<td>96.4198535</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.8396990</td>\n",
       "<td>1.8466613</td>\n",
       "<td>1.9250142</td>\n",
       "<td>0.9065495</td>\n",
       "<td>0.8888878</td>\n",
       "<td>0.9450140</td>\n",
       "<td>0.9459644</td>\n",
       "<td>0.1847030</td>\n",
       "<td>0.5775427</td>\n",
       "<td>84.6661290</td>\n",
       "<td>92.5014237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.7015813</td>\n",
       "<td>1.6553347</td>\n",
       "<td>1.8576078</td>\n",
       "<td>0.8126249</td>\n",
       "<td>0.7759244</td>\n",
       "<td>0.9119233</td>\n",
       "<td>0.9034629</td>\n",
       "<td>0.1655004</td>\n",
       "<td>0.7430431</td>\n",
       "<td>65.5334672</td>\n",
       "<td>85.7607811</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4911243</td>\n",
       "<td>1.2829008</td>\n",
       "<td>1.7426480</td>\n",
       "<td>0.6297923</td>\n",
       "<td>0.6000110</td>\n",
       "<td>0.8554881</td>\n",
       "<td>0.8427628</td>\n",
       "<td>0.1283157</td>\n",
       "<td>0.8713588</td>\n",
       "<td>28.2900817</td>\n",
       "<td>74.2648048</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.2831091</td>\n",
       "<td>0.6640871</td>\n",
       "<td>1.5629238</td>\n",
       "<td>0.3260088</td>\n",
       "<td>0.3821024</td>\n",
       "<td>0.7672592</td>\n",
       "<td>0.7660014</td>\n",
       "<td>0.0663954</td>\n",
       "<td>0.9377543</td>\n",
       "<td>-33.5912934</td>\n",
       "<td>56.2923786</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.1375014</td>\n",
       "<td>0.3654107</td>\n",
       "<td>1.3918798</td>\n",
       "<td>0.1793847</td>\n",
       "<td>0.2038193</td>\n",
       "<td>0.6832915</td>\n",
       "<td>0.6857034</td>\n",
       "<td>0.0365338</td>\n",
       "<td>0.9742880</td>\n",
       "<td>-63.4589347</td>\n",
       "<td>39.1879768</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0470043</td>\n",
       "<td>0.1708365</td>\n",
       "<td>1.2392189</td>\n",
       "<td>0.0838658</td>\n",
       "<td>0.0887539</td>\n",
       "<td>0.6083483</td>\n",
       "<td>0.6110698</td>\n",
       "<td>0.0170871</td>\n",
       "<td>0.9913751</td>\n",
       "<td>-82.9163493</td>\n",
       "<td>23.9218877</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0052733</td>\n",
       "<td>0.0724311</td>\n",
       "<td>1.1095988</td>\n",
       "<td>0.0355573</td>\n",
       "<td>0.0217954</td>\n",
       "<td>0.5447162</td>\n",
       "<td>0.5456065</td>\n",
       "<td>0.0072417</td>\n",
       "<td>0.9986168</td>\n",
       "<td>-92.7568935</td>\n",
       "<td>10.9598803</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0138296</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0067891</td>\n",
       "<td>0.0012162</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4911566</td>\n",
       "<td>0.0013832</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.6170378</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score       cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  ----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.999079           2.03702    2.03702            1                0.999529    1                           0.999529            0.0204231       0.0204231                  103.702   103.702\n",
       "    2        0.020012                    0.997717           2.01258    2.02482            0.988            0.998435    0.994012                    0.998983            0.0200976       0.0405207                  101.258   102.482\n",
       "    3        0.0300379                   0.996327           2.02891    2.02619            0.996016         0.997085    0.994681                    0.998349            0.0203417       0.0608625                  102.891   102.619\n",
       "    4        0.0400639                   0.994551           2.02079    2.02484            0.992032         0.995518    0.994018                    0.997641            0.0202604       0.0811229                  102.079   102.484\n",
       "    5        0.05001                     0.99246            2.0043     2.02075            0.983936         0.993541    0.992013                    0.996825            0.0199349       0.101058                   100.43    102.075\n",
       "    6        0.10002                     0.979184           1.98008    2.00041            0.972045         0.986376    0.982029                    0.991601            0.0990236       0.200081                   98.0076   100.041\n",
       "    7        0.15003                     0.958849           1.95242    1.98442            0.958466         0.969872    0.974175                    0.984358            0.0976404       0.297722                   95.2417   98.4415\n",
       "    8        0.2                         0.929282           1.9035     1.9642             0.934452         0.944937    0.96425                     0.974508            0.095118        0.39284                    90.35     96.4199\n",
       "    9        0.30002                     0.839699           1.84666    1.92501            0.90655          0.888888    0.945014                    0.945964            0.184703        0.577543                   84.6661   92.5014\n",
       "    10       0.4                         0.701581           1.65533    1.85761            0.812625         0.775924    0.911923                    0.903463            0.1655          0.743043                   65.5335   85.7608\n",
       "    11       0.50002                     0.491124           1.2829     1.74265            0.629792         0.600011    0.855488                    0.842763            0.128316        0.871359                   28.2901   74.2648\n",
       "    12       0.6                         0.283109           0.664087   1.56292            0.326009         0.382102    0.767259                    0.766001            0.0663954       0.937754                   -33.5913  56.2924\n",
       "    13       0.69998                     0.137501           0.365411   1.39188            0.179385         0.203819    0.683291                    0.685703            0.0365338       0.974288                   -63.4589  39.188\n",
       "    14       0.8                         0.0470043          0.170837   1.23922            0.0838658        0.0887539   0.608348                    0.61107             0.0170871       0.991375                   -82.9163  23.9219\n",
       "    15       0.89998                     0.00527327         0.0724311  1.1096             0.0355573        0.0217954   0.544716                    0.545607            0.00724166      0.998617                   -92.7569  10.9599\n",
       "    16       1                           3.03751e-08        0.0138296  1                  0.00678914       0.00121624  0.490913                    0.491157            0.00138324      1                          -98.617   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:16</td>\n",
       "<td> 0.048 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:19</td>\n",
       "<td> 3.272 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3443959</td>\n",
       "<td>0.3699292</td>\n",
       "<td>0.9177508</td>\n",
       "<td>0.9122976</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1752461</td>\n",
       "<td>0.3602829</td>\n",
       "<td>0.3999922</td>\n",
       "<td>0.8991834</td>\n",
       "<td>0.8906775</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1964849</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:21</td>\n",
       "<td> 5.345 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.3243500</td>\n",
       "<td>0.3340693</td>\n",
       "<td>0.9365699</td>\n",
       "<td>0.9282681</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1520437</td>\n",
       "<td>0.3522664</td>\n",
       "<td>0.3841445</td>\n",
       "<td>0.9072996</td>\n",
       "<td>0.8979126</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1884562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:24</td>\n",
       "<td> 7.653 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2997774</td>\n",
       "<td>0.2946218</td>\n",
       "<td>0.9561986</td>\n",
       "<td>0.9462367</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1202252</td>\n",
       "<td>0.3436300</td>\n",
       "<td>0.3692685</td>\n",
       "<td>0.9154097</td>\n",
       "<td>0.9019943</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1719193</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:26</td>\n",
       "<td> 9.949 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.2774761</td>\n",
       "<td>0.2615907</td>\n",
       "<td>0.9703391</td>\n",
       "<td>0.9570516</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0965037</td>\n",
       "<td>0.3369161</td>\n",
       "<td>0.3583677</td>\n",
       "<td>0.9210009</td>\n",
       "<td>0.9052103</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1671660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:28</td>\n",
       "<td>12.323 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.2561205</td>\n",
       "<td>0.2318094</td>\n",
       "<td>0.9809056</td>\n",
       "<td>0.9652806</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0747789</td>\n",
       "<td>0.3304743</td>\n",
       "<td>0.3487572</td>\n",
       "<td>0.9257806</td>\n",
       "<td>0.9060202</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1536649</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:31</td>\n",
       "<td>14.787 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.2364040</td>\n",
       "<td>0.2062824</td>\n",
       "<td>0.9881253</td>\n",
       "<td>0.9701808</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0570576</td>\n",
       "<td>0.3255187</td>\n",
       "<td>0.3414633</td>\n",
       "<td>0.9290400</td>\n",
       "<td>0.9062400</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1451168</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:33</td>\n",
       "<td>17.284 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.2168847</td>\n",
       "<td>0.1824398</td>\n",
       "<td>0.9930099</td>\n",
       "<td>0.9743248</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0429305</td>\n",
       "<td>0.3205744</td>\n",
       "<td>0.3344415</td>\n",
       "<td>0.9319943</td>\n",
       "<td>0.9083504</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1385660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:35</td>\n",
       "<td>18.902 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.2076764</td>\n",
       "<td>0.1717712</td>\n",
       "<td>0.9947705</td>\n",
       "<td>0.9728693</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0360516</td>\n",
       "<td>0.3187458</td>\n",
       "<td>0.3323685</td>\n",
       "<td>0.9328576</td>\n",
       "<td>0.9076440</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1350909</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:38:16  0.048 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:38:19  3.272 sec   20                 0.344396         0.369929            0.917751        0.912298           2.03499          0.175246                         0.360283           0.399992              0.899183          0.890677             2.03702            0.196485\n",
       "    2020-07-16 22:38:21  5.345 sec   40                 0.32435          0.334069            0.93657         0.928268           2.03499          0.152044                         0.352266           0.384144              0.9073            0.897913             2.03702            0.188456\n",
       "    2020-07-16 22:38:24  7.653 sec   60                 0.299777         0.294622            0.956199        0.946237           2.03499          0.120225                         0.34363            0.369268              0.91541           0.901994             2.03702            0.171919\n",
       "    2020-07-16 22:38:26  9.949 sec   80                 0.277476         0.261591            0.970339        0.957052           2.03499          0.0965037                        0.336916           0.358368              0.921001          0.90521              2.03702            0.167166\n",
       "    2020-07-16 22:38:28  12.323 sec  100                0.256121         0.231809            0.980906        0.965281           2.03499          0.0747789                        0.330474           0.348757              0.925781          0.90602              2.03702            0.153665\n",
       "    2020-07-16 22:38:31  14.787 sec  120                0.236404         0.206282            0.988125        0.970181           2.03499          0.0570576                        0.325519           0.341463              0.92904           0.90624              2.03702            0.145117\n",
       "    2020-07-16 22:38:33  17.284 sec  140                0.216885         0.18244             0.99301         0.974325           2.03499          0.0429305                        0.320574           0.334442              0.931994          0.90835              2.03702            0.138566\n",
       "    2020-07-16 22:38:35  18.902 sec  150                0.207676         0.171771            0.99477         0.972869           2.03499          0.0360516                        0.318746           0.332369              0.932858          0.907644             2.03702            0.135091"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>71804.2578125</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3668746</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>11932.8251953</td>\n",
       "<td>0.1661855</td>\n",
       "<td>0.0609692</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>5919.8657227</td>\n",
       "<td>0.0824445</td>\n",
       "<td>0.0302468</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>5810.25</td>\n",
       "<td>0.0809179</td>\n",
       "<td>0.0296867</td></tr>\n",
       "<tr><td>e18</td>\n",
       "<td>5588.7402344</td>\n",
       "<td>0.0778330</td>\n",
       "<td>0.0285549</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>a02.G</td>\n",
       "<td>2.0994661</td>\n",
       "<td>0.0000292</td>\n",
       "<td>0.0000107</td></tr>\n",
       "<tr><td>a19.Y</td>\n",
       "<td>1.1625078</td>\n",
       "<td>0.0000162</td>\n",
       "<td>0.0000059</td></tr>\n",
       "<tr><td>e13.G</td>\n",
       "<td>0.9801430</td>\n",
       "<td>0.0000137</td>\n",
       "<td>0.0000050</td></tr>\n",
       "<tr><td>e13.C</td>\n",
       "<td>0.9274654</td>\n",
       "<td>0.0000129</td>\n",
       "<td>0.0000047</td></tr>\n",
       "<tr><td>a13.E</td>\n",
       "<td>0.8629808</td>\n",
       "<td>0.0000120</td>\n",
       "<td>0.0000044</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         71804.2578125          1.0                     0.36687459036253667\n",
       "e19         11932.8251953125       0.16618548201517908     0.06096923063851953\n",
       "f02         5919.86572265625       0.082444494282144       0.030246790067408078\n",
       "e12         5810.25                0.08091790343648014     0.02968672267625395\n",
       "e18         5588.740234375         0.07783299214607421     0.02855494711028151\n",
       "---         ---                    ---                     ---\n",
       "a02.G       2.09946608543396       2.923874084063683e-05   1.0726951068625009e-05\n",
       "a19.Y       1.1625077724456787     1.618995597003865e-05   5.939683464495435e-06\n",
       "e13.G       0.9801430106163025     1.3650207389869781e-05  5.007914244522147e-06\n",
       "e13.C       0.9274654388427734     1.2916579978650182e-05  4.738764988552228e-06\n",
       "a13.E       0.862980842590332      1.2018519080634527e-05  4.409289264472123e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1069\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.029030680458400515\n",
      "RMSE: 0.1703839207742342\n",
      "LogLoss: 0.1293547308527292\n",
      "Mean Per-Class Error: 0.018820984373221994\n",
      "AUC: 0.9984687169069951\n",
      "pr_auc: 0.9624729759451258\n",
      "Gini: 0.9969374338139902\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5027582335472107: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>49958.0</td>\n",
       "<td>984.0</td>\n",
       "<td>0.0193</td>\n",
       "<td> (984.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>902.0</td>\n",
       "<td>48318.0</td>\n",
       "<td>0.0183</td>\n",
       "<td> (902.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50860.0</td>\n",
       "<td>49302.0</td>\n",
       "<td>0.0188</td>\n",
       "<td> (1886.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      49958  984    0.0193   (984.0/50942.0)\n",
       "1      902    48318  0.0183   (902.0/49220.0)\n",
       "Total  50860  49302  0.0188   (1886.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5027582</td>\n",
       "<td>0.9808571</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4177324</td>\n",
       "<td>0.9857441</td>\n",
       "<td>222.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5820136</td>\n",
       "<td>0.9857253</td>\n",
       "<td>179.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5027582</td>\n",
       "<td>0.9811705</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9996705</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0245338</td>\n",
       "<td>1.0</td>\n",
       "<td>379.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9996705</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5027582</td>\n",
       "<td>0.9623322</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5072876</td>\n",
       "<td>0.9808411</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5027582</td>\n",
       "<td>0.9811790</td>\n",
       "<td>200.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.502758     0.980857  200\n",
       "max f2                       0.417732     0.985744  222\n",
       "max f0point5                 0.582014     0.985725  179\n",
       "max accuracy                 0.502758     0.981171  200\n",
       "max precision                0.99967      1         0\n",
       "max recall                   0.0245338    1         379\n",
       "max specificity              0.99967      1         0\n",
       "max absolute_mcc             0.502758     0.962332  200\n",
       "max min_per_class_accuracy   0.507288     0.980841  199\n",
       "max mean_per_class_accuracy  0.502758     0.981179  200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100038</td>\n",
       "<td>0.9996790</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998491</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998491</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0203576</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200076</td>\n",
       "<td>0.9991983</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994548</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9996519</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0407152</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300014</td>\n",
       "<td>0.9985401</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9988841</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9993961</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0610524</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9977033</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9981409</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9990823</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9967339</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9972312</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9987120</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9892604</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9933397</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9960261</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9772443</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9837171</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9919234</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9598860</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9690457</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9862043</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9025055</td>\n",
       "<td>2.0347826</td>\n",
       "<td>2.0349181</td>\n",
       "<td>0.9999002</td>\n",
       "<td>0.9342773</td>\n",
       "<td>0.9999667</td>\n",
       "<td>0.9688958</td>\n",
       "<td>0.2034742</td>\n",
       "<td>0.6104835</td>\n",
       "<td>103.4782605</td>\n",
       "<td>103.4918056</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.7957273</td>\n",
       "<td>2.0303128</td>\n",
       "<td>2.0337668</td>\n",
       "<td>0.9977037</td>\n",
       "<td>0.8549051</td>\n",
       "<td>0.9994010</td>\n",
       "<td>0.9403989</td>\n",
       "<td>0.2030272</td>\n",
       "<td>0.8135108</td>\n",
       "<td>103.0312788</td>\n",
       "<td>103.3766768</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4562848</td>\n",
       "<td>1.7454635</td>\n",
       "<td>1.9761073</td>\n",
       "<td>0.8577276</td>\n",
       "<td>0.6654601</td>\n",
       "<td>0.9710669</td>\n",
       "<td>0.8854122</td>\n",
       "<td>0.1745429</td>\n",
       "<td>0.9880536</td>\n",
       "<td>74.5463540</td>\n",
       "<td>97.6107273</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.1885726</td>\n",
       "<td>0.1160121</td>\n",
       "<td>1.6660966</td>\n",
       "<td>0.0570088</td>\n",
       "<td>0.2914636</td>\n",
       "<td>0.8187264</td>\n",
       "<td>0.7864224</td>\n",
       "<td>0.0116010</td>\n",
       "<td>0.9996546</td>\n",
       "<td>-88.3987931</td>\n",
       "<td>66.6096565</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0840321</td>\n",
       "<td>0.0028444</td>\n",
       "<td>1.4284925</td>\n",
       "<td>0.0013978</td>\n",
       "<td>0.1315578</td>\n",
       "<td>0.7019668</td>\n",
       "<td>0.6928717</td>\n",
       "<td>0.0002844</td>\n",
       "<td>0.9999390</td>\n",
       "<td>-99.7155571</td>\n",
       "<td>42.8492506</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0258693</td>\n",
       "<td>0.0004063</td>\n",
       "<td>1.2499840</td>\n",
       "<td>0.0001997</td>\n",
       "<td>0.0522786</td>\n",
       "<td>0.6142470</td>\n",
       "<td>0.6127985</td>\n",
       "<td>0.0000406</td>\n",
       "<td>0.9999797</td>\n",
       "<td>-99.9593653</td>\n",
       "<td>24.9983964</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0024457</td>\n",
       "<td>0.0002032</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0000998</td>\n",
       "<td>0.0114396</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459816</td>\n",
       "<td>0.0000203</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9796826</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0005664</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4914357</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100038                   0.999679           2.03499      2.03499            1                0.999849     1                           0.999849            0.0203576       0.0203576                  103.499   103.499\n",
       "    2        0.0200076                   0.999198           2.03499      2.03499            1                0.999455     1                           0.999652            0.0203576       0.0407152                  103.499   103.499\n",
       "    3        0.0300014                   0.99854            2.03499      2.03499            1                0.998884     1                           0.999396            0.0203373       0.0610524                  103.499   103.499\n",
       "    4        0.0400052                   0.997703           2.03499      2.03499            1                0.998141     1                           0.999082            0.0203576       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.996734           2.03499      2.03499            1                0.997231     1                           0.998712            0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.98926            2.03499      2.03499            1                0.99334      1                           0.996026            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.977244           2.03499      2.03499            1                0.983717     1                           0.991923            0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.959886           2.03499      2.03499            1                0.969046     1                           0.986204            0.101747        0.407009                   103.499   103.499\n",
       "    9        0.300004                    0.902505           2.03478      2.03492            0.9999           0.934277     0.999967                    0.968896            0.203474        0.610484                   103.478   103.492\n",
       "    10       0.400002                    0.795727           2.03031      2.03377            0.997704         0.854905     0.999401                    0.940399            0.203027        0.813511                   103.031   103.377\n",
       "    11       0.5                         0.456285           1.74546      1.97611            0.857728         0.66546      0.971067                    0.885412            0.174543        0.988054                   74.5464   97.6107\n",
       "    12       0.599998                    0.188573           0.116012     1.6661             0.0570088        0.291464     0.818726                    0.786422            0.011601        0.999655                   -88.3988  66.6097\n",
       "    13       0.699996                    0.0840321          0.00284443   1.42849            0.00139776       0.131558     0.701967                    0.692872            0.000284437     0.999939                   -99.7156  42.8493\n",
       "    14       0.799994                    0.0258693          0.000406347  1.24998            0.000199681      0.0522786    0.614247                    0.612799            4.06339e-05     0.99998                    -99.9594  24.9984\n",
       "    15       0.899992                    0.00244571         0.000203174  1.11112            9.98403e-05      0.0114396    0.546009                    0.545982            2.03169e-05     1                          -99.9797  11.1121\n",
       "    16       1                           2.80496e-09        0            1                  0                0.000566351  0.491404                    0.491436            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10046947063102081\n",
      "RMSE: 0.3169691950821417\n",
      "LogLoss: 0.3369564875468102\n",
      "Mean Per-Class Error: 0.12966063687126939\n",
      "AUC: 0.932015943966922\n",
      "pr_auc: 0.8811035983848589\n",
      "Gini: 0.864031887933844\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4283395806948344: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10763.0</td>\n",
       "<td>1982.0</td>\n",
       "<td>0.1555</td>\n",
       "<td> (1982.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1312.0</td>\n",
       "<td>10978.0</td>\n",
       "<td>0.1068</td>\n",
       "<td> (1312.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12075.0</td>\n",
       "<td>12960.0</td>\n",
       "<td>0.1316</td>\n",
       "<td> (3294.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10763  1982   0.1555   (1982.0/12745.0)\n",
       "1      1312   10978  0.1068   (1312.0/12290.0)\n",
       "Total  12075  12960  0.1316   (3294.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4283396</td>\n",
       "<td>0.8695446</td>\n",
       "<td>223.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1666269</td>\n",
       "<td>0.8995774</td>\n",
       "<td>311.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6628779</td>\n",
       "<td>0.8771194</td>\n",
       "<td>152.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5130556</td>\n",
       "<td>0.8703815</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9995314</td>\n",
       "<td>0.9963768</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0001035</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9995314</td>\n",
       "<td>0.9998431</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5130556</td>\n",
       "<td>0.7406777</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5047667</td>\n",
       "<td>0.8700667</td>\n",
       "<td>201.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5130556</td>\n",
       "<td>0.8703394</td>\n",
       "<td>199.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.42834      0.869545  223\n",
       "max f2                       0.166627     0.899577  311\n",
       "max f0point5                 0.662878     0.877119  152\n",
       "max accuracy                 0.513056     0.870381  199\n",
       "max precision                0.999531     0.996377  0\n",
       "max recall                   0.000103518  1         399\n",
       "max specificity              0.999531     0.999843  0\n",
       "max absolute_mcc             0.513056     0.740678  199\n",
       "max min_per_class_accuracy   0.504767     0.870067  201\n",
       "max mean_per_class_accuracy  0.513056     0.870339  199"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.16 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9996639</td>\n",
       "<td>2.0289063</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9998404</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9998404</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0203417</td>\n",
       "<td>102.8906344</td>\n",
       "<td>102.8906344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9991361</td>\n",
       "<td>2.0288739</td>\n",
       "<td>2.0288901</td>\n",
       "<td>0.996</td>\n",
       "<td>0.9994041</td>\n",
       "<td>0.9960080</td>\n",
       "<td>0.9996227</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0406021</td>\n",
       "<td>102.8873881</td>\n",
       "<td>102.8890145</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9984819</td>\n",
       "<td>2.0126751</td>\n",
       "<td>2.0234779</td>\n",
       "<td>0.9880478</td>\n",
       "<td>0.9988215</td>\n",
       "<td>0.9933511</td>\n",
       "<td>0.9993553</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.0607811</td>\n",
       "<td>101.2675093</td>\n",
       "<td>102.3477940</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9975225</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0268572</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9980150</td>\n",
       "<td>0.9950100</td>\n",
       "<td>0.9990209</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0811229</td>\n",
       "<td>103.7021969</td>\n",
       "<td>102.6857189</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9964268</td>\n",
       "<td>2.0207258</td>\n",
       "<td>2.0256329</td>\n",
       "<td>0.992</td>\n",
       "<td>0.9969938</td>\n",
       "<td>0.9944089</td>\n",
       "<td>0.9986161</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.1013019</td>\n",
       "<td>102.0725793</td>\n",
       "<td>102.5632869</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9879323</td>\n",
       "<td>1.9621793</td>\n",
       "<td>1.9939061</td>\n",
       "<td>0.9632588</td>\n",
       "<td>0.9926064</td>\n",
       "<td>0.9788339</td>\n",
       "<td>0.9956113</td>\n",
       "<td>0.0981286</td>\n",
       "<td>0.1994304</td>\n",
       "<td>96.2179309</td>\n",
       "<td>99.3906089</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9740753</td>\n",
       "<td>1.9524172</td>\n",
       "<td>1.9800765</td>\n",
       "<td>0.9584665</td>\n",
       "<td>0.9814795</td>\n",
       "<td>0.9720447</td>\n",
       "<td>0.9909007</td>\n",
       "<td>0.0976404</td>\n",
       "<td>0.2970708</td>\n",
       "<td>95.2417223</td>\n",
       "<td>98.0076467</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9524959</td>\n",
       "<td>1.9083851</td>\n",
       "<td>1.9621644</td>\n",
       "<td>0.9368505</td>\n",
       "<td>0.9638570</td>\n",
       "<td>0.9632514</td>\n",
       "<td>0.9841438</td>\n",
       "<td>0.0953621</td>\n",
       "<td>0.3924329</td>\n",
       "<td>90.8385090</td>\n",
       "<td>96.2164361</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.8794708</td>\n",
       "<td>1.8051724</td>\n",
       "<td>1.9098267</td>\n",
       "<td>0.8861821</td>\n",
       "<td>0.9200663</td>\n",
       "<td>0.9375582</td>\n",
       "<td>0.9627818</td>\n",
       "<td>0.1805533</td>\n",
       "<td>0.5729862</td>\n",
       "<td>80.5172424</td>\n",
       "<td>90.9826748</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.7401755</td>\n",
       "<td>1.7049784</td>\n",
       "<td>1.8586249</td>\n",
       "<td>0.8369956</td>\n",
       "<td>0.8176581</td>\n",
       "<td>0.9124226</td>\n",
       "<td>0.9265081</td>\n",
       "<td>0.1704638</td>\n",
       "<td>0.7434500</td>\n",
       "<td>70.4978436</td>\n",
       "<td>85.8624898</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4826614</td>\n",
       "<td>1.3243897</td>\n",
       "<td>1.7517608</td>\n",
       "<td>0.6501597</td>\n",
       "<td>0.6226465</td>\n",
       "<td>0.8599617</td>\n",
       "<td>0.8657261</td>\n",
       "<td>0.1324654</td>\n",
       "<td>0.8759154</td>\n",
       "<td>32.4389683</td>\n",
       "<td>75.1760784</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.2383248</td>\n",
       "<td>0.6339753</td>\n",
       "<td>1.5655004</td>\n",
       "<td>0.3112265</td>\n",
       "<td>0.3496510</td>\n",
       "<td>0.7685241</td>\n",
       "<td>0.7797307</td>\n",
       "<td>0.0633849</td>\n",
       "<td>0.9393002</td>\n",
       "<td>-36.6024725</td>\n",
       "<td>56.5500407</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.1034971</td>\n",
       "<td>0.3149531</td>\n",
       "<td>1.3868814</td>\n",
       "<td>0.1546145</td>\n",
       "<td>0.1629826</td>\n",
       "<td>0.6808377</td>\n",
       "<td>0.6916390</td>\n",
       "<td>0.0314890</td>\n",
       "<td>0.9707893</td>\n",
       "<td>-68.5046943</td>\n",
       "<td>38.6881369</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0300866</td>\n",
       "<td>0.1862931</td>\n",
       "<td>1.2367779</td>\n",
       "<td>0.0914537</td>\n",
       "<td>0.0614741</td>\n",
       "<td>0.6071500</td>\n",
       "<td>0.6128526</td>\n",
       "<td>0.0186330</td>\n",
       "<td>0.9894223</td>\n",
       "<td>-81.3706857</td>\n",
       "<td>23.6777868</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0027087</td>\n",
       "<td>0.0952184</td>\n",
       "<td>1.1099604</td>\n",
       "<td>0.0467439</td>\n",
       "<td>0.0129805</td>\n",
       "<td>0.5448937</td>\n",
       "<td>0.5462120</td>\n",
       "<td>0.0095199</td>\n",
       "<td>0.9989422</td>\n",
       "<td>-90.4781634</td>\n",
       "<td>10.9960442</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0105756</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0051917</td>\n",
       "<td>0.0006224</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4916421</td>\n",
       "<td>0.0010578</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.9424407</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.999664           2.02891    2.02891            0.996016         0.99984      0.996016                    0.99984             0.0203417       0.0203417                  102.891   102.891\n",
       "    2        0.020012                    0.999136           2.02887    2.02889            0.996            0.999404     0.996008                    0.999623            0.0202604       0.0406021                  102.887   102.889\n",
       "    3        0.0300379                   0.998482           2.01268    2.02348            0.988048         0.998821     0.993351                    0.999355            0.020179        0.0607811                  101.268   102.348\n",
       "    4        0.040024                    0.997522           2.03702    2.02686            1                0.998015     0.99501                     0.999021            0.0203417       0.0811229                  103.702   102.686\n",
       "    5        0.05001                     0.996427           2.02073    2.02563            0.992            0.996994     0.994409                    0.998616            0.020179        0.101302                   102.073   102.563\n",
       "    6        0.10002                     0.987932           1.96218    1.99391            0.963259         0.992606     0.978834                    0.995611            0.0981286       0.19943                    96.2179   99.3906\n",
       "    7        0.15003                     0.974075           1.95242    1.98008            0.958466         0.981479     0.972045                    0.990901            0.0976404       0.297071                   95.2417   98.0076\n",
       "    8        0.2                         0.952496           1.90839    1.96216            0.936851         0.963857     0.963251                    0.984144            0.0953621       0.392433                   90.8385   96.2164\n",
       "    9        0.30002                     0.879471           1.80517    1.90983            0.886182         0.920066     0.937558                    0.962782            0.180553        0.572986                   80.5172   90.9827\n",
       "    10       0.4                         0.740175           1.70498    1.85862            0.836996         0.817658     0.912423                    0.926508            0.170464        0.74345                    70.4978   85.8625\n",
       "    11       0.50002                     0.482661           1.32439    1.75176            0.65016          0.622647     0.859962                    0.865726            0.132465        0.875915                   32.439    75.1761\n",
       "    12       0.6                         0.238325           0.633975   1.5655             0.311227         0.349651     0.768524                    0.779731            0.0633849       0.9393                     -36.6025  56.55\n",
       "    13       0.69998                     0.103497           0.314953   1.38688            0.154614         0.162983     0.680838                    0.691639            0.031489        0.970789                   -68.5047  38.6881\n",
       "    14       0.8                         0.0300866          0.186293   1.23678            0.0914537        0.0614741    0.60715                     0.612853            0.018633        0.989422                   -81.3707  23.6778\n",
       "    15       0.89998                     0.00270865         0.0952184  1.10996            0.0467439        0.0129805    0.544894                    0.546212            0.00951993      0.998942                   -90.4782  10.996\n",
       "    16       1                           2.23861e-09        0.0105756  1                  0.00519169       0.000622376  0.490913                    0.491642            0.00105777      1                          -98.9424  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:36</td>\n",
       "<td> 0.046 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:39</td>\n",
       "<td> 3.156 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3386555</td>\n",
       "<td>0.3583819</td>\n",
       "<td>0.9225760</td>\n",
       "<td>0.9134136</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1685669</td>\n",
       "<td>0.3577813</td>\n",
       "<td>0.3945805</td>\n",
       "<td>0.9013514</td>\n",
       "<td>0.8937902</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1920911</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:41</td>\n",
       "<td> 5.351 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.3056252</td>\n",
       "<td>0.3027760</td>\n",
       "<td>0.9511343</td>\n",
       "<td>0.9396547</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1288513</td>\n",
       "<td>0.3472095</td>\n",
       "<td>0.3758926</td>\n",
       "<td>0.9115324</td>\n",
       "<td>0.8985954</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1742361</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:43</td>\n",
       "<td> 7.527 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2799604</td>\n",
       "<td>0.2637360</td>\n",
       "<td>0.9681745</td>\n",
       "<td>0.9553931</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0995188</td>\n",
       "<td>0.3402906</td>\n",
       "<td>0.3650817</td>\n",
       "<td>0.9173836</td>\n",
       "<td>0.8997567</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1656081</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:46</td>\n",
       "<td> 9.831 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.2526147</td>\n",
       "<td>0.2252364</td>\n",
       "<td>0.9814482</td>\n",
       "<td>0.9639248</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0733112</td>\n",
       "<td>0.3329016</td>\n",
       "<td>0.3539880</td>\n",
       "<td>0.9230441</td>\n",
       "<td>0.8991852</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1528660</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:48</td>\n",
       "<td>12.245 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.2229349</td>\n",
       "<td>0.1875576</td>\n",
       "<td>0.9909765</td>\n",
       "<td>0.9695273</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0499691</td>\n",
       "<td>0.3256277</td>\n",
       "<td>0.3439567</td>\n",
       "<td>0.9278364</td>\n",
       "<td>0.8991950</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1445576</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:50</td>\n",
       "<td>14.677 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1990982</td>\n",
       "<td>0.1600627</td>\n",
       "<td>0.9956311</td>\n",
       "<td>0.9686642</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0335556</td>\n",
       "<td>0.3212799</td>\n",
       "<td>0.3396280</td>\n",
       "<td>0.9300337</td>\n",
       "<td>0.8983580</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1375275</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:53</td>\n",
       "<td>17.169 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.1800805</td>\n",
       "<td>0.1394170</td>\n",
       "<td>0.9977719</td>\n",
       "<td>0.9667411</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0228829</td>\n",
       "<td>0.3178589</td>\n",
       "<td>0.3368743</td>\n",
       "<td>0.9317130</td>\n",
       "<td>0.8950806</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1326143</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:55</td>\n",
       "<td>18.788 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.1703839</td>\n",
       "<td>0.1293547</td>\n",
       "<td>0.9984687</td>\n",
       "<td>0.9624730</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0188295</td>\n",
       "<td>0.3169692</td>\n",
       "<td>0.3369565</td>\n",
       "<td>0.9320159</td>\n",
       "<td>0.8811036</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1315758</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:38:36  0.046 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:38:39  3.156 sec   20                 0.338655         0.358382            0.922576        0.913414           2.03499          0.168567                         0.357781           0.394581              0.901351          0.89379              2.03702            0.192091\n",
       "    2020-07-16 22:38:41  5.351 sec   40                 0.305625         0.302776            0.951134        0.939655           2.03499          0.128851                         0.34721            0.375893              0.911532          0.898595             2.02079            0.174236\n",
       "    2020-07-16 22:38:43  7.527 sec   60                 0.27996          0.263736            0.968175        0.955393           2.03499          0.0995188                        0.340291           0.365082              0.917384          0.899757             2.03702            0.165608\n",
       "    2020-07-16 22:38:46  9.831 sec   80                 0.252615         0.225236            0.981448        0.963925           2.03499          0.0733112                        0.332902           0.353988              0.923044          0.899185             2.03702            0.152866\n",
       "    2020-07-16 22:38:48  12.245 sec  100                0.222935         0.187558            0.990976        0.969527           2.03499          0.0499691                        0.325628           0.343957              0.927836          0.899195             2.03702            0.144558\n",
       "    2020-07-16 22:38:50  14.677 sec  120                0.199098         0.160063            0.995631        0.968664           2.03499          0.0335556                        0.32128            0.339628              0.930034          0.898358             2.02891            0.137527\n",
       "    2020-07-16 22:38:53  17.169 sec  140                0.18008          0.139417            0.997772        0.966741           2.03499          0.0228829                        0.317859           0.336874              0.931713          0.895081             2.02891            0.132614\n",
       "    2020-07-16 22:38:55  18.788 sec  150                0.170384         0.129355            0.998469        0.962473           2.03499          0.0188295                        0.316969           0.336956              0.932016          0.881104             2.02891            0.131576"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>58606.1718750</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3471041</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>9875.5371094</td>\n",
       "<td>0.1685068</td>\n",
       "<td>0.0584894</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>5422.6391602</td>\n",
       "<td>0.0925268</td>\n",
       "<td>0.0321164</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>5236.8603516</td>\n",
       "<td>0.0893568</td>\n",
       "<td>0.0310161</td></tr>\n",
       "<tr><td>e18</td>\n",
       "<td>4693.5424805</td>\n",
       "<td>0.0800861</td>\n",
       "<td>0.0277982</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>a13.E</td>\n",
       "<td>2.0896964</td>\n",
       "<td>0.0000357</td>\n",
       "<td>0.0000124</td></tr>\n",
       "<tr><td>c05.C</td>\n",
       "<td>2.0535100</td>\n",
       "<td>0.0000350</td>\n",
       "<td>0.0000122</td></tr>\n",
       "<tr><td>e22.B</td>\n",
       "<td>1.7597125</td>\n",
       "<td>0.0000300</td>\n",
       "<td>0.0000104</td></tr>\n",
       "<tr><td>a12.D</td>\n",
       "<td>1.5873065</td>\n",
       "<td>0.0000271</td>\n",
       "<td>0.0000094</td></tr>\n",
       "<tr><td>e13.D</td>\n",
       "<td>1.3814276</td>\n",
       "<td>0.0000236</td>\n",
       "<td>0.0000082</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         58606.171875           1.0                     0.34710406959142454\n",
       "e19         9875.537109375         0.16850677656336857     0.05848938789887811\n",
       "f02         5422.63916015625       0.09252675932702267     0.03211641470851587\n",
       "e12         5236.8603515625        0.0893568063570523      0.03101611113222573\n",
       "e18         4693.54248046875       0.08008614673689178     0.02779822745027112\n",
       "---         ---                    ---                     ---\n",
       "a13.E       2.0896964073181152     3.565659282055121e-05   1.2376548475777695e-05\n",
       "c05.C       2.0535099506378174     3.5039141526215194e-05  1.216222861873917e-05\n",
       "e22.B       1.7597124576568604     3.0026060419201546e-05  1.0422167765302851e-05\n",
       "a12.D       1.5873064994812012     2.7084289055199464e-05  9.401066953050212e-06\n",
       "e13.D       1.3814276456832886     2.357136802297392e-05   8.181717766611419e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1088\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.020781632241614403\n",
      "RMSE: 0.144158358209347\n",
      "LogLoss: 0.10178268770022122\n",
      "Mean Per-Class Error: 0.0109904634396224\n",
      "AUC: 0.9993852108279208\n",
      "pr_auc: 0.947660991730197\n",
      "Gini: 0.9987704216558415\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5062346437147686: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>50377.0</td>\n",
       "<td>565.0</td>\n",
       "<td>0.0111</td>\n",
       "<td> (565.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>536.0</td>\n",
       "<td>48684.0</td>\n",
       "<td>0.0109</td>\n",
       "<td> (536.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50913.0</td>\n",
       "<td>49249.0</td>\n",
       "<td>0.011</td>\n",
       "<td> (1101.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  -----------------\n",
       "0      50377  565    0.0111   (565.0/50942.0)\n",
       "1      536    48684  0.0109   (536.0/49220.0)\n",
       "Total  50913  49249  0.011    (1101.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5062346</td>\n",
       "<td>0.9888188</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4280604</td>\n",
       "<td>0.9913406</td>\n",
       "<td>214.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5790262</td>\n",
       "<td>0.9913225</td>\n",
       "<td>180.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5131847</td>\n",
       "<td>0.9890178</td>\n",
       "<td>195.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9997592</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0969914</td>\n",
       "<td>1.0</td>\n",
       "<td>334.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9997592</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5131847</td>\n",
       "<td>0.9780294</td>\n",
       "<td>195.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5062346</td>\n",
       "<td>0.9889090</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5062346</td>\n",
       "<td>0.9890095</td>\n",
       "<td>197.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.506235     0.988819  197\n",
       "max f2                       0.42806      0.991341  214\n",
       "max f0point5                 0.579026     0.991323  180\n",
       "max accuracy                 0.513185     0.989018  195\n",
       "max precision                0.999759     1         0\n",
       "max recall                   0.0969914    1         334\n",
       "max specificity              0.999759     1         0\n",
       "max absolute_mcc             0.513185     0.978029  195\n",
       "max min_per_class_accuracy   0.506235     0.988909  197\n",
       "max mean_per_class_accuracy  0.506235     0.98901   197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100038</td>\n",
       "<td>0.9998714</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999418</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999418</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0203576</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200176</td>\n",
       "<td>0.9996303</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997580</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998499</td>\n",
       "<td>0.0203779</td>\n",
       "<td>0.0407355</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300014</td>\n",
       "<td>0.9992855</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994686</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997230</td>\n",
       "<td>0.0203169</td>\n",
       "<td>0.0610524</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9988241</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9990579</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995567</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9982638</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9985514</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9993556</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9936128</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9962452</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9978006</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9854173</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9898334</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9951450</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9720965</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9791303</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9911416</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9264111</td>\n",
       "<td>2.0345794</td>\n",
       "<td>2.0348503</td>\n",
       "<td>0.9998003</td>\n",
       "<td>0.9517112</td>\n",
       "<td>0.9999334</td>\n",
       "<td>0.9779986</td>\n",
       "<td>0.2034539</td>\n",
       "<td>0.6104632</td>\n",
       "<td>103.4579431</td>\n",
       "<td>103.4850334</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.8372134</td>\n",
       "<td>2.0335636</td>\n",
       "<td>2.0345286</td>\n",
       "<td>0.9993011</td>\n",
       "<td>0.8875199</td>\n",
       "<td>0.9997754</td>\n",
       "<td>0.9553794</td>\n",
       "<td>0.2033523</td>\n",
       "<td>0.8138155</td>\n",
       "<td>103.3563564</td>\n",
       "<td>103.4528649</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.4290267</td>\n",
       "<td>1.8098695</td>\n",
       "<td>1.9895977</td>\n",
       "<td>0.8893770</td>\n",
       "<td>0.7045545</td>\n",
       "<td>0.9776961</td>\n",
       "<td>0.9052155</td>\n",
       "<td>0.1809833</td>\n",
       "<td>0.9947989</td>\n",
       "<td>80.9869540</td>\n",
       "<td>98.9597725</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.1474401</td>\n",
       "<td>0.0511997</td>\n",
       "<td>1.6665368</td>\n",
       "<td>0.0251597</td>\n",
       "<td>0.2446318</td>\n",
       "<td>0.8189427</td>\n",
       "<td>0.7951200</td>\n",
       "<td>0.0051199</td>\n",
       "<td>0.9999187</td>\n",
       "<td>-94.8800278</td>\n",
       "<td>66.6536767</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0612361</td>\n",
       "<td>0.0008127</td>\n",
       "<td>1.4285796</td>\n",
       "<td>0.0003994</td>\n",
       "<td>0.0997107</td>\n",
       "<td>0.7020096</td>\n",
       "<td>0.6957772</td>\n",
       "<td>0.0000813</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.9187306</td>\n",
       "<td>42.8579579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0164802</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0363804</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6133537</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0010871</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0067012</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459486</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0002228</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4913716</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift         cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100038                   0.999871           2.03499      2.03499            1                0.999942     1                           0.999942            0.0203576       0.0203576                  103.499   103.499\n",
       "    2        0.0200176                   0.99963            2.03499      2.03499            1                0.999758     1                           0.99985             0.0203779       0.0407355                  103.499   103.499\n",
       "    3        0.0300014                   0.999286           2.03499      2.03499            1                0.999469     1                           0.999723            0.0203169       0.0610524                  103.499   103.499\n",
       "    4        0.0400052                   0.998824           2.03499      2.03499            1                0.999058     1                           0.999557            0.0203576       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.998264           2.03499      2.03499            1                0.998551     1                           0.999356            0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.993613           2.03499      2.03499            1                0.996245     1                           0.997801            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.985417           2.03499      2.03499            1                0.989833     1                           0.995145            0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.972097           2.03499      2.03499            1                0.97913      1                           0.991142            0.101747        0.407009                   103.499   103.499\n",
       "    9        0.300004                    0.926411           2.03458      2.03485            0.9998           0.951711     0.999933                    0.977999            0.203454        0.610463                   103.458   103.485\n",
       "    10       0.400002                    0.837213           2.03356      2.03453            0.999301         0.88752      0.999775                    0.955379            0.203352        0.813816                   103.356   103.453\n",
       "    11       0.5                         0.429027           1.80987      1.9896             0.889377         0.704555     0.977696                    0.905215            0.180983        0.994799                   80.987    98.9598\n",
       "    12       0.599998                    0.14744            0.0511997    1.66654            0.0251597        0.244632     0.818943                    0.79512             0.00511987      0.999919                   -94.88    66.6537\n",
       "    13       0.699996                    0.0612361          0.000812694  1.42858            0.000399361      0.0997107    0.70201                     0.695777            8.12678e-05     1                          -99.9187  42.858\n",
       "    14       0.799994                    0.0164802          0            1.25001            0                0.0363804    0.61426                     0.613354            0               1                          -100      25.0009\n",
       "    15       0.899992                    0.00108707         0            1.11112            0                0.00670122   0.546009                    0.545949            0               1                          -100      11.1121\n",
       "    16       1                           4.5533e-11         0            1                  0                0.000222765  0.491404                    0.491372            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10053556408557983\n",
      "RMSE: 0.31707343642377206\n",
      "LogLoss: 0.3475803755538777\n",
      "Mean Per-Class Error: 0.12989761935390987\n",
      "AUC: 0.9313070969294744\n",
      "pr_auc: 0.872623583671625\n",
      "Gini: 0.8626141938589489\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4470094817656058: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10865.0</td>\n",
       "<td>1880.0</td>\n",
       "<td>0.1475</td>\n",
       "<td> (1880.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1380.0</td>\n",
       "<td>10910.0</td>\n",
       "<td>0.1123</td>\n",
       "<td> (1380.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12245.0</td>\n",
       "<td>12790.0</td>\n",
       "<td>0.1302</td>\n",
       "<td> (3260.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10865  1880   0.1475   (1880.0/12745.0)\n",
       "1      1380   10910  0.1123   (1380.0/12290.0)\n",
       "Total  12245  12790  0.1302   (3260.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4470095</td>\n",
       "<td>0.8700159</td>\n",
       "<td>217.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1597181</td>\n",
       "<td>0.8998598</td>\n",
       "<td>311.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6528821</td>\n",
       "<td>0.8766856</td>\n",
       "<td>157.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5527022</td>\n",
       "<td>0.8700220</td>\n",
       "<td>187.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9984827</td>\n",
       "<td>0.9894737</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000828</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9997674</td>\n",
       "<td>0.9994508</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4470095</td>\n",
       "<td>0.7402579</td>\n",
       "<td>217.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5094209</td>\n",
       "<td>0.8693605</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4470095</td>\n",
       "<td>0.8701024</td>\n",
       "<td>217.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.447009     0.870016  217\n",
       "max f2                       0.159718     0.89986   311\n",
       "max f0point5                 0.652882     0.876686  157\n",
       "max accuracy                 0.552702     0.870022  187\n",
       "max precision                0.998483     0.989474  2\n",
       "max recall                   8.28055e-05  1         399\n",
       "max specificity              0.999767     0.999451  0\n",
       "max absolute_mcc             0.447009     0.740258  217\n",
       "max min_per_class_accuracy   0.509421     0.869361  199\n",
       "max mean_per_class_accuracy  0.447009     0.870102  217"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.26 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9998672</td>\n",
       "<td>2.0289063</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9999398</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9999398</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0203417</td>\n",
       "<td>102.8906344</td>\n",
       "<td>102.8906344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9996222</td>\n",
       "<td>2.0125777</td>\n",
       "<td>2.0207583</td>\n",
       "<td>0.988</td>\n",
       "<td>0.9997594</td>\n",
       "<td>0.9920160</td>\n",
       "<td>0.9998498</td>\n",
       "<td>0.0200976</td>\n",
       "<td>0.0404394</td>\n",
       "<td>101.2577705</td>\n",
       "<td>102.0758321</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9992666</td>\n",
       "<td>2.0126751</td>\n",
       "<td>2.0180603</td>\n",
       "<td>0.9880478</td>\n",
       "<td>0.9994558</td>\n",
       "<td>0.9906915</td>\n",
       "<td>0.9997183</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.0606184</td>\n",
       "<td>101.2675093</td>\n",
       "<td>101.8060328</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9988203</td>\n",
       "<td>2.0125777</td>\n",
       "<td>2.0166924</td>\n",
       "<td>0.988</td>\n",
       "<td>0.9990470</td>\n",
       "<td>0.9900200</td>\n",
       "<td>0.9995508</td>\n",
       "<td>0.0200976</td>\n",
       "<td>0.0807160</td>\n",
       "<td>101.2577705</td>\n",
       "<td>101.6692409</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9982264</td>\n",
       "<td>1.9962815</td>\n",
       "<td>2.0126168</td>\n",
       "<td>0.98</td>\n",
       "<td>0.9985369</td>\n",
       "<td>0.9880192</td>\n",
       "<td>0.9993483</td>\n",
       "<td>0.0199349</td>\n",
       "<td>0.1006509</td>\n",
       "<td>99.6281530</td>\n",
       "<td>101.2616754</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9928385</td>\n",
       "<td>1.9491632</td>\n",
       "<td>1.9808900</td>\n",
       "<td>0.9568690</td>\n",
       "<td>0.9959225</td>\n",
       "<td>0.9724441</td>\n",
       "<td>0.9976354</td>\n",
       "<td>0.0974776</td>\n",
       "<td>0.1981286</td>\n",
       "<td>94.9163194</td>\n",
       "<td>98.0889974</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9829946</td>\n",
       "<td>1.9394011</td>\n",
       "<td>1.9670604</td>\n",
       "<td>0.9520767</td>\n",
       "<td>0.9884123</td>\n",
       "<td>0.9656550</td>\n",
       "<td>0.9945610</td>\n",
       "<td>0.0969894</td>\n",
       "<td>0.2951180</td>\n",
       "<td>93.9401108</td>\n",
       "<td>96.7060352</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9659533</td>\n",
       "<td>1.8921019</td>\n",
       "<td>1.9483320</td>\n",
       "<td>0.9288569</td>\n",
       "<td>0.9751054</td>\n",
       "<td>0.9564610</td>\n",
       "<td>0.9897000</td>\n",
       "<td>0.0945484</td>\n",
       "<td>0.3896664</td>\n",
       "<td>89.2101941</td>\n",
       "<td>94.8331977</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.9036252</td>\n",
       "<td>1.8360857</td>\n",
       "<td>1.9109116</td>\n",
       "<td>0.9013578</td>\n",
       "<td>0.9389797</td>\n",
       "<td>0.9380908</td>\n",
       "<td>0.9727910</td>\n",
       "<td>0.1836452</td>\n",
       "<td>0.5733116</td>\n",
       "<td>83.6085697</td>\n",
       "<td>91.0911569</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.7761711</td>\n",
       "<td>1.6903295</td>\n",
       "<td>1.8557771</td>\n",
       "<td>0.8298042</td>\n",
       "<td>0.8485556</td>\n",
       "<td>0.9110246</td>\n",
       "<td>0.9417384</td>\n",
       "<td>0.1689992</td>\n",
       "<td>0.7423108</td>\n",
       "<td>69.0329457</td>\n",
       "<td>85.5777055</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4855365</td>\n",
       "<td>1.3422868</td>\n",
       "<td>1.7530626</td>\n",
       "<td>0.6589457</td>\n",
       "<td>0.6465387</td>\n",
       "<td>0.8606007</td>\n",
       "<td>0.8826890</td>\n",
       "<td>0.1342555</td>\n",
       "<td>0.8765663</td>\n",
       "<td>34.2286841</td>\n",
       "<td>75.3062604</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.2040725</td>\n",
       "<td>0.6258370</td>\n",
       "<td>1.5652292</td>\n",
       "<td>0.3072313</td>\n",
       "<td>0.3255229</td>\n",
       "<td>0.7683909</td>\n",
       "<td>0.7898465</td>\n",
       "<td>0.0625712</td>\n",
       "<td>0.9391375</td>\n",
       "<td>-37.4163047</td>\n",
       "<td>56.5229184</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.0786247</td>\n",
       "<td>0.3222775</td>\n",
       "<td>1.3876951</td>\n",
       "<td>0.1582101</td>\n",
       "<td>0.1338244</td>\n",
       "<td>0.6812372</td>\n",
       "<td>0.6961451</td>\n",
       "<td>0.0322213</td>\n",
       "<td>0.9713588</td>\n",
       "<td>-67.7722453</td>\n",
       "<td>38.7695062</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0192510</td>\n",
       "<td>0.1757176</td>\n",
       "<td>1.2361676</td>\n",
       "<td>0.0862620</td>\n",
       "<td>0.0451488</td>\n",
       "<td>0.6068504</td>\n",
       "<td>0.6147543</td>\n",
       "<td>0.0175753</td>\n",
       "<td>0.9889341</td>\n",
       "<td>-82.4282450</td>\n",
       "<td>23.6167616</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0011549</td>\n",
       "<td>0.0992875</td>\n",
       "<td>1.1098700</td>\n",
       "<td>0.0487415</td>\n",
       "<td>0.0076033</td>\n",
       "<td>0.5448493</td>\n",
       "<td>0.5473051</td>\n",
       "<td>0.0099268</td>\n",
       "<td>0.9988609</td>\n",
       "<td>-90.0712473</td>\n",
       "<td>10.9870032</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0113891</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0055911</td>\n",
       "<td>0.0002377</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4925874</td>\n",
       "<td>0.0011391</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.8610900</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.999867           2.02891    2.02891            0.996016         0.99994      0.996016                    0.99994             0.0203417       0.0203417                  102.891   102.891\n",
       "    2        0.020012                    0.999622           2.01258    2.02076            0.988            0.999759     0.992016                    0.99985             0.0200976       0.0404394                  101.258   102.076\n",
       "    3        0.0300379                   0.999267           2.01268    2.01806            0.988048         0.999456     0.990691                    0.999718            0.020179        0.0606184                  101.268   101.806\n",
       "    4        0.040024                    0.99882            2.01258    2.01669            0.988            0.999047     0.99002                     0.999551            0.0200976       0.080716                   101.258   101.669\n",
       "    5        0.05001                     0.998226           1.99628    2.01262            0.98             0.998537     0.988019                    0.999348            0.0199349       0.100651                   99.6282   101.262\n",
       "    6        0.10002                     0.992839           1.94916    1.98089            0.956869         0.995922     0.972444                    0.997635            0.0974776       0.198129                   94.9163   98.089\n",
       "    7        0.15003                     0.982995           1.9394     1.96706            0.952077         0.988412     0.965655                    0.994561            0.0969894       0.295118                   93.9401   96.706\n",
       "    8        0.2                         0.965953           1.8921     1.94833            0.928857         0.975105     0.956461                    0.9897              0.0945484       0.389666                   89.2102   94.8332\n",
       "    9        0.30002                     0.903625           1.83609    1.91091            0.901358         0.93898      0.938091                    0.972791            0.183645        0.573312                   83.6086   91.0912\n",
       "    10       0.4                         0.776171           1.69033    1.85578            0.829804         0.848556     0.911025                    0.941738            0.168999        0.742311                   69.0329   85.5777\n",
       "    11       0.50002                     0.485537           1.34229    1.75306            0.658946         0.646539     0.860601                    0.882689            0.134255        0.876566                   34.2287   75.3063\n",
       "    12       0.6                         0.204072           0.625837   1.56523            0.307231         0.325523     0.768391                    0.789847            0.0625712       0.939138                   -37.4163  56.5229\n",
       "    13       0.69998                     0.0786247          0.322278   1.3877             0.15821          0.133824     0.681237                    0.696145            0.0322213       0.971359                   -67.7722  38.7695\n",
       "    14       0.8                         0.019251           0.175718   1.23617            0.086262         0.0451488    0.60685                     0.614754            0.0175753       0.988934                   -82.4282  23.6168\n",
       "    15       0.89998                     0.00115491         0.0992875  1.10987            0.0487415        0.00760329   0.544849                    0.547305            0.00992677      0.998861                   -90.0712  10.987\n",
       "    16       1                           1.16959e-10        0.0113891  1                  0.00559105       0.000237673  0.490913                    0.492587            0.00113914      1                          -98.8611  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:56</td>\n",
       "<td> 0.042 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:38:59</td>\n",
       "<td> 3.158 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3280079</td>\n",
       "<td>0.3396261</td>\n",
       "<td>0.9320165</td>\n",
       "<td>0.9235968</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1536111</td>\n",
       "<td>0.3557247</td>\n",
       "<td>0.3916370</td>\n",
       "<td>0.9029461</td>\n",
       "<td>0.8915023</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1925305</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:01</td>\n",
       "<td> 5.373 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.2907793</td>\n",
       "<td>0.2783442</td>\n",
       "<td>0.9605277</td>\n",
       "<td>0.9475107</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1133264</td>\n",
       "<td>0.3464648</td>\n",
       "<td>0.3765142</td>\n",
       "<td>0.9115194</td>\n",
       "<td>0.8939022</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1803875</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:03</td>\n",
       "<td> 7.638 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2545804</td>\n",
       "<td>0.2263659</td>\n",
       "<td>0.9797197</td>\n",
       "<td>0.9593172</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0767257</td>\n",
       "<td>0.3372780</td>\n",
       "<td>0.3638776</td>\n",
       "<td>0.9189366</td>\n",
       "<td>0.8904464</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1602956</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:05</td>\n",
       "<td> 9.955 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.2234713</td>\n",
       "<td>0.1865897</td>\n",
       "<td>0.9899421</td>\n",
       "<td>0.9643241</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0514866</td>\n",
       "<td>0.3297627</td>\n",
       "<td>0.3545353</td>\n",
       "<td>0.9240469</td>\n",
       "<td>0.8929498</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1490314</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:08</td>\n",
       "<td>12.321 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.1962362</td>\n",
       "<td>0.1548730</td>\n",
       "<td>0.9953477</td>\n",
       "<td>0.9656754</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0334858</td>\n",
       "<td>0.3248756</td>\n",
       "<td>0.3510661</td>\n",
       "<td>0.9266772</td>\n",
       "<td>0.8895636</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1417216</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:10</td>\n",
       "<td>14.761 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1736813</td>\n",
       "<td>0.1305936</td>\n",
       "<td>0.9978116</td>\n",
       "<td>0.9620278</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0222240</td>\n",
       "<td>0.3215629</td>\n",
       "<td>0.3498503</td>\n",
       "<td>0.9283921</td>\n",
       "<td>0.8864585</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1352506</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:13</td>\n",
       "<td>17.258 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.1541116</td>\n",
       "<td>0.1110802</td>\n",
       "<td>0.9990100</td>\n",
       "<td>0.9492249</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0148060</td>\n",
       "<td>0.3183327</td>\n",
       "<td>0.3481291</td>\n",
       "<td>0.9303996</td>\n",
       "<td>0.8751193</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1322548</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:14</td>\n",
       "<td>18.871 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.1441584</td>\n",
       "<td>0.1017827</td>\n",
       "<td>0.9993852</td>\n",
       "<td>0.9476610</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0109922</td>\n",
       "<td>0.3170734</td>\n",
       "<td>0.3475804</td>\n",
       "<td>0.9313071</td>\n",
       "<td>0.8726236</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1302177</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:38:56  0.042 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:38:59  3.158 sec   20                 0.328008         0.339626            0.932016        0.923597           2.03499          0.153611                         0.355725           0.391637              0.902946          0.891502             2.02891            0.19253\n",
       "    2020-07-16 22:39:01  5.373 sec   40                 0.290779         0.278344            0.960528        0.947511           2.03499          0.113326                         0.346465           0.376514              0.911519          0.893902             2.03702            0.180387\n",
       "    2020-07-16 22:39:03  7.638 sec   60                 0.25458          0.226366            0.97972         0.959317           2.03499          0.0767257                        0.337278           0.363878              0.918937          0.890446             2.03702            0.160296\n",
       "    2020-07-16 22:39:05  9.955 sec   80                 0.223471         0.18659             0.989942        0.964324           2.03499          0.0514866                        0.329763           0.354535              0.924047          0.89295              2.03702            0.149031\n",
       "    2020-07-16 22:39:08  12.321 sec  100                0.196236         0.154873            0.995348        0.965675           2.03499          0.0334858                        0.324876           0.351066              0.926677          0.889564             2.03702            0.141722\n",
       "    2020-07-16 22:39:10  14.761 sec  120                0.173681         0.130594            0.997812        0.962028           2.03499          0.022224                         0.321563           0.34985               0.928392          0.886459             2.02891            0.135251\n",
       "    2020-07-16 22:39:13  17.258 sec  140                0.154112         0.11108             0.99901         0.949225           2.03499          0.014806                         0.318333           0.348129              0.9304            0.875119             2.02891            0.132255\n",
       "    2020-07-16 22:39:14  18.871 sec  150                0.144158         0.101783            0.999385        0.947661           2.03499          0.0109922                        0.317073           0.34758               0.931307          0.872624             2.02891            0.130218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>50913.2734375</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3367930</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>8737.2539062</td>\n",
       "<td>0.1716105</td>\n",
       "<td>0.0577972</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>5006.6196289</td>\n",
       "<td>0.0983362</td>\n",
       "<td>0.0331190</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>4532.4692383</td>\n",
       "<td>0.0890233</td>\n",
       "<td>0.0299824</td></tr>\n",
       "<tr><td>e02</td>\n",
       "<td>4526.7358398</td>\n",
       "<td>0.0889107</td>\n",
       "<td>0.0299445</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>e24.L</td>\n",
       "<td>1.4449694</td>\n",
       "<td>0.0000284</td>\n",
       "<td>0.0000096</td></tr>\n",
       "<tr><td>b02.Y</td>\n",
       "<td>1.2299147</td>\n",
       "<td>0.0000242</td>\n",
       "<td>0.0000081</td></tr>\n",
       "<tr><td>e24.C</td>\n",
       "<td>1.0355808</td>\n",
       "<td>0.0000203</td>\n",
       "<td>0.0000069</td></tr>\n",
       "<tr><td>a19.N</td>\n",
       "<td>0.7371121</td>\n",
       "<td>0.0000145</td>\n",
       "<td>0.0000049</td></tr>\n",
       "<tr><td>a03.K</td>\n",
       "<td>0.1874695</td>\n",
       "<td>0.0000037</td>\n",
       "<td>0.0000012</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         50913.2734375          1.0                     0.3367930310003618\n",
       "e19         8737.25390625          0.17161053132786008     0.05779723099749254\n",
       "e12         5006.61962890625       0.0983362351480359      0.03311895869267132\n",
       "f02         4532.46923828125       0.08902333187916522     0.02998243777333519\n",
       "e02         4526.73583984375       0.08891072080448079     0.02994451114816801\n",
       "---         ---                    ---                     ---\n",
       "e24.L       1.4449694156646729     2.8380996115649197e-05  9.558521704598988e-06\n",
       "b02.Y       1.229914665222168      2.4157053400465083e-05  8.135927234780233e-06\n",
       "e24.C       1.0355807542800903     2.0340093739039312e-05  6.850401821202533e-06\n",
       "a19.N       0.7371121048927307     1.4477798324980873e-05  4.87602158008227e-06\n",
       "a03.K       0.187469482421875      3.6821337495026393e-06  1.2401169860437209e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1107\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.015735968122159442\n",
      "RMSE: 0.1254430871836286\n",
      "LogLoss: 0.0836913374781404\n",
      "Mean Per-Class Error: 0.00701827269488664\n",
      "AUC: 0.9997560341468242\n",
      "pr_auc: 0.9308990222035537\n",
      "Gini: 0.9995120682936485\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5094583149879209: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>50613.0</td>\n",
       "<td>329.0</td>\n",
       "<td>0.0065</td>\n",
       "<td> (329.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>373.0</td>\n",
       "<td>48847.0</td>\n",
       "<td>0.0076</td>\n",
       "<td> (373.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50986.0</td>\n",
       "<td>49176.0</td>\n",
       "<td>0.007</td>\n",
       "<td> (702.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      50613  329    0.0065   (329.0/50942.0)\n",
       "1      373    48847  0.0076   (373.0/49220.0)\n",
       "Total  50986  49176  0.007    (702.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5094583</td>\n",
       "<td>0.9928656</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4091902</td>\n",
       "<td>0.9945642</td>\n",
       "<td>217.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5955454</td>\n",
       "<td>0.9943724</td>\n",
       "<td>179.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5094583</td>\n",
       "<td>0.9929914</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1593946</td>\n",
       "<td>1.0</td>\n",
       "<td>297.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998471</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5094583</td>\n",
       "<td>0.9859787</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5045983</td>\n",
       "<td>0.9927875</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5094583</td>\n",
       "<td>0.9929817</td>\n",
       "<td>196.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.509458     0.992866  196\n",
       "max f2                       0.40919      0.994564  217\n",
       "max f0point5                 0.595545     0.994372  179\n",
       "max accuracy                 0.509458     0.992991  196\n",
       "max precision                0.999847     1         0\n",
       "max recall                   0.159395     1         297\n",
       "max specificity              0.999847     1         0\n",
       "max absolute_mcc             0.509458     0.985979  196\n",
       "max min_per_class_accuracy   0.504598     0.992787  197\n",
       "max mean_per_class_accuracy  0.509458     0.992982  196"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.15 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100337</td>\n",
       "<td>0.9999547</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999812</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999812</td>\n",
       "<td>0.0204185</td>\n",
       "<td>0.0204185</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200076</td>\n",
       "<td>0.9998535</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999079</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999447</td>\n",
       "<td>0.0202966</td>\n",
       "<td>0.0407152</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300014</td>\n",
       "<td>0.9996947</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997823</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998906</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0610524</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400052</td>\n",
       "<td>0.9994574</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9995828</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998136</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0814100</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9991704</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9993157</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997140</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9962712</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9979514</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9988328</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9902395</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9935131</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9970597</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9799014</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9855098</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9941724</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9421564</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9633353</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9838937</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.6105039</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.8647097</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9080796</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9649407</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.8139984</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.3998563</td>\n",
       "<td>1.8381107</td>\n",
       "<td>1.9956115</td>\n",
       "<td>0.9032548</td>\n",
       "<td>0.7327649</td>\n",
       "<td>0.9806513</td>\n",
       "<td>0.9185064</td>\n",
       "<td>0.1838074</td>\n",
       "<td>0.9978058</td>\n",
       "<td>83.8110656</td>\n",
       "<td>99.5611540</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.1210647</td>\n",
       "<td>0.0219427</td>\n",
       "<td>1.6666722</td>\n",
       "<td>0.0107827</td>\n",
       "<td>0.2105050</td>\n",
       "<td>0.8190093</td>\n",
       "<td>0.8005082</td>\n",
       "<td>0.0021942</td>\n",
       "<td>1.0</td>\n",
       "<td>-97.8057262</td>\n",
       "<td>66.6672213</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0477066</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4285796</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0801410</td>\n",
       "<td>0.7020096</td>\n",
       "<td>0.6976000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8579579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0113945</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0270622</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6137838</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0005490</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0042906</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5460631</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0001044</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4914629</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100337                   0.999955           2.03499    2.03499            1                0.999981     1                           0.999981            0.0204185       0.0204185                  103.499   103.499\n",
       "    2        0.0200076                   0.999853           2.03499    2.03499            1                0.999908     1                           0.999945            0.0202966       0.0407152                  103.499   103.499\n",
       "    3        0.0300014                   0.999695           2.03499    2.03499            1                0.999782     1                           0.999891            0.0203373       0.0610524                  103.499   103.499\n",
       "    4        0.0400052                   0.999457           2.03499    2.03499            1                0.999583     1                           0.999814            0.0203576       0.08141                    103.499   103.499\n",
       "    5        0.050009                    0.99917            2.03499    2.03499            1                0.999316     1                           0.999714            0.0203576       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.996271           2.03499    2.03499            1                0.997951     1                           0.998833            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.990239           2.03499    2.03499            1                0.993513     1                           0.99706             0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.979901           2.03499    2.03499            1                0.98551      1                           0.994172            0.101747        0.407009                   103.499   103.499\n",
       "    9        0.300004                    0.942156           2.03499    2.03499            1                0.963335     1                           0.983894            0.203495        0.610504                   103.499   103.499\n",
       "    10       0.400002                    0.86471            2.03499    2.03499            1                0.90808      1                           0.964941            0.203495        0.813998                   103.499   103.499\n",
       "    11       0.5                         0.399856           1.83811    1.99561            0.903255         0.732765     0.980651                    0.918506            0.183807        0.997806                   83.8111   99.5612\n",
       "    12       0.599998                    0.121065           0.0219427  1.66667            0.0107827        0.210505     0.819009                    0.800508            0.00219423      1                          -97.8057  66.6672\n",
       "    13       0.699996                    0.0477066          0          1.42858            0                0.080141     0.70201                     0.6976              0               1                          -100      42.858\n",
       "    14       0.799994                    0.0113945          0          1.25001            0                0.0270622    0.61426                     0.613784            0               1                          -100      25.0009\n",
       "    15       0.899992                    0.000549007        0          1.11112            0                0.0042906    0.546009                    0.546063            0               1                          -100      11.1121\n",
       "    16       1                           3.25149e-13        0          1                  0                0.000104351  0.491404                    0.491463            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10209431863233472\n",
      "RMSE: 0.3195220158804941\n",
      "LogLoss: 0.3642523087790856\n",
      "Mean Per-Class Error: 0.13037851120479604\n",
      "AUC: 0.9297409249020261\n",
      "pr_auc: 0.8481588205514409\n",
      "Gini: 0.8594818498040522\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4090271183738002: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10802.0</td>\n",
       "<td>1943.0</td>\n",
       "<td>0.1525</td>\n",
       "<td> (1943.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1345.0</td>\n",
       "<td>10945.0</td>\n",
       "<td>0.1094</td>\n",
       "<td> (1345.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12147.0</td>\n",
       "<td>12888.0</td>\n",
       "<td>0.1313</td>\n",
       "<td> (3288.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10802  1943   0.1525   (1943.0/12745.0)\n",
       "1      1345   10945  0.1094   (1345.0/12290.0)\n",
       "Total  12147  12888  0.1313   (3288.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4090271</td>\n",
       "<td>0.8694098</td>\n",
       "<td>227.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1730289</td>\n",
       "<td>0.8972282</td>\n",
       "<td>303.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6652332</td>\n",
       "<td>0.8760825</td>\n",
       "<td>153.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4484035</td>\n",
       "<td>0.8694228</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998186</td>\n",
       "<td>0.9859002</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000653</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998186</td>\n",
       "<td>0.9989800</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4484035</td>\n",
       "<td>0.7391471</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5017740</td>\n",
       "<td>0.8681855</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4484035</td>\n",
       "<td>0.8696215</td>\n",
       "<td>215.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.409027     0.86941   227\n",
       "max f2                       0.173029     0.897228  303\n",
       "max f0point5                 0.665233     0.876082  153\n",
       "max accuracy                 0.448403     0.869423  215\n",
       "max precision                0.999819     0.9859    0\n",
       "max recall                   6.5308e-05   1         399\n",
       "max specificity              0.999819     0.99898   0\n",
       "max absolute_mcc             0.448403     0.739147  215\n",
       "max min_per_class_accuracy   0.501774     0.868186  200\n",
       "max mean_per_class_accuracy  0.448403     0.869621  215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.09 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9999544</td>\n",
       "<td>2.0207907</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.9920319</td>\n",
       "<td>0.9999807</td>\n",
       "<td>0.9920319</td>\n",
       "<td>0.9999807</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0202604</td>\n",
       "<td>102.0790718</td>\n",
       "<td>102.0790718</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9998547</td>\n",
       "<td>2.0207258</td>\n",
       "<td>2.0207583</td>\n",
       "<td>0.992</td>\n",
       "<td>0.9999100</td>\n",
       "<td>0.9920160</td>\n",
       "<td>0.9999454</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.0404394</td>\n",
       "<td>102.0725793</td>\n",
       "<td>102.0758321</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9997052</td>\n",
       "<td>2.0126751</td>\n",
       "<td>2.0180603</td>\n",
       "<td>0.9880478</td>\n",
       "<td>0.9997812</td>\n",
       "<td>0.9906915</td>\n",
       "<td>0.9998906</td>\n",
       "<td>0.0201790</td>\n",
       "<td>0.0606184</td>\n",
       "<td>101.2675093</td>\n",
       "<td>101.8060328</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9994458</td>\n",
       "<td>1.9799854</td>\n",
       "<td>2.0085606</td>\n",
       "<td>0.972</td>\n",
       "<td>0.9995754</td>\n",
       "<td>0.9860279</td>\n",
       "<td>0.9998120</td>\n",
       "<td>0.0197722</td>\n",
       "<td>0.0803906</td>\n",
       "<td>97.9985354</td>\n",
       "<td>100.8560584</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9991322</td>\n",
       "<td>1.9718373</td>\n",
       "<td>2.0012277</td>\n",
       "<td>0.968</td>\n",
       "<td>0.9992934</td>\n",
       "<td>0.9824281</td>\n",
       "<td>0.9997084</td>\n",
       "<td>0.0196908</td>\n",
       "<td>0.1000814</td>\n",
       "<td>97.1837266</td>\n",
       "<td>100.1227653</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9957849</td>\n",
       "<td>1.9605523</td>\n",
       "<td>1.9808900</td>\n",
       "<td>0.9624601</td>\n",
       "<td>0.9977265</td>\n",
       "<td>0.9724441</td>\n",
       "<td>0.9987174</td>\n",
       "<td>0.0980472</td>\n",
       "<td>0.1981286</td>\n",
       "<td>96.0552295</td>\n",
       "<td>98.0889974</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9886759</td>\n",
       "<td>1.9198769</td>\n",
       "<td>1.9605523</td>\n",
       "<td>0.9424920</td>\n",
       "<td>0.9926887</td>\n",
       "<td>0.9624601</td>\n",
       "<td>0.9967079</td>\n",
       "<td>0.0960130</td>\n",
       "<td>0.2941416</td>\n",
       "<td>91.9876936</td>\n",
       "<td>96.0552295</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9750084</td>\n",
       "<td>1.9018718</td>\n",
       "<td>1.9458910</td>\n",
       "<td>0.9336531</td>\n",
       "<td>0.9823442</td>\n",
       "<td>0.9552626</td>\n",
       "<td>0.9931191</td>\n",
       "<td>0.0950366</td>\n",
       "<td>0.3891782</td>\n",
       "<td>90.1871830</td>\n",
       "<td>94.5890968</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.9218192</td>\n",
       "<td>1.8328317</td>\n",
       "<td>1.9081995</td>\n",
       "<td>0.8997604</td>\n",
       "<td>0.9523476</td>\n",
       "<td>0.9367594</td>\n",
       "<td>0.9795268</td>\n",
       "<td>0.1833198</td>\n",
       "<td>0.5724980</td>\n",
       "<td>83.2831668</td>\n",
       "<td>90.8199517</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.8009691</td>\n",
       "<td>1.7074199</td>\n",
       "<td>1.8580146</td>\n",
       "<td>0.8381942</td>\n",
       "<td>0.8709061</td>\n",
       "<td>0.9121230</td>\n",
       "<td>0.9523770</td>\n",
       "<td>0.1707079</td>\n",
       "<td>0.7432059</td>\n",
       "<td>70.7419933</td>\n",
       "<td>85.8014646</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4660690</td>\n",
       "<td>1.3284572</td>\n",
       "<td>1.7520862</td>\n",
       "<td>0.6521565</td>\n",
       "<td>0.6568816</td>\n",
       "<td>0.8601214</td>\n",
       "<td>0.8932685</td>\n",
       "<td>0.1328723</td>\n",
       "<td>0.8760781</td>\n",
       "<td>32.8457219</td>\n",
       "<td>75.2086239</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1718128</td>\n",
       "<td>0.6095603</td>\n",
       "<td>1.5617033</td>\n",
       "<td>0.2992409</td>\n",
       "<td>0.2963422</td>\n",
       "<td>0.7666600</td>\n",
       "<td>0.7938006</td>\n",
       "<td>0.0609439</td>\n",
       "<td>0.9370220</td>\n",
       "<td>-39.0439690</td>\n",
       "<td>56.1703282</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.0609067</td>\n",
       "<td>0.3271605</td>\n",
       "<td>1.3853702</td>\n",
       "<td>0.1606073</td>\n",
       "<td>0.1084650</td>\n",
       "<td>0.6800959</td>\n",
       "<td>0.6959123</td>\n",
       "<td>0.0327095</td>\n",
       "<td>0.9697315</td>\n",
       "<td>-67.2839460</td>\n",
       "<td>38.5370225</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0129803</td>\n",
       "<td>0.1822256</td>\n",
       "<td>1.2349471</td>\n",
       "<td>0.0894569</td>\n",
       "<td>0.0332473</td>\n",
       "<td>0.6062512</td>\n",
       "<td>0.6130626</td>\n",
       "<td>0.0182262</td>\n",
       "<td>0.9879577</td>\n",
       "<td>-81.7774393</td>\n",
       "<td>23.4947111</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0006096</td>\n",
       "<td>0.1057982</td>\n",
       "<td>1.1095084</td>\n",
       "<td>0.0519377</td>\n",
       "<td>0.0047485</td>\n",
       "<td>0.5446718</td>\n",
       "<td>0.5454842</td>\n",
       "<td>0.0105777</td>\n",
       "<td>0.9985354</td>\n",
       "<td>-89.4201815</td>\n",
       "<td>10.9508393</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0146431</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0071885</td>\n",
       "<td>0.0001139</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4909363</td>\n",
       "<td>0.0014646</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.5356871</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.999954           2.02079    2.02079            0.992032         0.999981     0.992032                    0.999981            0.0202604       0.0202604                  102.079   102.079\n",
       "    2        0.020012                    0.999855           2.02073    2.02076            0.992            0.99991      0.992016                    0.999945            0.020179        0.0404394                  102.073   102.076\n",
       "    3        0.0300379                   0.999705           2.01268    2.01806            0.988048         0.999781     0.990691                    0.999891            0.020179        0.0606184                  101.268   101.806\n",
       "    4        0.040024                    0.999446           1.97999    2.00856            0.972            0.999575     0.986028                    0.999812            0.0197722       0.0803906                  97.9985   100.856\n",
       "    5        0.05001                     0.999132           1.97184    2.00123            0.968            0.999293     0.982428                    0.999708            0.0196908       0.100081                   97.1837   100.123\n",
       "    6        0.10002                     0.995785           1.96055    1.98089            0.96246          0.997726     0.972444                    0.998717            0.0980472       0.198129                   96.0552   98.089\n",
       "    7        0.15003                     0.988676           1.91988    1.96055            0.942492         0.992689     0.96246                     0.996708            0.096013        0.294142                   91.9877   96.0552\n",
       "    8        0.2                         0.975008           1.90187    1.94589            0.933653         0.982344     0.955263                    0.993119            0.0950366       0.389178                   90.1872   94.5891\n",
       "    9        0.30002                     0.921819           1.83283    1.9082             0.89976          0.952348     0.936759                    0.979527            0.18332         0.572498                   83.2832   90.82\n",
       "    10       0.4                         0.800969           1.70742    1.85801            0.838194         0.870906     0.912123                    0.952377            0.170708        0.743206                   70.742    85.8015\n",
       "    11       0.50002                     0.466069           1.32846    1.75209            0.652157         0.656882     0.860121                    0.893269            0.132872        0.876078                   32.8457   75.2086\n",
       "    12       0.6                         0.171813           0.60956    1.5617             0.299241         0.296342     0.76666                     0.793801            0.0609439       0.937022                   -39.044   56.1703\n",
       "    13       0.69998                     0.0609067          0.327161   1.38537            0.160607         0.108465     0.680096                    0.695912            0.0327095       0.969731                   -67.2839  38.537\n",
       "    14       0.8                         0.0129803          0.182226   1.23495            0.0894569        0.0332473    0.606251                    0.613063            0.0182262       0.987958                   -81.7774  23.4947\n",
       "    15       0.89998                     0.000609557        0.105798   1.10951            0.0519377        0.0047485    0.544672                    0.545484            0.0105777       0.998535                   -89.4202  10.9508\n",
       "    16       1                           1.3008e-12         0.0146431  1                  0.0071885        0.000113905  0.490913                    0.490936            0.00146461      1                          -98.5357  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:15</td>\n",
       "<td> 0.047 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:18</td>\n",
       "<td> 3.130 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3256289</td>\n",
       "<td>0.3346993</td>\n",
       "<td>0.9335348</td>\n",
       "<td>0.9228871</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1547992</td>\n",
       "<td>0.3569567</td>\n",
       "<td>0.3940706</td>\n",
       "<td>0.9014812</td>\n",
       "<td>0.8851287</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1938087</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:21</td>\n",
       "<td> 5.341 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.2833320</td>\n",
       "<td>0.2656982</td>\n",
       "<td>0.9640647</td>\n",
       "<td>0.9462064</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1078253</td>\n",
       "<td>0.3466695</td>\n",
       "<td>0.3790711</td>\n",
       "<td>0.9110234</td>\n",
       "<td>0.8867677</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1729179</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:23</td>\n",
       "<td> 7.548 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2489192</td>\n",
       "<td>0.2167026</td>\n",
       "<td>0.9808630</td>\n",
       "<td>0.9557229</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0746790</td>\n",
       "<td>0.3385384</td>\n",
       "<td>0.3699589</td>\n",
       "<td>0.9173167</td>\n",
       "<td>0.8881519</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1584582</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:25</td>\n",
       "<td> 9.834 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.2168107</td>\n",
       "<td>0.1758038</td>\n",
       "<td>0.9907615</td>\n",
       "<td>0.9555474</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0489307</td>\n",
       "<td>0.3323691</td>\n",
       "<td>0.3647155</td>\n",
       "<td>0.9215394</td>\n",
       "<td>0.8724303</td>\n",
       "<td>2.0126751</td>\n",
       "<td>0.1484721</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:28</td>\n",
       "<td>12.226 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.1821055</td>\n",
       "<td>0.1373682</td>\n",
       "<td>0.9968057</td>\n",
       "<td>0.9404879</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0276452</td>\n",
       "<td>0.3260935</td>\n",
       "<td>0.3605634</td>\n",
       "<td>0.9254958</td>\n",
       "<td>0.8756659</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1420012</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:30</td>\n",
       "<td>14.679 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1570659</td>\n",
       "<td>0.1121921</td>\n",
       "<td>0.9987497</td>\n",
       "<td>0.9450531</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0161638</td>\n",
       "<td>0.3223682</td>\n",
       "<td>0.3605683</td>\n",
       "<td>0.9276427</td>\n",
       "<td>0.8614938</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1350509</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:32</td>\n",
       "<td>17.150 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.1360542</td>\n",
       "<td>0.0928464</td>\n",
       "<td>0.9995221</td>\n",
       "<td>0.9319399</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0096843</td>\n",
       "<td>0.3205056</td>\n",
       "<td>0.3628241</td>\n",
       "<td>0.9289732</td>\n",
       "<td>0.8378261</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1312562</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:34</td>\n",
       "<td>18.765 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.1254431</td>\n",
       "<td>0.0836913</td>\n",
       "<td>0.9997560</td>\n",
       "<td>0.9308990</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0070086</td>\n",
       "<td>0.3195220</td>\n",
       "<td>0.3642523</td>\n",
       "<td>0.9297409</td>\n",
       "<td>0.8481588</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1313361</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:39:15  0.047 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:39:18  3.130 sec   20                 0.325629         0.334699            0.933535        0.922887           2.03499          0.154799                         0.356957           0.394071              0.901481          0.885129             2.03702            0.193809\n",
       "    2020-07-16 22:39:21  5.341 sec   40                 0.283332         0.265698            0.964065        0.946206           2.03499          0.107825                         0.346669           0.379071              0.911023          0.886768             2.02891            0.172918\n",
       "    2020-07-16 22:39:23  7.548 sec   60                 0.248919         0.216703            0.980863        0.955723           2.03499          0.074679                         0.338538           0.369959              0.917317          0.888152             2.02891            0.158458\n",
       "    2020-07-16 22:39:25  9.834 sec   80                 0.216811         0.175804            0.990761        0.955547           2.03499          0.0489307                        0.332369           0.364716              0.921539          0.87243              2.01268            0.148472\n",
       "    2020-07-16 22:39:28  12.226 sec  100                0.182106         0.137368            0.996806        0.940488           2.03499          0.0276452                        0.326094           0.360563              0.925496          0.875666             2.02079            0.142001\n",
       "    2020-07-16 22:39:30  14.679 sec  120                0.157066         0.112192            0.99875         0.945053           2.03499          0.0161638                        0.322368           0.360568              0.927643          0.861494             2.02079            0.135051\n",
       "    2020-07-16 22:39:32  17.150 sec  140                0.136054         0.0928464           0.999522        0.93194            2.03499          0.00968431                       0.320506           0.362824              0.928973          0.837826             2.02079            0.131256\n",
       "    2020-07-16 22:39:34  18.765 sec  150                0.125443         0.0836913           0.999756        0.930899           2.03499          0.00700865                       0.319522           0.364252              0.929741          0.848159             2.02079            0.131336"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>46045.375</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3325550</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>7614.0092773</td>\n",
       "<td>0.1653588</td>\n",
       "<td>0.0549909</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>4936.6943359</td>\n",
       "<td>0.1072137</td>\n",
       "<td>0.0356544</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>4222.2587891</td>\n",
       "<td>0.0916978</td>\n",
       "<td>0.0304946</td></tr>\n",
       "<tr><td>e02</td>\n",
       "<td>3685.8862305</td>\n",
       "<td>0.0800490</td>\n",
       "<td>0.0266207</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>e20.87D4E</td>\n",
       "<td>1.4336857</td>\n",
       "<td>0.0000311</td>\n",
       "<td>0.0000104</td></tr>\n",
       "<tr><td>f09.E</td>\n",
       "<td>1.3636196</td>\n",
       "<td>0.0000296</td>\n",
       "<td>0.0000098</td></tr>\n",
       "<tr><td>f09.K</td>\n",
       "<td>1.1274998</td>\n",
       "<td>0.0000245</td>\n",
       "<td>0.0000081</td></tr>\n",
       "<tr><td>e03.L</td>\n",
       "<td>0.9577506</td>\n",
       "<td>0.0000208</td>\n",
       "<td>0.0000069</td></tr>\n",
       "<tr><td>e24.E</td>\n",
       "<td>0.8786974</td>\n",
       "<td>0.0000191</td>\n",
       "<td>0.0000063</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         46045.375              1.0                     0.33255496202803647\n",
       "e19         7614.00927734375       0.16535882870633045     0.05499089900143431\n",
       "e12         4936.6943359375        0.10721368510816776     0.03565444298003259\n",
       "f02         4222.2587890625        0.09169778265596708     0.030494552629210272\n",
       "e02         3685.88623046875       0.08004900015406        0.026620692206615705\n",
       "---         ---                    ---                     ---\n",
       "e20.87D4E   1.4336856603622437     3.113636625529152e-05   1.0354553097719507e-05\n",
       "f09.E       1.3636195659637451     2.9614691290140326e-05  9.848512537464638e-06\n",
       "f09.K       1.1274998188018799     2.448671161439949e-05   8.143177451118102e-06\n",
       "e03.L       0.9577505588531494     2.0800146786797792e-05  6.917192024861125e-06\n",
       "e24.E       0.878697395324707      1.908329328026337e-05   6.34624387218787e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1126\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.009873713953249279\n",
      "RMSE: 0.09936656355761368\n",
      "LogLoss: 0.06163608532888788\n",
      "Mean Per-Class Error: 0.0031720181300750605\n",
      "AUC: 0.9999542404121387\n",
      "pr_auc: 0.9045446838202031\n",
      "Gini: 0.9999084808242773\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5154163369110653: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>50804.0</td>\n",
       "<td>138.0</td>\n",
       "<td>0.0027</td>\n",
       "<td> (138.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>179.0</td>\n",
       "<td>49041.0</td>\n",
       "<td>0.0036</td>\n",
       "<td> (179.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50983.0</td>\n",
       "<td>49179.0</td>\n",
       "<td>0.0032</td>\n",
       "<td> (317.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      50804  138    0.0027   (138.0/50942.0)\n",
       "1      179    49041  0.0036   (179.0/49220.0)\n",
       "Total  50983  49179  0.0032   (317.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5154163</td>\n",
       "<td>0.9967784</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4215763</td>\n",
       "<td>0.9975286</td>\n",
       "<td>212.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5806359</td>\n",
       "<td>0.9976435</td>\n",
       "<td>185.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5208809</td>\n",
       "<td>0.9968351</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998811</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1672438</td>\n",
       "<td>1.0</td>\n",
       "<td>282.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998811</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5208809</td>\n",
       "<td>0.9936689</td>\n",
       "<td>196.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5054268</td>\n",
       "<td>0.9967696</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4996165</td>\n",
       "<td>0.9968280</td>\n",
       "<td>200.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.515416     0.996778  197\n",
       "max f2                       0.421576     0.997529  212\n",
       "max f0point5                 0.580636     0.997643  185\n",
       "max accuracy                 0.520881     0.996835  196\n",
       "max precision                0.999881     1         0\n",
       "max recall                   0.167244     1         282\n",
       "max specificity              0.999881     1         0\n",
       "max absolute_mcc             0.520881     0.993669  196\n",
       "max min_per_class_accuracy   0.505427     0.99677   199\n",
       "max mean_per_class_accuracy  0.499616     0.996828  200"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100138</td>\n",
       "<td>0.9999850</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999941</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999941</td>\n",
       "<td>0.0203779</td>\n",
       "<td>0.0203779</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200176</td>\n",
       "<td>0.9999478</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999683</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999812</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.0407355</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300114</td>\n",
       "<td>0.9998816</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999171</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999598</td>\n",
       "<td>0.0203373</td>\n",
       "<td>0.0610727</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400451</td>\n",
       "<td>0.9997795</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998345</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999284</td>\n",
       "<td>0.0204185</td>\n",
       "<td>0.0814913</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500090</td>\n",
       "<td>0.9996465</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997165</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998862</td>\n",
       "<td>0.0202763</td>\n",
       "<td>0.1017676</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9981073</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9990072</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994468</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9944925</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9965316</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9984751</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9875059</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9913052</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9966827</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9594351</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9751136</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9894932</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.6105039</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.8978857</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9329537</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9753587</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.8139984</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.3290036</td>\n",
       "<td>1.8572090</td>\n",
       "<td>1.9994311</td>\n",
       "<td>0.9126398</td>\n",
       "<td>0.7724329</td>\n",
       "<td>0.9825283</td>\n",
       "<td>0.9347744</td>\n",
       "<td>0.1857172</td>\n",
       "<td>0.9997156</td>\n",
       "<td>85.7208965</td>\n",
       "<td>99.9431126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.0888844</td>\n",
       "<td>0.0028444</td>\n",
       "<td>1.6666722</td>\n",
       "<td>0.0013978</td>\n",
       "<td>0.1615648</td>\n",
       "<td>0.8190093</td>\n",
       "<td>0.8059082</td>\n",
       "<td>0.0002844</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.7155571</td>\n",
       "<td>66.6672213</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0330923</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4285796</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0575660</td>\n",
       "<td>0.7020096</td>\n",
       "<td>0.6990037</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8579579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0070019</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0179434</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6138723</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0003137</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0025993</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459538</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000542</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4913595</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100138                   0.999985           2.03499     2.03499            1                0.999994     1                           0.999994            0.0203779       0.0203779                  103.499   103.499\n",
       "    2        0.0200176                   0.999948           2.03499     2.03499            1                0.999968     1                           0.999981            0.0203576       0.0407355                  103.499   103.499\n",
       "    3        0.0300114                   0.999882           2.03499     2.03499            1                0.999917     1                           0.99996             0.0203373       0.0610727                  103.499   103.499\n",
       "    4        0.0400451                   0.99978            2.03499     2.03499            1                0.999835     1                           0.999928            0.0204185       0.0814913                  103.499   103.499\n",
       "    5        0.050009                    0.999647           2.03499     2.03499            1                0.999716     1                           0.999886            0.0202763       0.101768                   103.499   103.499\n",
       "    6        0.100008                    0.998107           2.03499     2.03499            1                0.999007     1                           0.999447            0.101747        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.994492           2.03499     2.03499            1                0.996532     1                           0.998475            0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.987506           2.03499     2.03499            1                0.991305     1                           0.996683            0.101747        0.407009                   103.499   103.499\n",
       "    9        0.300004                    0.959435           2.03499     2.03499            1                0.975114     1                           0.989493            0.203495        0.610504                   103.499   103.499\n",
       "    10       0.400002                    0.897886           2.03499     2.03499            1                0.932954     1                           0.975359            0.203495        0.813998                   103.499   103.499\n",
       "    11       0.5                         0.329004           1.85721     1.99943            0.91264          0.772433     0.982528                    0.934774            0.185717        0.999716                   85.7209   99.9431\n",
       "    12       0.599998                    0.0888844          0.00284443  1.66667            0.00139776       0.161565     0.819009                    0.805908            0.000284437     1                          -99.7156  66.6672\n",
       "    13       0.699996                    0.0330923          0           1.42858            0                0.057566     0.70201                     0.699004            0               1                          -100      42.858\n",
       "    14       0.799994                    0.00700188         0           1.25001            0                0.0179434    0.61426                     0.613872            0               1                          -100      25.0009\n",
       "    15       0.899992                    0.000313669        0           1.11112            0                0.00259933   0.546009                    0.545954            0               1                          -100      11.1121\n",
       "    16       1                           4.16816e-16        0           1                  0                5.42051e-05  0.491404                    0.491359            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10287137060518117\n",
      "RMSE: 0.3207356709272936\n",
      "LogLoss: 0.3873868258715965\n",
      "Mean Per-Class Error: 0.12850122944239206\n",
      "AUC: 0.9283808995438789\n",
      "pr_auc: 0.8364774974136862\n",
      "Gini: 0.8567617990877578\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3879125043749809: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10816.0</td>\n",
       "<td>1929.0</td>\n",
       "<td>0.1514</td>\n",
       "<td> (1929.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1317.0</td>\n",
       "<td>10973.0</td>\n",
       "<td>0.1072</td>\n",
       "<td> (1317.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12133.0</td>\n",
       "<td>12902.0</td>\n",
       "<td>0.1297</td>\n",
       "<td> (3246.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10816  1929   0.1514   (1929.0/12745.0)\n",
       "1      1317   10973  0.1072   (1317.0/12290.0)\n",
       "Total  12133  12902  0.1297   (3246.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3879125</td>\n",
       "<td>0.8711496</td>\n",
       "<td>235.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1677355</td>\n",
       "<td>0.8995478</td>\n",
       "<td>303.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7785173</td>\n",
       "<td>0.8757374</td>\n",
       "<td>120.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4556416</td>\n",
       "<td>0.8713401</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998998</td>\n",
       "<td>0.9829060</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000583</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998998</td>\n",
       "<td>0.9985877</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4460008</td>\n",
       "<td>0.7428814</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5031817</td>\n",
       "<td>0.8698313</td>\n",
       "<td>201.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4556416</td>\n",
       "<td>0.8714988</td>\n",
       "<td>215.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.387913     0.87115   235\n",
       "max f2                       0.167736     0.899548  303\n",
       "max f0point5                 0.778517     0.875737  120\n",
       "max accuracy                 0.455642     0.87134   215\n",
       "max precision                0.9999       0.982906  0\n",
       "max recall                   5.82658e-05  1         399\n",
       "max specificity              0.9999       0.998588  0\n",
       "max absolute_mcc             0.446001     0.742881  218\n",
       "max min_per_class_accuracy   0.503182     0.869831  201\n",
       "max mean_per_class_accuracy  0.455642     0.871499  215"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.18 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100260</td>\n",
       "<td>0.9999843</td>\n",
       "<td>2.0289063</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9999939</td>\n",
       "<td>0.9960159</td>\n",
       "<td>0.9999939</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0203417</td>\n",
       "<td>102.8906344</td>\n",
       "<td>102.8906344</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200120</td>\n",
       "<td>0.9999391</td>\n",
       "<td>2.0370220</td>\n",
       "<td>2.0329561</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999663</td>\n",
       "<td>0.9980040</td>\n",
       "<td>0.9999801</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0406835</td>\n",
       "<td>103.7021969</td>\n",
       "<td>103.2956057</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300779</td>\n",
       "<td>0.9998714</td>\n",
       "<td>1.9804380</td>\n",
       "<td>2.0153803</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.9999060</td>\n",
       "<td>0.9893758</td>\n",
       "<td>0.9999553</td>\n",
       "<td>0.0199349</td>\n",
       "<td>0.0606184</td>\n",
       "<td>98.0438025</td>\n",
       "<td>101.5380301</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400240</td>\n",
       "<td>0.9997571</td>\n",
       "<td>1.9715755</td>\n",
       "<td>2.0044947</td>\n",
       "<td>0.9678715</td>\n",
       "<td>0.9998163</td>\n",
       "<td>0.9840319</td>\n",
       "<td>0.9999208</td>\n",
       "<td>0.0196094</td>\n",
       "<td>0.0802278</td>\n",
       "<td>97.1575480</td>\n",
       "<td>100.4494672</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9996009</td>\n",
       "<td>2.0044296</td>\n",
       "<td>2.0044817</td>\n",
       "<td>0.984</td>\n",
       "<td>0.9996863</td>\n",
       "<td>0.9840256</td>\n",
       "<td>0.9998740</td>\n",
       "<td>0.0200163</td>\n",
       "<td>0.1002441</td>\n",
       "<td>100.4429618</td>\n",
       "<td>100.4481682</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9979743</td>\n",
       "<td>1.9670604</td>\n",
       "<td>1.9857710</td>\n",
       "<td>0.9656550</td>\n",
       "<td>0.9989381</td>\n",
       "<td>0.9748403</td>\n",
       "<td>0.9994060</td>\n",
       "<td>0.0983727</td>\n",
       "<td>0.1986168</td>\n",
       "<td>96.7060352</td>\n",
       "<td>98.5771017</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9933430</td>\n",
       "<td>1.9117419</td>\n",
       "<td>1.9610946</td>\n",
       "<td>0.9384984</td>\n",
       "<td>0.9960245</td>\n",
       "<td>0.9627263</td>\n",
       "<td>0.9982788</td>\n",
       "<td>0.0956062</td>\n",
       "<td>0.2942229</td>\n",
       "<td>91.1741864</td>\n",
       "<td>96.1094633</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9844735</td>\n",
       "<td>1.8774471</td>\n",
       "<td>1.9401953</td>\n",
       "<td>0.9216627</td>\n",
       "<td>0.9894355</td>\n",
       "<td>0.9524665</td>\n",
       "<td>0.9960693</td>\n",
       "<td>0.0938161</td>\n",
       "<td>0.3880391</td>\n",
       "<td>87.7447107</td>\n",
       "<td>94.0195281</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.9441502</td>\n",
       "<td>1.8133075</td>\n",
       "<td>1.8978937</td>\n",
       "<td>0.8901757</td>\n",
       "<td>0.9676611</td>\n",
       "<td>0.9317002</td>\n",
       "<td>0.9865987</td>\n",
       "<td>0.1813670</td>\n",
       "<td>0.5694060</td>\n",
       "<td>81.3307496</td>\n",
       "<td>89.7893721</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.8375315</td>\n",
       "<td>1.7285796</td>\n",
       "<td>1.8555736</td>\n",
       "<td>0.8485817</td>\n",
       "<td>0.9011264</td>\n",
       "<td>0.9109247</td>\n",
       "<td>0.9652349</td>\n",
       "<td>0.1728234</td>\n",
       "<td>0.7422295</td>\n",
       "<td>72.8579569</td>\n",
       "<td>85.5573637</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4657180</td>\n",
       "<td>1.3585570</td>\n",
       "<td>1.7561544</td>\n",
       "<td>0.6669329</td>\n",
       "<td>0.6886388</td>\n",
       "<td>0.8621185</td>\n",
       "<td>0.9099068</td>\n",
       "<td>0.1358828</td>\n",
       "<td>0.8781123</td>\n",
       "<td>35.8556984</td>\n",
       "<td>75.6154425</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1334992</td>\n",
       "<td>0.6014220</td>\n",
       "<td>1.5637375</td>\n",
       "<td>0.2952457</td>\n",
       "<td>0.2628712</td>\n",
       "<td>0.7676586</td>\n",
       "<td>0.8020891</td>\n",
       "<td>0.0601302</td>\n",
       "<td>0.9382425</td>\n",
       "<td>-39.8578012</td>\n",
       "<td>56.3737456</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.0422017</td>\n",
       "<td>0.2791444</td>\n",
       "<td>1.3802556</td>\n",
       "<td>0.1370356</td>\n",
       "<td>0.0807091</td>\n",
       "<td>0.6775850</td>\n",
       "<td>0.6990524</td>\n",
       "<td>0.0279089</td>\n",
       "<td>0.9661513</td>\n",
       "<td>-72.0855559</td>\n",
       "<td>38.0255584</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0079802</td>\n",
       "<td>0.1952417</td>\n",
       "<td>1.2320993</td>\n",
       "<td>0.0958466</td>\n",
       "<td>0.0219015</td>\n",
       "<td>0.6048532</td>\n",
       "<td>0.6143917</td>\n",
       "<td>0.0195281</td>\n",
       "<td>0.9856794</td>\n",
       "<td>-80.4758278</td>\n",
       "<td>23.2099268</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0003191</td>\n",
       "<td>0.1253302</td>\n",
       "<td>1.1091468</td>\n",
       "<td>0.0615262</td>\n",
       "<td>0.0027562</td>\n",
       "<td>0.5444943</td>\n",
       "<td>0.5464442</td>\n",
       "<td>0.0125305</td>\n",
       "<td>0.9982099</td>\n",
       "<td>-87.4669843</td>\n",
       "<td>10.9146754</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0178972</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0087859</td>\n",
       "<td>0.0000549</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4917944</td>\n",
       "<td>0.0017901</td>\n",
       "<td>1.0</td>\n",
       "<td>-98.2102842</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010026                    0.999984           2.02891    2.02891            0.996016         0.999994     0.996016                    0.999994            0.0203417       0.0203417                  102.891   102.891\n",
       "    2        0.020012                    0.999939           2.03702    2.03296            1                0.999966     0.998004                    0.99998             0.0203417       0.0406835                  103.702   103.296\n",
       "    3        0.0300779                   0.999871           1.98044    2.01538            0.972222         0.999906     0.989376                    0.999955            0.0199349       0.0606184                  98.0438   101.538\n",
       "    4        0.040024                    0.999757           1.97158    2.00449            0.967871         0.999816     0.984032                    0.999921            0.0196094       0.0802278                  97.1575   100.449\n",
       "    5        0.05001                     0.999601           2.00443    2.00448            0.984            0.999686     0.984026                    0.999874            0.0200163       0.100244                   100.443   100.448\n",
       "    6        0.10002                     0.997974           1.96706    1.98577            0.965655         0.998938     0.97484                     0.999406            0.0983727       0.198617                   96.706    98.5771\n",
       "    7        0.15003                     0.993343           1.91174    1.96109            0.938498         0.996024     0.962726                    0.998279            0.0956062       0.294223                   91.1742   96.1095\n",
       "    8        0.2                         0.984474           1.87745    1.9402             0.921663         0.989436     0.952467                    0.996069            0.0938161       0.388039                   87.7447   94.0195\n",
       "    9        0.30002                     0.94415            1.81331    1.89789            0.890176         0.967661     0.9317                      0.986599            0.181367        0.569406                   81.3307   89.7894\n",
       "    10       0.4                         0.837531           1.72858    1.85557            0.848582         0.901126     0.910925                    0.965235            0.172823        0.742229                   72.858    85.5574\n",
       "    11       0.50002                     0.465718           1.35856    1.75615            0.666933         0.688639     0.862119                    0.909907            0.135883        0.878112                   35.8557   75.6154\n",
       "    12       0.6                         0.133499           0.601422   1.56374            0.295246         0.262871     0.767659                    0.802089            0.0601302       0.938242                   -39.8578  56.3737\n",
       "    13       0.69998                     0.0422017          0.279144   1.38026            0.137036         0.0807091    0.677585                    0.699052            0.0279089       0.966151                   -72.0856  38.0256\n",
       "    14       0.8                         0.0079802          0.195242   1.2321             0.0958466        0.0219015    0.604853                    0.614392            0.0195281       0.985679                   -80.4758  23.2099\n",
       "    15       0.89998                     0.000319065        0.12533    1.10915            0.0615262        0.00275618   0.544494                    0.546444            0.0125305       0.99821                    -87.467   10.9147\n",
       "    16       1                           4.16816e-16        0.0178972  1                  0.00878594       5.49383e-05  0.490913                    0.491794            0.00179007      1                          -98.2103  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:35</td>\n",
       "<td> 0.051 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:38</td>\n",
       "<td> 3.160 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3202572</td>\n",
       "<td>0.3240109</td>\n",
       "<td>0.9374790</td>\n",
       "<td>0.9254157</td>\n",
       "<td>2.0329549</td>\n",
       "<td>0.1486791</td>\n",
       "<td>0.3575269</td>\n",
       "<td>0.3955642</td>\n",
       "<td>0.9008242</td>\n",
       "<td>0.8877695</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1909726</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:40</td>\n",
       "<td> 5.395 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.2717040</td>\n",
       "<td>0.2465280</td>\n",
       "<td>0.9699901</td>\n",
       "<td>0.9467486</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0968631</td>\n",
       "<td>0.3457163</td>\n",
       "<td>0.3820866</td>\n",
       "<td>0.9113601</td>\n",
       "<td>0.8850457</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1682445</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:43</td>\n",
       "<td> 7.683 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2264206</td>\n",
       "<td>0.1854653</td>\n",
       "<td>0.9878319</td>\n",
       "<td>0.9510044</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0579761</td>\n",
       "<td>0.3373000</td>\n",
       "<td>0.3749336</td>\n",
       "<td>0.9180758</td>\n",
       "<td>0.8745409</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1551428</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:45</td>\n",
       "<td> 9.990 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.1926806</td>\n",
       "<td>0.1464053</td>\n",
       "<td>0.9950590</td>\n",
       "<td>0.9358193</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0347437</td>\n",
       "<td>0.3306586</td>\n",
       "<td>0.3732471</td>\n",
       "<td>0.9221359</td>\n",
       "<td>0.8579083</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1452766</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:47</td>\n",
       "<td>12.313 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.1611863</td>\n",
       "<td>0.1141720</td>\n",
       "<td>0.9983376</td>\n",
       "<td>0.9060802</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0188594</td>\n",
       "<td>0.3258584</td>\n",
       "<td>0.3733470</td>\n",
       "<td>0.9250332</td>\n",
       "<td>0.8404337</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1396844</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:50</td>\n",
       "<td>14.784 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1311874</td>\n",
       "<td>0.0872148</td>\n",
       "<td>0.9995853</td>\n",
       "<td>0.9296075</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0088756</td>\n",
       "<td>0.3231042</td>\n",
       "<td>0.3790989</td>\n",
       "<td>0.9262752</td>\n",
       "<td>0.8317803</td>\n",
       "<td>2.0289385</td>\n",
       "<td>0.1332135</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:52</td>\n",
       "<td>17.306 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.1096221</td>\n",
       "<td>0.0694108</td>\n",
       "<td>0.9998850</td>\n",
       "<td>0.9120319</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0045227</td>\n",
       "<td>0.3213875</td>\n",
       "<td>0.3844641</td>\n",
       "<td>0.9278113</td>\n",
       "<td>0.8439206</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1300180</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:54</td>\n",
       "<td>18.918 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.0993666</td>\n",
       "<td>0.0616361</td>\n",
       "<td>0.9999542</td>\n",
       "<td>0.9045447</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0031649</td>\n",
       "<td>0.3207357</td>\n",
       "<td>0.3873868</td>\n",
       "<td>0.9283809</td>\n",
       "<td>0.8364775</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1296585</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:39:35  0.051 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:39:38  3.160 sec   20                 0.320257         0.324011            0.937479        0.925416           2.03295          0.148679                         0.357527           0.395564              0.900824          0.88777              2.03702            0.190973\n",
       "    2020-07-16 22:39:40  5.395 sec   40                 0.271704         0.246528            0.96999         0.946749           2.03499          0.0968631                        0.345716           0.382087              0.91136           0.885046             2.03702            0.168244\n",
       "    2020-07-16 22:39:43  7.683 sec   60                 0.226421         0.185465            0.987832        0.951004           2.03499          0.0579761                        0.3373             0.374934              0.918076          0.874541             2.03702            0.155143\n",
       "    2020-07-16 22:39:45  9.990 sec   80                 0.192681         0.146405            0.995059        0.935819           2.03499          0.0347437                        0.330659           0.373247              0.922136          0.857908             2.02891            0.145277\n",
       "    2020-07-16 22:39:47  12.313 sec  100                0.161186         0.114172            0.998338        0.90608            2.03499          0.0188594                        0.325858           0.373347              0.925033          0.840434             2.02891            0.139684\n",
       "    2020-07-16 22:39:50  14.784 sec  120                0.131187         0.0872148           0.999585        0.929608           2.03499          0.00887562                       0.323104           0.379099              0.926275          0.83178              2.02894            0.133214\n",
       "    2020-07-16 22:39:52  17.306 sec  140                0.109622         0.0694108           0.999885        0.912032           2.03499          0.00452267                       0.321387           0.384464              0.927811          0.843921             2.03702            0.130018\n",
       "    2020-07-16 22:39:54  18.918 sec  150                0.0993666        0.0616361           0.999954        0.904545           2.03499          0.00316487                       0.320736           0.387387              0.928381          0.836477             2.02891            0.129658"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>42194.5312500</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3198713</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>7351.7509766</td>\n",
       "<td>0.1742347</td>\n",
       "<td>0.0557327</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>4801.5302734</td>\n",
       "<td>0.1137951</td>\n",
       "<td>0.0363998</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>4292.5634766</td>\n",
       "<td>0.1017327</td>\n",
       "<td>0.0325414</td></tr>\n",
       "<tr><td>e02</td>\n",
       "<td>4026.3303223</td>\n",
       "<td>0.0954230</td>\n",
       "<td>0.0305231</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>b07.F</td>\n",
       "<td>1.3848097</td>\n",
       "<td>0.0000328</td>\n",
       "<td>0.0000105</td></tr>\n",
       "<tr><td>a12.E</td>\n",
       "<td>1.3642898</td>\n",
       "<td>0.0000323</td>\n",
       "<td>0.0000103</td></tr>\n",
       "<tr><td>b04.I</td>\n",
       "<td>1.3202560</td>\n",
       "<td>0.0000313</td>\n",
       "<td>0.0000100</td></tr>\n",
       "<tr><td>a18.F</td>\n",
       "<td>0.9458054</td>\n",
       "<td>0.0000224</td>\n",
       "<td>0.0000072</td></tr>\n",
       "<tr><td>a02.F</td>\n",
       "<td>0.6991501</td>\n",
       "<td>0.0000166</td>\n",
       "<td>0.0000053</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         42194.53125            1.0                     0.31987126327754195\n",
       "e19         7351.7509765625        0.17423468773722897     0.055732669673275476\n",
       "e12         4801.5302734375        0.11379508507841286     0.036399777618807284\n",
       "f02         4292.5634765625        0.10173269732822307     0.032541366411010526\n",
       "e02         4026.330322265625      0.0954230371327001      0.030523087433416574\n",
       "---         ---                    ---                     ---\n",
       "b07.F       1.3848097324371338     3.281964964208801e-05   1.0498062791341019e-05\n",
       "a12.E       1.3642897605895996     3.2333331362452326e-05  1.0342503548878991e-05\n",
       "b04.I       1.320255994796753      3.1289741956708025e-05  1.0008689287320502e-05\n",
       "a18.F       0.9458054304122925     2.2415355791215064e-05  7.170028173751528e-06\n",
       "a02.F       0.6991500854492188     1.656968485576478e-05   5.300166026924236e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1145\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.007508724782781761\n",
      "RMSE: 0.08665289829418149\n",
      "LogLoss: 0.050973294934603966\n",
      "Mean Per-Class Error: 0.002034698782056976\n",
      "AUC: 0.9999710108049515\n",
      "pr_auc: 0.8620390275175865\n",
      "Gini: 0.9999420216099031\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.516464195468209: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>50862.0</td>\n",
       "<td>80.0</td>\n",
       "<td>0.0016</td>\n",
       "<td> (80.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>123.0</td>\n",
       "<td>49097.0</td>\n",
       "<td>0.0025</td>\n",
       "<td> (123.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50985.0</td>\n",
       "<td>49177.0</td>\n",
       "<td>0.002</td>\n",
       "<td> (203.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      50862  80     0.0016   (80.0/50942.0)\n",
       "1      123    49097  0.0025   (123.0/49220.0)\n",
       "Total  50985  49177  0.002    (203.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5164642</td>\n",
       "<td>0.9979369</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4168798</td>\n",
       "<td>0.9983233</td>\n",
       "<td>212.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5619846</td>\n",
       "<td>0.9983998</td>\n",
       "<td>191.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5164642</td>\n",
       "<td>0.9979733</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9998798</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.1163632</td>\n",
       "<td>1.0</td>\n",
       "<td>307.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9998798</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5164642</td>\n",
       "<td>0.9959457</td>\n",
       "<td>197.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4993287</td>\n",
       "<td>0.9978870</td>\n",
       "<td>200.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5164642</td>\n",
       "<td>0.9979653</td>\n",
       "<td>197.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.516464     0.997937  197\n",
       "max f2                       0.41688      0.998323  212\n",
       "max f0point5                 0.561985     0.9984    191\n",
       "max accuracy                 0.516464     0.997973  197\n",
       "max precision                0.99988      1         0\n",
       "max recall                   0.116363     1         307\n",
       "max specificity              0.99988      1         0\n",
       "max absolute_mcc             0.516464     0.995946  197\n",
       "max min_per_class_accuracy   0.499329     0.997887  200\n",
       "max mean_per_class_accuracy  0.516464     0.997965  197"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.14 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100537</td>\n",
       "<td>0.9999948</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999981</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999981</td>\n",
       "<td>0.0204592</td>\n",
       "<td>0.0204592</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200675</td>\n",
       "<td>0.9999788</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999878</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999930</td>\n",
       "<td>0.0203779</td>\n",
       "<td>0.0408371</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300413</td>\n",
       "<td>0.9999474</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999644</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999835</td>\n",
       "<td>0.0202966</td>\n",
       "<td>0.0611337</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400152</td>\n",
       "<td>0.9998981</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999248</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999689</td>\n",
       "<td>0.0202966</td>\n",
       "<td>0.0814303</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500190</td>\n",
       "<td>0.9998259</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998628</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999477</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1017879</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9988898</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994568</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997023</td>\n",
       "<td>0.1017269</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500070</td>\n",
       "<td>0.9963433</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9977870</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9990639</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.3052621</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9912377</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9940941</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9978215</td>\n",
       "<td>0.1017473</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9688520</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9817026</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9924487</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.6105039</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.9151235</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9462541</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9809004</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.8139984</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.2909436</td>\n",
       "<td>1.8578185</td>\n",
       "<td>1.9995530</td>\n",
       "<td>0.9129393</td>\n",
       "<td>0.7926582</td>\n",
       "<td>0.9825882</td>\n",
       "<td>0.9432527</td>\n",
       "<td>0.1857781</td>\n",
       "<td>0.9997765</td>\n",
       "<td>85.7818486</td>\n",
       "<td>99.9553027</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.0728753</td>\n",
       "<td>0.0022349</td>\n",
       "<td>1.6666722</td>\n",
       "<td>0.0010982</td>\n",
       "<td>0.1367358</td>\n",
       "<td>0.8190093</td>\n",
       "<td>0.8088354</td>\n",
       "<td>0.0002235</td>\n",
       "<td>1.0</td>\n",
       "<td>-99.7765091</td>\n",
       "<td>66.6672213</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0250401</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4285796</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0456822</td>\n",
       "<td>0.7020096</td>\n",
       "<td>0.6998151</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8579579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0048350</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0133097</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6140030</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0001502</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0016602</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459657</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000246</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4913672</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift        cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ----------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100537                   0.999995           2.03499     2.03499            1                0.999998     1                           0.999998            0.0204592       0.0204592                  103.499   103.499\n",
       "    2        0.0200675                   0.999979           2.03499     2.03499            1                0.999988     1                           0.999993            0.0203779       0.0408371                  103.499   103.499\n",
       "    3        0.0300413                   0.999947           2.03499     2.03499            1                0.999964     1                           0.999983            0.0202966       0.0611337                  103.499   103.499\n",
       "    4        0.0400152                   0.999898           2.03499     2.03499            1                0.999925     1                           0.999969            0.0202966       0.0814303                  103.499   103.499\n",
       "    5        0.050019                    0.999826           2.03499     2.03499            1                0.999863     1                           0.999948            0.0203576       0.101788                   103.499   103.499\n",
       "    6        0.100008                    0.99889            2.03499     2.03499            1                0.999457     1                           0.999702            0.101727        0.203515                   103.499   103.499\n",
       "    7        0.150007                    0.996343           2.03499     2.03499            1                0.997787     1                           0.999064            0.101747        0.305262                   103.499   103.499\n",
       "    8        0.200006                    0.991238           2.03499     2.03499            1                0.994094     1                           0.997822            0.101747        0.407009                   103.499   103.499\n",
       "    9        0.300004                    0.968852           2.03499     2.03499            1                0.981703     1                           0.992449            0.203495        0.610504                   103.499   103.499\n",
       "    10       0.400002                    0.915124           2.03499     2.03499            1                0.946254     1                           0.9809              0.203495        0.813998                   103.499   103.499\n",
       "    11       0.5                         0.290944           1.85782     1.99955            0.912939         0.792658     0.982588                    0.943253            0.185778        0.999777                   85.7818   99.9553\n",
       "    12       0.599998                    0.0728753          0.00223491  1.66667            0.00109824       0.136736     0.819009                    0.808835            0.000223486     1                          -99.7765  66.6672\n",
       "    13       0.699996                    0.0250401          0           1.42858            0                0.0456822    0.70201                     0.699815            0               1                          -100      42.858\n",
       "    14       0.799994                    0.004835           0           1.25001            0                0.0133097    0.61426                     0.614003            0               1                          -100      25.0009\n",
       "    15       0.899992                    0.000150247        0           1.11112            0                0.0016602    0.546009                    0.545966            0               1                          -100      11.1121\n",
       "    16       1                           2.08273e-14        0           1                  0                2.45856e-05  0.491404                    0.491367            0               1                          -100      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10554356915632593\n",
      "RMSE: 0.32487469762406235\n",
      "LogLoss: 0.4145621810448015\n",
      "Mean Per-Class Error: 0.13164261994604687\n",
      "AUC: 0.9263248466748235\n",
      "pr_auc: 0.807093431927192\n",
      "Gini: 0.852649693349647\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3900935615484531: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10803.0</td>\n",
       "<td>1942.0</td>\n",
       "<td>0.1524</td>\n",
       "<td> (1942.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1378.0</td>\n",
       "<td>10912.0</td>\n",
       "<td>0.1121</td>\n",
       "<td> (1378.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12181.0</td>\n",
       "<td>12854.0</td>\n",
       "<td>0.1326</td>\n",
       "<td> (3320.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10803  1942   0.1524   (1942.0/12745.0)\n",
       "1      1378   10912  0.1121   (1378.0/12290.0)\n",
       "Total  12181  12854  0.1326   (3320.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3900936</td>\n",
       "<td>0.8679605</td>\n",
       "<td>231.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1225307</td>\n",
       "<td>0.8990684</td>\n",
       "<td>319.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7757764</td>\n",
       "<td>0.8759377</td>\n",
       "<td>122.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4374397</td>\n",
       "<td>0.8681446</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999217</td>\n",
       "<td>0.9818709</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000365</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999217</td>\n",
       "<td>0.9980384</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4374397</td>\n",
       "<td>0.7366301</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5054668</td>\n",
       "<td>0.8673718</td>\n",
       "<td>199.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4374397</td>\n",
       "<td>0.8683574</td>\n",
       "<td>218.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.390094     0.867961  231\n",
       "max f2                       0.122531     0.899068  319\n",
       "max f0point5                 0.775776     0.875938  122\n",
       "max accuracy                 0.43744      0.868145  218\n",
       "max precision                0.999922     0.981871  0\n",
       "max recall                   3.65082e-05  1         399\n",
       "max specificity              0.999922     0.998038  0\n",
       "max absolute_mcc             0.43744      0.73663   218\n",
       "max min_per_class_accuracy   0.505467     0.867372  199\n",
       "max mean_per_class_accuracy  0.43744      0.868357  218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.27 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100659</td>\n",
       "<td>0.9999952</td>\n",
       "<td>2.0208551</td>\n",
       "<td>2.0208551</td>\n",
       "<td>0.9920635</td>\n",
       "<td>0.9999983</td>\n",
       "<td>0.9920635</td>\n",
       "<td>0.9999983</td>\n",
       "<td>0.0203417</td>\n",
       "<td>0.0203417</td>\n",
       "<td>102.0855128</td>\n",
       "<td>102.0855128</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200519</td>\n",
       "<td>0.9999802</td>\n",
       "<td>1.9881334</td>\n",
       "<td>2.0045595</td>\n",
       "<td>0.976</td>\n",
       "<td>0.9999887</td>\n",
       "<td>0.9840637</td>\n",
       "<td>0.9999936</td>\n",
       "<td>0.0198535</td>\n",
       "<td>0.0401953</td>\n",
       "<td>98.8133442</td>\n",
       "<td>100.4559468</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9999479</td>\n",
       "<td>2.0044296</td>\n",
       "<td>2.0045163</td>\n",
       "<td>0.984</td>\n",
       "<td>0.9999663</td>\n",
       "<td>0.9840426</td>\n",
       "<td>0.9999845</td>\n",
       "<td>0.0200163</td>\n",
       "<td>0.0602116</td>\n",
       "<td>100.4429618</td>\n",
       "<td>100.4516299</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400639</td>\n",
       "<td>0.9998963</td>\n",
       "<td>2.0207907</td>\n",
       "<td>2.0085890</td>\n",
       "<td>0.9920319</td>\n",
       "<td>0.9999240</td>\n",
       "<td>0.9860419</td>\n",
       "<td>0.9999694</td>\n",
       "<td>0.0202604</td>\n",
       "<td>0.0804719</td>\n",
       "<td>102.0790718</td>\n",
       "<td>100.8588961</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9998151</td>\n",
       "<td>1.9797563</td>\n",
       "<td>2.0028547</td>\n",
       "<td>0.9718876</td>\n",
       "<td>0.9998577</td>\n",
       "<td>0.9832268</td>\n",
       "<td>0.9999472</td>\n",
       "<td>0.0196908</td>\n",
       "<td>0.1001627</td>\n",
       "<td>97.9756291</td>\n",
       "<td>100.2854668</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9987264</td>\n",
       "<td>1.9377741</td>\n",
       "<td>1.9703144</td>\n",
       "<td>0.9512780</td>\n",
       "<td>0.9993835</td>\n",
       "<td>0.9672524</td>\n",
       "<td>0.9996653</td>\n",
       "<td>0.0969081</td>\n",
       "<td>0.1970708</td>\n",
       "<td>93.7774094</td>\n",
       "<td>97.0314381</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9957498</td>\n",
       "<td>1.8970987</td>\n",
       "<td>1.9459092</td>\n",
       "<td>0.9313099</td>\n",
       "<td>0.9974001</td>\n",
       "<td>0.9552716</td>\n",
       "<td>0.9989103</td>\n",
       "<td>0.0948739</td>\n",
       "<td>0.2919447</td>\n",
       "<td>89.7098735</td>\n",
       "<td>94.5909165</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9892189</td>\n",
       "<td>1.8644206</td>\n",
       "<td>1.9255492</td>\n",
       "<td>0.9152678</td>\n",
       "<td>0.9928363</td>\n",
       "<td>0.9452766</td>\n",
       "<td>0.9973927</td>\n",
       "<td>0.0931652</td>\n",
       "<td>0.3851098</td>\n",
       "<td>86.4420587</td>\n",
       "<td>92.5549227</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.9572834</td>\n",
       "<td>1.8173750</td>\n",
       "<td>1.8894864</td>\n",
       "<td>0.8921725</td>\n",
       "<td>0.9758763</td>\n",
       "<td>0.9275729</td>\n",
       "<td>0.9902196</td>\n",
       "<td>0.1817738</td>\n",
       "<td>0.5668836</td>\n",
       "<td>81.7375032</td>\n",
       "<td>88.9486361</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.8631974</td>\n",
       "<td>1.7334626</td>\n",
       "<td>1.8504882</td>\n",
       "<td>0.8509788</td>\n",
       "<td>0.9194916</td>\n",
       "<td>0.9084282</td>\n",
       "<td>0.9725411</td>\n",
       "<td>0.1733116</td>\n",
       "<td>0.7401953</td>\n",
       "<td>73.3462563</td>\n",
       "<td>85.0488202</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4648776</td>\n",
       "<td>1.3447274</td>\n",
       "<td>1.7493199</td>\n",
       "<td>0.6601438</td>\n",
       "<td>0.7075938</td>\n",
       "<td>0.8587634</td>\n",
       "<td>0.9195432</td>\n",
       "<td>0.1344996</td>\n",
       "<td>0.8746949</td>\n",
       "<td>34.4727362</td>\n",
       "<td>74.9319873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.1158433</td>\n",
       "<td>0.6404859</td>\n",
       "<td>1.5645511</td>\n",
       "<td>0.3144227</td>\n",
       "<td>0.2452152</td>\n",
       "<td>0.7680581</td>\n",
       "<td>0.8071776</td>\n",
       "<td>0.0640358</td>\n",
       "<td>0.9387307</td>\n",
       "<td>-35.9514067</td>\n",
       "<td>56.4551126</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.0325093</td>\n",
       "<td>0.2832136</td>\n",
       "<td>1.3815342</td>\n",
       "<td>0.1390332</td>\n",
       "<td>0.0664130</td>\n",
       "<td>0.6782127</td>\n",
       "<td>0.7013722</td>\n",
       "<td>0.0283157</td>\n",
       "<td>0.9670464</td>\n",
       "<td>-71.6786398</td>\n",
       "<td>38.1534245</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0052859</td>\n",
       "<td>0.1879202</td>\n",
       "<td>1.2323027</td>\n",
       "<td>0.0922524</td>\n",
       "<td>0.0162064</td>\n",
       "<td>0.6049531</td>\n",
       "<td>0.6157094</td>\n",
       "<td>0.0187958</td>\n",
       "<td>0.9858421</td>\n",
       "<td>-81.2079842</td>\n",
       "<td>23.2302685</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0001610</td>\n",
       "<td>0.1196333</td>\n",
       "<td>1.1086947</td>\n",
       "<td>0.0587295</td>\n",
       "<td>0.0017609</td>\n",
       "<td>0.5442723</td>\n",
       "<td>0.5475050</td>\n",
       "<td>0.0119609</td>\n",
       "<td>0.9978031</td>\n",
       "<td>-88.0366668</td>\n",
       "<td>10.8694705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0219647</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0107827</td>\n",
       "<td>0.0000263</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4927462</td>\n",
       "<td>0.0021969</td>\n",
       "<td>1.0</td>\n",
       "<td>-97.8035306</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0100659                   0.999995           2.02086    2.02086            0.992063         0.999998     0.992063                    0.999998            0.0203417       0.0203417                  102.086   102.086\n",
       "    2        0.0200519                   0.99998            1.98813    2.00456            0.976            0.999989     0.984064                    0.999994            0.0198535       0.0401953                  98.8133   100.456\n",
       "    3        0.0300379                   0.999948           2.00443    2.00452            0.984            0.999966     0.984043                    0.999984            0.0200163       0.0602116                  100.443   100.452\n",
       "    4        0.0400639                   0.999896           2.02079    2.00859            0.992032         0.999924     0.986042                    0.999969            0.0202604       0.0804719                  102.079   100.859\n",
       "    5        0.05001                     0.999815           1.97976    2.00285            0.971888         0.999858     0.983227                    0.999947            0.0196908       0.100163                   97.9756   100.285\n",
       "    6        0.10002                     0.998726           1.93777    1.97031            0.951278         0.999384     0.967252                    0.999665            0.0969081       0.197071                   93.7774   97.0314\n",
       "    7        0.15003                     0.99575            1.8971     1.94591            0.93131          0.9974       0.955272                    0.99891             0.0948739       0.291945                   89.7099   94.5909\n",
       "    8        0.2                         0.989219           1.86442    1.92555            0.915268         0.992836     0.945277                    0.997393            0.0931652       0.38511                    86.4421   92.5549\n",
       "    9        0.30002                     0.957283           1.81738    1.88949            0.892173         0.975876     0.927573                    0.99022             0.181774        0.566884                   81.7375   88.9486\n",
       "    10       0.4                         0.863197           1.73346    1.85049            0.850979         0.919492     0.908428                    0.972541            0.173312        0.740195                   73.3463   85.0488\n",
       "    11       0.50002                     0.464878           1.34473    1.74932            0.660144         0.707594     0.858763                    0.919543            0.1345          0.874695                   34.4727   74.932\n",
       "    12       0.6                         0.115843           0.640486   1.56455            0.314423         0.245215     0.768058                    0.807178            0.0640358       0.938731                   -35.9514  56.4551\n",
       "    13       0.69998                     0.0325093          0.283214   1.38153            0.139033         0.066413     0.678213                    0.701372            0.0283157       0.967046                   -71.6786  38.1534\n",
       "    14       0.8                         0.0052859          0.18792    1.2323             0.0922524        0.0162064    0.604953                    0.615709            0.0187958       0.985842                   -81.208   23.2303\n",
       "    15       0.89998                     0.000160977        0.119633   1.10869            0.0587295        0.00176087   0.544272                    0.547505            0.0119609       0.997803                   -88.0367  10.8695\n",
       "    16       1                           3.39719e-13        0.0219647  1                  0.0107827        2.62573e-05  0.490913                    0.492746            0.00219691      1                          -97.8035  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:55</td>\n",
       "<td> 0.049 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:39:58</td>\n",
       "<td> 3.151 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3150159</td>\n",
       "<td>0.3148487</td>\n",
       "<td>0.9408471</td>\n",
       "<td>0.9242516</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1407220</td>\n",
       "<td>0.3585353</td>\n",
       "<td>0.4002780</td>\n",
       "<td>0.8996912</td>\n",
       "<td>0.8787204</td>\n",
       "<td>2.0370220</td>\n",
       "<td>0.1886559</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:00</td>\n",
       "<td> 5.416 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.2615170</td>\n",
       "<td>0.2306642</td>\n",
       "<td>0.9737177</td>\n",
       "<td>0.9406127</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0889659</td>\n",
       "<td>0.3466435</td>\n",
       "<td>0.3886960</td>\n",
       "<td>0.9106724</td>\n",
       "<td>0.8700627</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1672459</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:03</td>\n",
       "<td> 7.716 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2180970</td>\n",
       "<td>0.1733894</td>\n",
       "<td>0.9890682</td>\n",
       "<td>0.9374779</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0538627</td>\n",
       "<td>0.3387942</td>\n",
       "<td>0.3875523</td>\n",
       "<td>0.9165116</td>\n",
       "<td>0.8569429</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1554624</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:05</td>\n",
       "<td>10.030 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.1805282</td>\n",
       "<td>0.1313539</td>\n",
       "<td>0.9960342</td>\n",
       "<td>0.9265117</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0308101</td>\n",
       "<td>0.3324925</td>\n",
       "<td>0.3885938</td>\n",
       "<td>0.9206258</td>\n",
       "<td>0.8467708</td>\n",
       "<td>2.0126751</td>\n",
       "<td>0.1447174</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:07</td>\n",
       "<td>12.441 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.1434872</td>\n",
       "<td>0.0959668</td>\n",
       "<td>0.9990380</td>\n",
       "<td>0.9170576</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0141071</td>\n",
       "<td>0.3280537</td>\n",
       "<td>0.3941330</td>\n",
       "<td>0.9230151</td>\n",
       "<td>0.8243194</td>\n",
       "<td>2.0126751</td>\n",
       "<td>0.1386858</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:10</td>\n",
       "<td>14.877 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1184840</td>\n",
       "<td>0.0747543</td>\n",
       "<td>0.9997271</td>\n",
       "<td>0.9076627</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0069587</td>\n",
       "<td>0.3265210</td>\n",
       "<td>0.4023702</td>\n",
       "<td>0.9241414</td>\n",
       "<td>0.8194982</td>\n",
       "<td>2.0207907</td>\n",
       "<td>0.1372479</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:12</td>\n",
       "<td>17.395 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.0957175</td>\n",
       "<td>0.0573764</td>\n",
       "<td>0.9999421</td>\n",
       "<td>0.8992296</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0032048</td>\n",
       "<td>0.3259205</td>\n",
       "<td>0.4116480</td>\n",
       "<td>0.9253844</td>\n",
       "<td>0.7971806</td>\n",
       "<td>2.0129627</td>\n",
       "<td>0.1349710</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:14</td>\n",
       "<td>18.981 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.0866529</td>\n",
       "<td>0.0509733</td>\n",
       "<td>0.9999710</td>\n",
       "<td>0.8620390</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0020267</td>\n",
       "<td>0.3248747</td>\n",
       "<td>0.4145622</td>\n",
       "<td>0.9263248</td>\n",
       "<td>0.8070934</td>\n",
       "<td>2.0208551</td>\n",
       "<td>0.1326143</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:39:55  0.049 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:39:58  3.151 sec   20                 0.315016         0.314849            0.940847        0.924252           2.03499          0.140722                         0.358535           0.400278              0.899691          0.87872              2.03702            0.188656\n",
       "    2020-07-16 22:40:00  5.416 sec   40                 0.261517         0.230664            0.973718        0.940613           2.03499          0.0889659                        0.346643           0.388696              0.910672          0.870063             2.02891            0.167246\n",
       "    2020-07-16 22:40:03  7.716 sec   60                 0.218097         0.173389            0.989068        0.937478           2.03499          0.0538627                        0.338794           0.387552              0.916512          0.856943             2.02079            0.155462\n",
       "    2020-07-16 22:40:05  10.030 sec  80                 0.180528         0.131354            0.996034        0.926512           2.03499          0.0308101                        0.332492           0.388594              0.920626          0.846771             2.01268            0.144717\n",
       "    2020-07-16 22:40:07  12.441 sec  100                0.143487         0.0959668           0.999038        0.917058           2.03499          0.0141071                        0.328054           0.394133              0.923015          0.824319             2.01268            0.138686\n",
       "    2020-07-16 22:40:10  14.877 sec  120                0.118484         0.0747543           0.999727        0.907663           2.03499          0.00695873                       0.326521           0.40237               0.924141          0.819498             2.02079            0.137248\n",
       "    2020-07-16 22:40:12  17.395 sec  140                0.0957175        0.0573764           0.999942        0.89923            2.03499          0.00320481                       0.32592            0.411648              0.925384          0.797181             2.01296            0.134971\n",
       "    2020-07-16 22:40:14  18.981 sec  150                0.0866529        0.0509733           0.999971        0.862039           2.03499          0.00202672                       0.324875           0.414562              0.926325          0.807093             2.02086            0.132614"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>39655.9609375</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3146381</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>7054.8198242</td>\n",
       "<td>0.1779006</td>\n",
       "<td>0.0559743</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>4577.6567383</td>\n",
       "<td>0.1154343</td>\n",
       "<td>0.0363200</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>4072.9721680</td>\n",
       "<td>0.1027077</td>\n",
       "<td>0.0323158</td></tr>\n",
       "<tr><td>e02</td>\n",
       "<td>3952.9724121</td>\n",
       "<td>0.0996817</td>\n",
       "<td>0.0313637</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>b03.Y</td>\n",
       "<td>1.6916449</td>\n",
       "<td>0.0000427</td>\n",
       "<td>0.0000134</td></tr>\n",
       "<tr><td>a16.Z</td>\n",
       "<td>1.5701280</td>\n",
       "<td>0.0000396</td>\n",
       "<td>0.0000125</td></tr>\n",
       "<tr><td>b04.I</td>\n",
       "<td>1.3797621</td>\n",
       "<td>0.0000348</td>\n",
       "<td>0.0000109</td></tr>\n",
       "<tr><td>e13.D</td>\n",
       "<td>1.3656588</td>\n",
       "<td>0.0000344</td>\n",
       "<td>0.0000108</td></tr>\n",
       "<tr><td>a13.F</td>\n",
       "<td>0.2971147</td>\n",
       "<td>0.0000075</td>\n",
       "<td>0.0000024</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         39655.9609375          1.0                     0.3146381011577528\n",
       "e19         7054.81982421875       0.1779006146222894      0.0559743115795543\n",
       "e12         4577.65673828125       0.11543426587230837     0.03632001822260229\n",
       "f02         4072.97216796875       0.10270769063919497     0.03231575275701421\n",
       "e02         3952.972412109375      0.09968167001020299     0.031363651372243985\n",
       "---         ---                    ---                     ---\n",
       "b03.Y       1.6916449069976807     4.265802333383894e-05   1.3421839460902198e-05\n",
       "a16.Z       1.5701279640197754     3.959374396435341e-05   1.2457700418670396e-05\n",
       "b04.I       1.379762053489685      3.4793307761833505e-05  1.0947300287180598e-05\n",
       "e13.D       1.3656587600708008     3.443766656476072e-05   1.0835402016240144e-05\n",
       "a13.F       0.29711470007896423    7.492308673271933e-06   2.3573657742460434e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Model Details\n",
      "=============\n",
      "H2OXGBoostEstimator :  XGBoost\n",
      "Model Key:  XGBoost_model_python_1594936985348_1164\n",
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.005928838938691499\n",
      "RMSE: 0.07699895414024466\n",
      "LogLoss: 0.043234300130669784\n",
      "Mean Per-Class Error: 0.0015582807552998457\n",
      "AUC: 0.999990509559748\n",
      "pr_auc: 0.8665484962033226\n",
      "Gini: 0.999981019119496\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.4929661273956299: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>50865.0</td>\n",
       "<td>77.0</td>\n",
       "<td>0.0015</td>\n",
       "<td> (77.0/50942.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>79.0</td>\n",
       "<td>49141.0</td>\n",
       "<td>0.0016</td>\n",
       "<td> (79.0/49220.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>50944.0</td>\n",
       "<td>49218.0</td>\n",
       "<td>0.0016</td>\n",
       "<td> (156.0/100162.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      50865  77     0.0015   (77.0/50942.0)\n",
       "1      79     49141  0.0016   (79.0/49220.0)\n",
       "Total  50944  49218  0.0016   (156.0/100162.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.4929661</td>\n",
       "<td>0.9984152</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.4183862</td>\n",
       "<td>0.9988711</td>\n",
       "<td>215.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5781933</td>\n",
       "<td>0.9988395</td>\n",
       "<td>192.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5252000</td>\n",
       "<td>0.9984425</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999449</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.2931657</td>\n",
       "<td>1.0</td>\n",
       "<td>237.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999449</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5252000</td>\n",
       "<td>0.9968846</td>\n",
       "<td>198.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.4929661</td>\n",
       "<td>0.9983950</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4929661</td>\n",
       "<td>0.9984417</td>\n",
       "<td>203.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.492966     0.998415  203\n",
       "max f2                       0.418386     0.998871  215\n",
       "max f0point5                 0.578193     0.998839  192\n",
       "max accuracy                 0.5252       0.998443  198\n",
       "max precision                0.999945     1         0\n",
       "max recall                   0.293166     1         237\n",
       "max specificity              0.999945     1         0\n",
       "max absolute_mcc             0.5252       0.996885  198\n",
       "max min_per_class_accuracy   0.492966     0.998395  203\n",
       "max mean_per_class_accuracy  0.492966     0.998442  203"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.14 %, avg score: 49.13 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0100936</td>\n",
       "<td>0.9999981</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999994</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999994</td>\n",
       "<td>0.0205404</td>\n",
       "<td>0.0205404</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200675</td>\n",
       "<td>0.9999917</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999952</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999973</td>\n",
       "<td>0.0202966</td>\n",
       "<td>0.0408371</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300413</td>\n",
       "<td>0.9999785</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999856</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999934</td>\n",
       "<td>0.0202966</td>\n",
       "<td>0.0611337</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400252</td>\n",
       "<td>0.9999554</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999675</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999869</td>\n",
       "<td>0.0203169</td>\n",
       "<td>0.0814506</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500290</td>\n",
       "<td>0.9999186</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999383</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9999772</td>\n",
       "<td>0.0203576</td>\n",
       "<td>0.1018082</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000080</td>\n",
       "<td>0.9993542</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9997049</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9998411</td>\n",
       "<td>0.1017066</td>\n",
       "<td>0.2035148</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500170</td>\n",
       "<td>0.9976158</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9986220</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9994347</td>\n",
       "<td>0.1017676</td>\n",
       "<td>0.3052824</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2000060</td>\n",
       "<td>0.9938052</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9959408</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9985614</td>\n",
       "<td>0.1017269</td>\n",
       "<td>0.4070093</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000040</td>\n",
       "<td>0.9751044</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9858971</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9943401</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.6105039</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4000020</td>\n",
       "<td>0.9281796</td>\n",
       "<td>2.0349858</td>\n",
       "<td>2.0349858</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9553251</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9845866</td>\n",
       "<td>0.2034945</td>\n",
       "<td>0.8139984</td>\n",
       "<td>103.4985778</td>\n",
       "<td>103.4985778</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5</td>\n",
       "<td>0.2606200</td>\n",
       "<td>1.8600534</td>\n",
       "<td>2.0</td>\n",
       "<td>0.9140375</td>\n",
       "<td>0.8080928</td>\n",
       "<td>0.9828079</td>\n",
       "<td>0.9492886</td>\n",
       "<td>0.1860016</td>\n",
       "<td>1.0</td>\n",
       "<td>86.0053394</td>\n",
       "<td>100.0</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.5999980</td>\n",
       "<td>0.0611523</td>\n",
       "<td>0.0</td>\n",
       "<td>1.6666722</td>\n",
       "<td>0.0</td>\n",
       "<td>0.1192591</td>\n",
       "<td>0.8190093</td>\n",
       "<td>0.8109526</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>66.6672213</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999960</td>\n",
       "<td>0.0192921</td>\n",
       "<td>0.0</td>\n",
       "<td>1.4285796</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0372168</td>\n",
       "<td>0.7020096</td>\n",
       "<td>0.7004205</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>42.8579579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.7999940</td>\n",
       "<td>0.0029889</td>\n",
       "<td>0.0</td>\n",
       "<td>1.2500094</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0095842</td>\n",
       "<td>0.6142595</td>\n",
       "<td>0.6140670</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>25.0009360</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999920</td>\n",
       "<td>0.0000654</td>\n",
       "<td>0.0</td>\n",
       "<td>1.1111210</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0009266</td>\n",
       "<td>0.5460092</td>\n",
       "<td>0.5459411</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>11.1120972</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td>\n",
       "<td>0.0000101</td>\n",
       "<td>0.4914039</td>\n",
       "<td>0.4913436</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>-100.0</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift     cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain     cumulative_gain\n",
       "--  -------  --------------------------  -----------------  -------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  -------  -----------------\n",
       "    1        0.0100936                   0.999998           2.03499  2.03499            1                0.999999     1                           0.999999            0.0205404       0.0205404                  103.499  103.499\n",
       "    2        0.0200675                   0.999992           2.03499  2.03499            1                0.999995     1                           0.999997            0.0202966       0.0408371                  103.499  103.499\n",
       "    3        0.0300413                   0.999979           2.03499  2.03499            1                0.999986     1                           0.999993            0.0202966       0.0611337                  103.499  103.499\n",
       "    4        0.0400252                   0.999955           2.03499  2.03499            1                0.999968     1                           0.999987            0.0203169       0.0814506                  103.499  103.499\n",
       "    5        0.050029                    0.999919           2.03499  2.03499            1                0.999938     1                           0.999977            0.0203576       0.101808                   103.499  103.499\n",
       "    6        0.100008                    0.999354           2.03499  2.03499            1                0.999705     1                           0.999841            0.101707        0.203515                   103.499  103.499\n",
       "    7        0.150017                    0.997616           2.03499  2.03499            1                0.998622     1                           0.999435            0.101768        0.305282                   103.499  103.499\n",
       "    8        0.200006                    0.993805           2.03499  2.03499            1                0.995941     1                           0.998561            0.101727        0.407009                   103.499  103.499\n",
       "    9        0.300004                    0.975104           2.03499  2.03499            1                0.985897     1                           0.99434             0.203495        0.610504                   103.499  103.499\n",
       "    10       0.400002                    0.92818            2.03499  2.03499            1                0.955325     1                           0.984587            0.203495        0.813998                   103.499  103.499\n",
       "    11       0.5                         0.26062            1.86005  2                  0.914038         0.808093     0.982808                    0.949289            0.186002        1                          86.0053  100\n",
       "    12       0.599998                    0.0611523          0        1.66667            0                0.119259     0.819009                    0.810953            0               1                          -100     66.6672\n",
       "    13       0.699996                    0.0192921          0        1.42858            0                0.0372168    0.70201                     0.700421            0               1                          -100     42.858\n",
       "    14       0.799994                    0.00298894         0        1.25001            0                0.0095842    0.61426                     0.614067            0               1                          -100     25.0009\n",
       "    15       0.899992                    6.53809e-05        0        1.11112            0                0.000926558  0.546009                    0.545941            0               1                          -100     11.1121\n",
       "    16       1                           2.55166e-16        0        1                  0                1.00702e-05  0.491404                    0.491344            0               1                          -100     0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomial: xgboost\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 0.10693768574137508\n",
      "RMSE: 0.327013280680426\n",
      "LogLoss: 0.4404460158544458\n",
      "Mean Per-Class Error: 0.13016345853971667\n",
      "AUC: 0.9247276536914715\n",
      "pr_auc: 0.7800482378156235\n",
      "Gini: 0.849455307382943\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.38322546807202423: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>10846.0</td>\n",
       "<td>1899.0</td>\n",
       "<td>0.149</td>\n",
       "<td> (1899.0/12745.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>1385.0</td>\n",
       "<td>10905.0</td>\n",
       "<td>0.1127</td>\n",
       "<td> (1385.0/12290.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>12231.0</td>\n",
       "<td>12804.0</td>\n",
       "<td>0.1312</td>\n",
       "<td> (3284.0/25035.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0      1      Error    Rate\n",
       "-----  -----  -----  -------  ----------------\n",
       "0      10846  1899   0.149    (1899.0/12745.0)\n",
       "1      1385   10905  0.1127   (1385.0/12290.0)\n",
       "Total  12231  12804  0.1312   (3284.0/25035.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3832255</td>\n",
       "<td>0.8691321</td>\n",
       "<td>232.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1240222</td>\n",
       "<td>0.8971603</td>\n",
       "<td>316.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.7198677</td>\n",
       "<td>0.8737353</td>\n",
       "<td>140.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.4419519</td>\n",
       "<td>0.8696625</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9999406</td>\n",
       "<td>0.9768652</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0000315</td>\n",
       "<td>1.0</td>\n",
       "<td>399.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9999406</td>\n",
       "<td>0.9968615</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.4419519</td>\n",
       "<td>0.7395630</td>\n",
       "<td>218.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.5033703</td>\n",
       "<td>0.8690467</td>\n",
       "<td>203.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.4419519</td>\n",
       "<td>0.8698365</td>\n",
       "<td>218.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.383225     0.869132  232\n",
       "max f2                       0.124022     0.89716   316\n",
       "max f0point5                 0.719868     0.873735  140\n",
       "max accuracy                 0.441952     0.869662  218\n",
       "max precision                0.999941     0.976865  0\n",
       "max recall                   3.14649e-05  1         399\n",
       "max specificity              0.999941     0.996862  0\n",
       "max absolute_mcc             0.441952     0.739563  218\n",
       "max min_per_class_accuracy   0.50337      0.869047  203\n",
       "max mean_per_class_accuracy  0.441952     0.869837  218"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 49.09 %, avg score: 49.18 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0102257</td>\n",
       "<td>0.9999983</td>\n",
       "<td>1.9972364</td>\n",
       "<td>1.9972364</td>\n",
       "<td>0.9804688</td>\n",
       "<td>0.9999995</td>\n",
       "<td>0.9804688</td>\n",
       "<td>0.9999995</td>\n",
       "<td>0.0204231</td>\n",
       "<td>0.0204231</td>\n",
       "<td>99.7236384</td>\n",
       "<td>99.7236384</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0200919</td>\n",
       "<td>0.9999925</td>\n",
       "<td>2.0205279</td>\n",
       "<td>2.0086738</td>\n",
       "<td>0.9919028</td>\n",
       "<td>0.9999958</td>\n",
       "<td>0.9860835</td>\n",
       "<td>0.9999977</td>\n",
       "<td>0.0199349</td>\n",
       "<td>0.0403580</td>\n",
       "<td>102.0527864</td>\n",
       "<td>100.8673751</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0300379</td>\n",
       "<td>0.9999809</td>\n",
       "<td>1.9715755</td>\n",
       "<td>1.9963899</td>\n",
       "<td>0.9678715</td>\n",
       "<td>0.9999875</td>\n",
       "<td>0.9800532</td>\n",
       "<td>0.9999943</td>\n",
       "<td>0.0196094</td>\n",
       "<td>0.0599675</td>\n",
       "<td>97.1575480</td>\n",
       "<td>99.6389882</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0400639</td>\n",
       "<td>0.9999595</td>\n",
       "<td>1.9883282</td>\n",
       "<td>1.9943725</td>\n",
       "<td>0.9760956</td>\n",
       "<td>0.9999717</td>\n",
       "<td>0.9790628</td>\n",
       "<td>0.9999886</td>\n",
       "<td>0.0199349</td>\n",
       "<td>0.0799024</td>\n",
       "<td>98.8328217</td>\n",
       "<td>99.4372456</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0500100</td>\n",
       "<td>0.9999241</td>\n",
       "<td>1.9552139</td>\n",
       "<td>1.9865845</td>\n",
       "<td>0.9598394</td>\n",
       "<td>0.9999431</td>\n",
       "<td>0.9752396</td>\n",
       "<td>0.9999796</td>\n",
       "<td>0.0194467</td>\n",
       "<td>0.0993491</td>\n",
       "<td>95.5213858</td>\n",
       "<td>98.6584524</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1000200</td>\n",
       "<td>0.9992922</td>\n",
       "<td>1.9686874</td>\n",
       "<td>1.9776359</td>\n",
       "<td>0.9664537</td>\n",
       "<td>0.9996920</td>\n",
       "<td>0.9708466</td>\n",
       "<td>0.9998358</td>\n",
       "<td>0.0984540</td>\n",
       "<td>0.1978031</td>\n",
       "<td>96.8687366</td>\n",
       "<td>97.7635945</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1500300</td>\n",
       "<td>0.9972697</td>\n",
       "<td>1.9068608</td>\n",
       "<td>1.9540442</td>\n",
       "<td>0.9361022</td>\n",
       "<td>0.9984325</td>\n",
       "<td>0.9592652</td>\n",
       "<td>0.9993680</td>\n",
       "<td>0.0953621</td>\n",
       "<td>0.2931652</td>\n",
       "<td>90.6860821</td>\n",
       "<td>95.4044237</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2</td>\n",
       "<td>0.9923285</td>\n",
       "<td>1.8497658</td>\n",
       "<td>1.9279902</td>\n",
       "<td>0.9080735</td>\n",
       "<td>0.9951261</td>\n",
       "<td>0.9464749</td>\n",
       "<td>0.9983082</td>\n",
       "<td>0.0924329</td>\n",
       "<td>0.3855980</td>\n",
       "<td>84.9765753</td>\n",
       "<td>92.7990236</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3000200</td>\n",
       "<td>0.9641639</td>\n",
       "<td>1.8149345</td>\n",
       "<td>1.8903000</td>\n",
       "<td>0.8909744</td>\n",
       "<td>0.9810628</td>\n",
       "<td>0.9279723</td>\n",
       "<td>0.9925590</td>\n",
       "<td>0.1815297</td>\n",
       "<td>0.5671277</td>\n",
       "<td>81.4934510</td>\n",
       "<td>89.0299977</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4</td>\n",
       "<td>0.8767043</td>\n",
       "<td>1.7171859</td>\n",
       "<td>1.8470301</td>\n",
       "<td>0.8429884</td>\n",
       "<td>0.9293878</td>\n",
       "<td>0.9067306</td>\n",
       "<td>0.9767693</td>\n",
       "<td>0.1716843</td>\n",
       "<td>0.7388120</td>\n",
       "<td>71.7185919</td>\n",
       "<td>84.7030106</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5000200</td>\n",
       "<td>0.4596129</td>\n",
       "<td>1.3732001</td>\n",
       "<td>1.7522490</td>\n",
       "<td>0.6741214</td>\n",
       "<td>0.7169084</td>\n",
       "<td>0.8602013</td>\n",
       "<td>0.9247888</td>\n",
       "<td>0.1373474</td>\n",
       "<td>0.8761595</td>\n",
       "<td>37.3200113</td>\n",
       "<td>75.2248967</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6</td>\n",
       "<td>0.0957247</td>\n",
       "<td>0.6063050</td>\n",
       "<td>1.5612964</td>\n",
       "<td>0.2976428</td>\n",
       "<td>0.2264442</td>\n",
       "<td>0.7664603</td>\n",
       "<td>0.8084213</td>\n",
       "<td>0.0606184</td>\n",
       "<td>0.9367779</td>\n",
       "<td>-39.3695019</td>\n",
       "<td>56.1296447</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.6999800</td>\n",
       "<td>0.0257117</td>\n",
       "<td>0.2726338</td>\n",
       "<td>1.3772333</td>\n",
       "<td>0.1338394</td>\n",
       "<td>0.0545726</td>\n",
       "<td>0.6761013</td>\n",
       "<td>0.7007471</td>\n",
       "<td>0.0272579</td>\n",
       "<td>0.9640358</td>\n",
       "<td>-72.7366217</td>\n",
       "<td>37.7233297</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8</td>\n",
       "<td>0.0033976</td>\n",
       "<td>0.2050038</td>\n",
       "<td>1.2306753</td>\n",
       "<td>0.1006390</td>\n",
       "<td>0.0120061</td>\n",
       "<td>0.6041542</td>\n",
       "<td>0.6146373</td>\n",
       "<td>0.0205045</td>\n",
       "<td>0.9845403</td>\n",
       "<td>-79.4996192</td>\n",
       "<td>23.0675346</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.8999800</td>\n",
       "<td>0.0000782</td>\n",
       "<td>0.1326546</td>\n",
       "<td>1.1086947</td>\n",
       "<td>0.0651219</td>\n",
       "<td>0.0010525</td>\n",
       "<td>0.5442723</td>\n",
       "<td>0.5464733</td>\n",
       "<td>0.0132628</td>\n",
       "<td>0.9978031</td>\n",
       "<td>-86.7345353</td>\n",
       "<td>10.8694705</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0000000</td>\n",
       "<td>0.0219647</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0107827</td>\n",
       "<td>0.0000119</td>\n",
       "<td>0.4909127</td>\n",
       "<td>0.4918162</td>\n",
       "<td>0.0021969</td>\n",
       "<td>1.0</td>\n",
       "<td>-97.8035306</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score        cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  ---------  -----------------  ---------------  -----------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.0102257                   0.999998           1.99724    1.99724            0.980469         0.999999     0.980469                    0.999999            0.0204231       0.0204231                  99.7236   99.7236\n",
       "    2        0.0200919                   0.999992           2.02053    2.00867            0.991903         0.999996     0.986083                    0.999998            0.0199349       0.040358                   102.053   100.867\n",
       "    3        0.0300379                   0.999981           1.97158    1.99639            0.967871         0.999987     0.980053                    0.999994            0.0196094       0.0599675                  97.1575   99.639\n",
       "    4        0.0400639                   0.999959           1.98833    1.99437            0.976096         0.999972     0.979063                    0.999989            0.0199349       0.0799024                  98.8328   99.4372\n",
       "    5        0.05001                     0.999924           1.95521    1.98658            0.959839         0.999943     0.97524                     0.99998             0.0194467       0.0993491                  95.5214   98.6585\n",
       "    6        0.10002                     0.999292           1.96869    1.97764            0.966454         0.999692     0.970847                    0.999836            0.098454        0.197803                   96.8687   97.7636\n",
       "    7        0.15003                     0.99727            1.90686    1.95404            0.936102         0.998433     0.959265                    0.999368            0.0953621       0.293165                   90.6861   95.4044\n",
       "    8        0.2                         0.992329           1.84977    1.92799            0.908074         0.995126     0.946475                    0.998308            0.0924329       0.385598                   84.9766   92.799\n",
       "    9        0.30002                     0.964164           1.81493    1.8903             0.890974         0.981063     0.927972                    0.992559            0.18153         0.567128                   81.4935   89.03\n",
       "    10       0.4                         0.876704           1.71719    1.84703            0.842988         0.929388     0.906731                    0.976769            0.171684        0.738812                   71.7186   84.703\n",
       "    11       0.50002                     0.459613           1.3732     1.75225            0.674121         0.716908     0.860201                    0.924789            0.137347        0.876159                   37.32     75.2249\n",
       "    12       0.6                         0.0957247          0.606305   1.5613             0.297643         0.226444     0.76646                     0.808421            0.0606184       0.936778                   -39.3695  56.1296\n",
       "    13       0.69998                     0.0257117          0.272634   1.37723            0.133839         0.0545726    0.676101                    0.700747            0.0272579       0.964036                   -72.7366  37.7233\n",
       "    14       0.8                         0.0033976          0.205004   1.23068            0.100639         0.0120061    0.604154                    0.614637            0.0205045       0.98454                    -79.4996  23.0675\n",
       "    15       0.89998                     7.82009e-05        0.132655   1.10869            0.0651219        0.00105245   0.544272                    0.546473            0.0132628       0.997803                   -86.7345  10.8695\n",
       "    16       1                           7.79398e-16        0.0219647  1                  0.0107827        1.18834e-05  0.490913                    0.491816            0.00219691      1                          -97.8035  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_logloss</b></td>\n",
       "<td><b>training_auc</b></td>\n",
       "<td><b>training_pr_auc</b></td>\n",
       "<td><b>training_lift</b></td>\n",
       "<td><b>training_classification_error</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_logloss</b></td>\n",
       "<td><b>validation_auc</b></td>\n",
       "<td><b>validation_pr_auc</b></td>\n",
       "<td><b>validation_lift</b></td>\n",
       "<td><b>validation_classification_error</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:15</td>\n",
       "<td> 0.042 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5085961</td>\n",
       "<td>0.5</td>\n",
       "<td>0.6931472</td>\n",
       "<td>0.5</td>\n",
       "<td>0.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.5090873</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:18</td>\n",
       "<td> 3.164 sec</td>\n",
       "<td>20.0</td>\n",
       "<td>0.3113491</td>\n",
       "<td>0.3071255</td>\n",
       "<td>0.9435017</td>\n",
       "<td>0.9225395</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.1395040</td>\n",
       "<td>0.3610780</td>\n",
       "<td>0.4083327</td>\n",
       "<td>0.8972132</td>\n",
       "<td>0.8720129</td>\n",
       "<td>2.0289063</td>\n",
       "<td>0.1895347</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:20</td>\n",
       "<td> 5.454 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>0.2508421</td>\n",
       "<td>0.2134483</td>\n",
       "<td>0.9776458</td>\n",
       "<td>0.9287550</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0822767</td>\n",
       "<td>0.3475067</td>\n",
       "<td>0.3992927</td>\n",
       "<td>0.9098383</td>\n",
       "<td>0.8522072</td>\n",
       "<td>2.0126751</td>\n",
       "<td>0.1666068</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:22</td>\n",
       "<td> 7.754 sec</td>\n",
       "<td>60.0</td>\n",
       "<td>0.2043778</td>\n",
       "<td>0.1549069</td>\n",
       "<td>0.9918299</td>\n",
       "<td>0.9275936</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0467443</td>\n",
       "<td>0.3392285</td>\n",
       "<td>0.4011364</td>\n",
       "<td>0.9158747</td>\n",
       "<td>0.8349049</td>\n",
       "<td>2.0045595</td>\n",
       "<td>0.1537448</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:25</td>\n",
       "<td>10.079 sec</td>\n",
       "<td>80.0</td>\n",
       "<td>0.1665331</td>\n",
       "<td>0.1149815</td>\n",
       "<td>0.9973054</td>\n",
       "<td>0.9136072</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0249895</td>\n",
       "<td>0.3335321</td>\n",
       "<td>0.4079062</td>\n",
       "<td>0.9190980</td>\n",
       "<td>0.8288153</td>\n",
       "<td>2.0127717</td>\n",
       "<td>0.1449571</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:27</td>\n",
       "<td>12.444 sec</td>\n",
       "<td>100.0</td>\n",
       "<td>0.1346245</td>\n",
       "<td>0.0861352</td>\n",
       "<td>0.9992695</td>\n",
       "<td>0.8881087</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0115613</td>\n",
       "<td>0.3298272</td>\n",
       "<td>0.4143816</td>\n",
       "<td>0.9218178</td>\n",
       "<td>0.7998356</td>\n",
       "<td>2.0127717</td>\n",
       "<td>0.1377671</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:29</td>\n",
       "<td>14.889 sec</td>\n",
       "<td>120.0</td>\n",
       "<td>0.1070012</td>\n",
       "<td>0.0646075</td>\n",
       "<td>0.9998697</td>\n",
       "<td>0.8876956</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0048222</td>\n",
       "<td>0.3280715</td>\n",
       "<td>0.4234127</td>\n",
       "<td>0.9233145</td>\n",
       "<td>0.7943388</td>\n",
       "<td>2.0127717</td>\n",
       "<td>0.1345317</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:32</td>\n",
       "<td>17.391 sec</td>\n",
       "<td>140.0</td>\n",
       "<td>0.0863779</td>\n",
       "<td>0.0496337</td>\n",
       "<td>0.9999733</td>\n",
       "<td>0.8750639</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0021565</td>\n",
       "<td>0.3272384</td>\n",
       "<td>0.4351605</td>\n",
       "<td>0.9241357</td>\n",
       "<td>0.7843560</td>\n",
       "<td>2.0061580</td>\n",
       "<td>0.1333333</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-16 22:40:34</td>\n",
       "<td>18.977 sec</td>\n",
       "<td>150.0</td>\n",
       "<td>0.0769990</td>\n",
       "<td>0.0432343</td>\n",
       "<td>0.9999905</td>\n",
       "<td>0.8665485</td>\n",
       "<td>2.0349858</td>\n",
       "<td>0.0015575</td>\n",
       "<td>0.3270133</td>\n",
       "<td>0.4404460</td>\n",
       "<td>0.9247277</td>\n",
       "<td>0.7800482</td>\n",
       "<td>1.9972364</td>\n",
       "<td>0.1311764</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error    validation_rmse    validation_logloss    validation_auc    validation_pr_auc    validation_lift    validation_classification_error\n",
       "--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------  -----------------  --------------------  ----------------  -------------------  -----------------  ---------------------------------\n",
       "    2020-07-16 22:40:15  0.042 sec   0                  0.5              0.693147            0.5             0                  1                0.508596                         0.5                0.693147              0.5               0                    1                  0.509087\n",
       "    2020-07-16 22:40:18  3.164 sec   20                 0.311349         0.307125            0.943502        0.92254            2.03499          0.139504                         0.361078           0.408333              0.897213          0.872013             2.02891            0.189535\n",
       "    2020-07-16 22:40:20  5.454 sec   40                 0.250842         0.213448            0.977646        0.928755           2.03499          0.0822767                        0.347507           0.399293              0.909838          0.852207             2.01268            0.166607\n",
       "    2020-07-16 22:40:22  7.754 sec   60                 0.204378         0.154907            0.99183         0.927594           2.03499          0.0467443                        0.339229           0.401136              0.915875          0.834905             2.00456            0.153745\n",
       "    2020-07-16 22:40:25  10.079 sec  80                 0.166533         0.114981            0.997305        0.913607           2.03499          0.0249895                        0.333532           0.407906              0.919098          0.828815             2.01277            0.144957\n",
       "    2020-07-16 22:40:27  12.444 sec  100                0.134624         0.0861352           0.99927         0.888109           2.03499          0.0115613                        0.329827           0.414382              0.921818          0.799836             2.01277            0.137767\n",
       "    2020-07-16 22:40:29  14.889 sec  120                0.107001         0.0646075           0.99987         0.887696           2.03499          0.00482219                       0.328072           0.423413              0.923315          0.794339             2.01277            0.134532\n",
       "    2020-07-16 22:40:32  17.391 sec  140                0.0863779        0.0496337           0.999973        0.875064           2.03499          0.00215651                       0.327238           0.43516               0.924136          0.784356             2.00616            0.133333\n",
       "    2020-07-16 22:40:34  18.977 sec  150                0.076999         0.0432343           0.999991        0.866548           2.03499          0.00155748                       0.327013           0.440446              0.924728          0.780048             1.99724            0.131176"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>f10</td>\n",
       "<td>37640.6171875</td>\n",
       "<td>1.0</td>\n",
       "<td>0.3085306</td></tr>\n",
       "<tr><td>e19</td>\n",
       "<td>6567.3510742</td>\n",
       "<td>0.1744751</td>\n",
       "<td>0.0538309</td></tr>\n",
       "<tr><td>e12</td>\n",
       "<td>4663.0898438</td>\n",
       "<td>0.1238845</td>\n",
       "<td>0.0382222</td></tr>\n",
       "<tr><td>e02</td>\n",
       "<td>4032.3986816</td>\n",
       "<td>0.1071289</td>\n",
       "<td>0.0330525</td></tr>\n",
       "<tr><td>f02</td>\n",
       "<td>3929.4072266</td>\n",
       "<td>0.1043927</td>\n",
       "<td>0.0322084</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>e20.AF683</td>\n",
       "<td>1.2517242</td>\n",
       "<td>0.0000333</td>\n",
       "<td>0.0000103</td></tr>\n",
       "<tr><td>b02.Y</td>\n",
       "<td>1.0619624</td>\n",
       "<td>0.0000282</td>\n",
       "<td>0.0000087</td></tr>\n",
       "<tr><td>e03.L</td>\n",
       "<td>0.8355081</td>\n",
       "<td>0.0000222</td>\n",
       "<td>0.0000068</td></tr>\n",
       "<tr><td>b04.I</td>\n",
       "<td>0.6811427</td>\n",
       "<td>0.0000181</td>\n",
       "<td>0.0000056</td></tr>\n",
       "<tr><td>a03.K</td>\n",
       "<td>0.0942010</td>\n",
       "<td>0.0000025</td>\n",
       "<td>0.0000008</td></tr></table></div>"
      ],
      "text/plain": [
       "variable    relative_importance    scaled_importance       percentage\n",
       "----------  ---------------------  ----------------------  ----------------------\n",
       "f10         37640.6171875          1.0                     0.3085305948003575\n",
       "e19         6567.35107421875       0.1744751166407465      0.05383091151503127\n",
       "e12         4663.08984375          0.12388452135419704     0.038222165059968005\n",
       "e02         4032.398681640625      0.10712892037752603     0.03305254952439824\n",
       "f02         3929.4072265625        0.10439274167553791     0.03220835468199378\n",
       "---         ---                    ---                     ---\n",
       "e20.AF683   1.2517242431640625     3.32546152718172e-05    1.0260066229670812e-05\n",
       "b02.Y       1.061962366104126      2.8213202796706293e-05  8.704636240090903e-06\n",
       "e03.L       0.8355081081390381     2.2196982158318605e-05  6.848448108078962e-06\n",
       "b04.I       0.6811427474021912     1.809595055280843e-05   5.583154387535843e-06\n",
       "a03.K       0.09420102834701538    2.5026430325987966e-06  7.721419434206771e-07"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[, , , , , , , , ]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Going through the above models we can identify the one we have previously seen, where learn_rate = 0.3, with AUC on validation data = 0.9328576403707831. Looking at the models before and after, we see that the model with learn_rate = 0.2 has an AUC (val) of 0.9292629697952675 (SMALLER), and the next model with learn_rate = 0.4 has an AUC (val) of 0.932015943966922 (ALSO SMALLER). It then continues to decrease as learn_rate moves away (in either direction) from 0.3. Looking also at AUC on train, I notice that it increases as learn_rate increases, indicating an increasing level of model complexity/overfitting. \n",
    "- It is interesting to note the train AUC for this model, 0.9947704804665793. A value so close to 1 seems like a big red-flag with regards to overfitting. Looking at the other models (as in, with learn_rate set to 0.1 and increasing), the train AUC starts at around 0.95 and converges closer to 1 as learn_rate increases. I am very suspicious of overfitting. I therefore would like to investigate the performance of a few models on data they haven't seen. In particular, I want to look at the top 10 models from each of the grid searches. \n",
    "- Using the test set as the previously unseen dataset is difficult, as I must submit to Kaggle to get the results and only get the AUC. Conveniently, given that the models were trained on a 125k subset of the entire train data, and I can use the remaining 125k examples as a proxy for the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the proxy \"test\" set to h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "### Declare all necessary variables for 125k sample subset of df_train (second half) to be used as proxy for df_test\n",
    "df_test_proxy = df_train.loc[125000:len(df_train)-1, :].copy()\n",
    "\n",
    "### Send data to h2o\n",
    "h2o_df_test_proxy= h2o.H2OFrame(df_test_proxy[vars_ind],\n",
    "                     destination_frame = 'df_test_proxy')\n",
    "#declare the target variable for convenience\n",
    "y_proxy = df_test_proxy[var_dep].values.ravel()\n",
    "\n",
    "# ### Set numerics type to numeric\n",
    "h2o_df_test_proxy[vars_ind_numeric] = h2o_df_test_proxy[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top ten models from the first grid search**\n",
    "- I check their performance over the set they were trained on (train and val, first 125k samples from df_train)\n",
    "- I also check their performance over the proxy test set (last 125k samples from df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_first = [xgboost_grid_performance[i] for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ FIRST GRID SEARCH ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_error</th>\n",
       "      <th>validation_error</th>\n",
       "      <th>test_error</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>validation_AUC</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>overfitting_VAL_vs_TRAIN</th>\n",
       "      <th>overfitting_TEST_vs_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.275601</td>\n",
       "      <td>0.280268</td>\n",
       "      <td>0.289510</td>\n",
       "      <td>0.893928</td>\n",
       "      <td>0.886819</td>\n",
       "      <td>0.871488</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.013909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.263930</td>\n",
       "      <td>0.272216</td>\n",
       "      <td>0.283482</td>\n",
       "      <td>0.906359</td>\n",
       "      <td>0.893465</td>\n",
       "      <td>0.875430</td>\n",
       "      <td>0.008286</td>\n",
       "      <td>0.019552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.246923</td>\n",
       "      <td>0.263778</td>\n",
       "      <td>0.279721</td>\n",
       "      <td>0.910548</td>\n",
       "      <td>0.885928</td>\n",
       "      <td>0.860977</td>\n",
       "      <td>0.016856</td>\n",
       "      <td>0.032798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.239619</td>\n",
       "      <td>0.259703</td>\n",
       "      <td>0.279603</td>\n",
       "      <td>0.928560</td>\n",
       "      <td>0.898802</td>\n",
       "      <td>0.868050</td>\n",
       "      <td>0.020084</td>\n",
       "      <td>0.039984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.232072</td>\n",
       "      <td>0.255981</td>\n",
       "      <td>0.279796</td>\n",
       "      <td>0.940562</td>\n",
       "      <td>0.905651</td>\n",
       "      <td>0.870087</td>\n",
       "      <td>0.023909</td>\n",
       "      <td>0.047724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.217557</td>\n",
       "      <td>0.249148</td>\n",
       "      <td>0.278222</td>\n",
       "      <td>0.948806</td>\n",
       "      <td>0.904524</td>\n",
       "      <td>0.863320</td>\n",
       "      <td>0.031591</td>\n",
       "      <td>0.060665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.210701</td>\n",
       "      <td>0.245295</td>\n",
       "      <td>0.276688</td>\n",
       "      <td>0.951554</td>\n",
       "      <td>0.904568</td>\n",
       "      <td>0.860129</td>\n",
       "      <td>0.034595</td>\n",
       "      <td>0.065987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.205354</td>\n",
       "      <td>0.245444</td>\n",
       "      <td>0.276165</td>\n",
       "      <td>0.943718</td>\n",
       "      <td>0.890216</td>\n",
       "      <td>0.847825</td>\n",
       "      <td>0.040090</td>\n",
       "      <td>0.070811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.191905</td>\n",
       "      <td>0.240681</td>\n",
       "      <td>0.279448</td>\n",
       "      <td>0.943369</td>\n",
       "      <td>0.881474</td>\n",
       "      <td>0.831091</td>\n",
       "      <td>0.048776</td>\n",
       "      <td>0.087543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.049933</td>\n",
       "      <td>0.162920</td>\n",
       "      <td>0.257336</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.927733</td>\n",
       "      <td>0.847843</td>\n",
       "      <td>0.112987</td>\n",
       "      <td>0.207403</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_error  validation_error  test_error  train_AUC  validation_AUC  \\\n",
       "8      0.275601          0.280268    0.289510   0.893928        0.886819   \n",
       "6      0.263930          0.272216    0.283482   0.906359        0.893465   \n",
       "9      0.246923          0.263778    0.279721   0.910548        0.885928   \n",
       "5      0.239619          0.259703    0.279603   0.928560        0.898802   \n",
       "2      0.232072          0.255981    0.279796   0.940562        0.905651   \n",
       "4      0.217557          0.249148    0.278222   0.948806        0.904524   \n",
       "3      0.210701          0.245295    0.276688   0.951554        0.904568   \n",
       "7      0.205354          0.245444    0.276165   0.943718        0.890216   \n",
       "10     0.191905          0.240681    0.279448   0.943369        0.881474   \n",
       "1      0.049933          0.162920    0.257336   0.999969        0.927733   \n",
       "\n",
       "    test_AUC  overfitting_VAL_vs_TRAIN  overfitting_TEST_vs_TRAIN  \n",
       "8   0.871488                  0.004666                   0.013909  \n",
       "6   0.875430                  0.008286                   0.019552  \n",
       "9   0.860977                  0.016856                   0.032798  \n",
       "5   0.868050                  0.020084                   0.039984  \n",
       "2   0.870087                  0.023909                   0.047724  \n",
       "4   0.863320                  0.031591                   0.060665  \n",
       "3   0.860129                  0.034595                   0.065987  \n",
       "7   0.847825                  0.040090                   0.070811  \n",
       "10  0.831091                  0.048776                   0.087543  \n",
       "1   0.847843                  0.112987                   0.207403  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------ FIRST GRID SEARCH ---------------')\n",
    "h2o.no_progress()\n",
    "df_grid1_perf = pd.DataFrame()\n",
    "for idx, model in enumerate(top10_first):\n",
    "    \n",
    "    #use model to predict over 125k train subset, and also over 125k test subset\n",
    "    pred_train = model.predict(h2o_df_train_125k[idx_h2o_train_125k, :])\n",
    "    pred_val   = model.predict(h2o_df_train_125k[idx_h2o_val_125k, :])\n",
    "    pred_test = model.predict(h2o_df_test_proxy)\n",
    "\n",
    "    pred_train = pred_train['p1'].as_data_frame().values.ravel()\n",
    "    pred_val  = pred_val['p1'].as_data_frame().values.ravel()\n",
    "    pred_test = pred_test['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "\n",
    "    train_error = fn_MAE(y_125k[idx_train_], pred_train)\n",
    "    val_error = fn_MAE(y_125k[idx_val_],   pred_val)\n",
    "    test_error = fn_MAE(y_proxy, pred_test)\n",
    "    \n",
    "    #create single row dataframe with this model's specs\n",
    "    df_model = pd.DataFrame({\n",
    "        'train_error': train_error,\n",
    "        'validation_error': val_error,\n",
    "        'test_error': test_error,\n",
    "        'train_AUC': roc_auc_score(y_125k[idx_train_], pred_train),\n",
    "        'validation_AUC': roc_auc_score(y_125k[idx_val_], pred_val),\n",
    "        'test_AUC': roc_auc_score(y_proxy, pred_test),\n",
    "        'overfitting_VAL_vs_TRAIN': (val_error - train_error),\n",
    "        'overfitting_TEST_vs_TRAIN': (test_error - train_error)\n",
    "    }, index = [idx+1]) #set index to 1 so we don't start counting from zero and can refer to models[0] as model 1\n",
    "    \n",
    "    df_grid1_perf = df_grid1_perf.append(df_model)\n",
    "\n",
    "## Easiest to create a dataframe with above values. Can then sort models \n",
    "# sort by overfitting (test) and by test AUC\n",
    "df_grid1_perf.sort_values(by=[df_grid1_perf.columns.values[7], df_grid1_perf.columns.values[5]], axis=0, ascending=True, inplace=True, kind='quicksort')\n",
    "df_grid1_perf\n",
    "#     print('model '+str(idx+1)+' performance:')\n",
    "#     print('train error', train_error)\n",
    "#     print('val error',   val_error)\n",
    "#     print('test error', test_error)\n",
    "\n",
    "#     print('train AUCROC computed via scikitlearn roc_auc_score: ', roc_auc_score(y_125k[idx_train_], pred_train))\n",
    "#     print('validation AUCROC computed via scikitlearn roc_auc_score:', roc_auc_score(y_125k[idx_val_], pred_val))\n",
    "#     print('test AUCROC computed via scikitlearn roc_auc_score:', roc_auc_score(y_proxy, pred_test))\n",
    "#     print('Overfitting metric (difference between VAL and TRAIN error):', (val_error - train_error))\n",
    "#     print('Overfitting metric (difference between TEST and TRAIN error)', (test_error - train_error))\n",
    "#     print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Top three models from the second grid search**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "top10_second = [xgboost2_grid_performance[i] for i in range(0,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------ SECOND GRID SEARCH ---------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_error</th>\n",
       "      <th>validation_error</th>\n",
       "      <th>test_error</th>\n",
       "      <th>train_AUC</th>\n",
       "      <th>validation_AUC</th>\n",
       "      <th>test_AUC</th>\n",
       "      <th>overfitting_VAL_vs_TRAIN</th>\n",
       "      <th>overfitting_TEST_vs_TRAIN</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.263064</td>\n",
       "      <td>0.271579</td>\n",
       "      <td>0.284361</td>\n",
       "      <td>0.905143</td>\n",
       "      <td>0.891406</td>\n",
       "      <td>0.870802</td>\n",
       "      <td>0.008515</td>\n",
       "      <td>0.021297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.261336</td>\n",
       "      <td>0.270766</td>\n",
       "      <td>0.284783</td>\n",
       "      <td>0.909809</td>\n",
       "      <td>0.895196</td>\n",
       "      <td>0.872611</td>\n",
       "      <td>0.009430</td>\n",
       "      <td>0.023447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.251951</td>\n",
       "      <td>0.265322</td>\n",
       "      <td>0.281239</td>\n",
       "      <td>0.915586</td>\n",
       "      <td>0.894720</td>\n",
       "      <td>0.869440</td>\n",
       "      <td>0.013370</td>\n",
       "      <td>0.029288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.250161</td>\n",
       "      <td>0.265553</td>\n",
       "      <td>0.283563</td>\n",
       "      <td>0.918215</td>\n",
       "      <td>0.895051</td>\n",
       "      <td>0.866769</td>\n",
       "      <td>0.015392</td>\n",
       "      <td>0.033401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.233903</td>\n",
       "      <td>0.257289</td>\n",
       "      <td>0.278249</td>\n",
       "      <td>0.931734</td>\n",
       "      <td>0.898090</td>\n",
       "      <td>0.866066</td>\n",
       "      <td>0.023386</td>\n",
       "      <td>0.044346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.236888</td>\n",
       "      <td>0.259687</td>\n",
       "      <td>0.281452</td>\n",
       "      <td>0.930339</td>\n",
       "      <td>0.897712</td>\n",
       "      <td>0.863891</td>\n",
       "      <td>0.022798</td>\n",
       "      <td>0.044564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.209271</td>\n",
       "      <td>0.243609</td>\n",
       "      <td>0.276294</td>\n",
       "      <td>0.954741</td>\n",
       "      <td>0.908454</td>\n",
       "      <td>0.862573</td>\n",
       "      <td>0.034339</td>\n",
       "      <td>0.067023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.139781</td>\n",
       "      <td>0.207446</td>\n",
       "      <td>0.265184</td>\n",
       "      <td>0.994650</td>\n",
       "      <td>0.930426</td>\n",
       "      <td>0.865863</td>\n",
       "      <td>0.067665</td>\n",
       "      <td>0.125403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.139992</td>\n",
       "      <td>0.206689</td>\n",
       "      <td>0.266610</td>\n",
       "      <td>0.994771</td>\n",
       "      <td>0.932868</td>\n",
       "      <td>0.865754</td>\n",
       "      <td>0.066697</td>\n",
       "      <td>0.126618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.134068</td>\n",
       "      <td>0.205258</td>\n",
       "      <td>0.266247</td>\n",
       "      <td>0.994343</td>\n",
       "      <td>0.926048</td>\n",
       "      <td>0.858799</td>\n",
       "      <td>0.071190</td>\n",
       "      <td>0.132179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    train_error  validation_error  test_error  train_AUC  validation_AUC  \\\n",
       "10     0.263064          0.271579    0.284361   0.905143        0.891406   \n",
       "7      0.261336          0.270766    0.284783   0.909809        0.895196   \n",
       "9      0.251951          0.265322    0.281239   0.915586        0.894720   \n",
       "8      0.250161          0.265553    0.283563   0.918215        0.895051   \n",
       "5      0.233903          0.257289    0.278249   0.931734        0.898090   \n",
       "6      0.236888          0.259687    0.281452   0.930339        0.897712   \n",
       "4      0.209271          0.243609    0.276294   0.954741        0.908454   \n",
       "2      0.139781          0.207446    0.265184   0.994650        0.930426   \n",
       "1      0.139992          0.206689    0.266610   0.994771        0.932868   \n",
       "3      0.134068          0.205258    0.266247   0.994343        0.926048   \n",
       "\n",
       "    test_AUC  overfitting_VAL_vs_TRAIN  overfitting_TEST_vs_TRAIN  \n",
       "10  0.870802                  0.008515                   0.021297  \n",
       "7   0.872611                  0.009430                   0.023447  \n",
       "9   0.869440                  0.013370                   0.029288  \n",
       "8   0.866769                  0.015392                   0.033401  \n",
       "5   0.866066                  0.023386                   0.044346  \n",
       "6   0.863891                  0.022798                   0.044564  \n",
       "4   0.862573                  0.034339                   0.067023  \n",
       "2   0.865863                  0.067665                   0.125403  \n",
       "1   0.865754                  0.066697                   0.126618  \n",
       "3   0.858799                  0.071190                   0.132179  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('------------------ SECOND GRID SEARCH ---------------')\n",
    "h2o.no_progress()\n",
    "df_grid2_perf = pd.DataFrame()\n",
    "for idx, model in enumerate(top10_second):\n",
    "\n",
    "    #use model to predict over 125k train subset, and also over 125k test subset\n",
    "    pred_train = model.predict(h2o_df_train_125k[idx_h2o_train_125k, :])\n",
    "    pred_val   = model.predict(h2o_df_train_125k[idx_h2o_val_125k, :])\n",
    "    pred_test = model.predict(h2o_df_test_proxy)\n",
    "\n",
    "    pred_train = pred_train['p1'].as_data_frame().values.ravel()\n",
    "    pred_val  = pred_val['p1'].as_data_frame().values.ravel()\n",
    "    pred_test = pred_test['p1'].as_data_frame().values.ravel()\n",
    "\n",
    "\n",
    "    train_error = fn_MAE(y_125k[idx_train_], pred_train)\n",
    "    val_error = fn_MAE(y_125k[idx_val_],   pred_val)\n",
    "    test_error = fn_MAE(y_proxy, pred_test)\n",
    "    \n",
    "    #create single row dataframe with this model's specs\n",
    "    df_model = pd.DataFrame({\n",
    "        'train_error': train_error,\n",
    "        'validation_error': val_error,\n",
    "        'test_error': test_error,\n",
    "        'train_AUC': roc_auc_score(y_125k[idx_train_], pred_train),\n",
    "        'validation_AUC': roc_auc_score(y_125k[idx_val_], pred_val),\n",
    "        'test_AUC': roc_auc_score(y_proxy, pred_test),\n",
    "        'overfitting_VAL_vs_TRAIN': (val_error - train_error),\n",
    "        'overfitting_TEST_vs_TRAIN': (test_error - train_error)\n",
    "    }, index = [idx+1])\n",
    "    \n",
    "    df_grid2_perf = df_grid2_perf.append(df_model)\n",
    "\n",
    "df_grid2_perf.sort_values(by=[df_grid1_perf.columns.values[7], df_grid1_perf.columns.values[5]], axis=0, ascending=True, inplace=True, kind='quicksort')\n",
    "df_grid2_perf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Hyper parameters of each model\n",
    "############################### FIRST GRID SEARCH ###############################\n",
    "#      col_sample_rate_per_tree learn_rate max_depth min_rows ntrees  sample_rate\n",
    "# 1                        0.95        0.7        10      2.0    150          0.9\n",
    "# 2                        0.85        0.2         8      5.0    150          0.6\n",
    "# 3                        0.55        0.4         8     10.0    150          0.7\n",
    "# 4                         0.8        0.3        10     10.0    100          0.6\n",
    "# 5                         0.5        0.3         6      2.0    200          0.6\n",
    "# 6                        0.65       0.25        10     40.0     50          0.8\n",
    "# 7                        0.75       0.55        10     40.0    150          0.6\n",
    "# 8                        0.75        0.3         6      2.0     50          0.6\n",
    "# 9                        0.75       0.55         6     40.0    150          0.6\n",
    "# 10                       0.55       0.65        10     10.0    100          0.5\n",
    "\n",
    "\n",
    "########################## SECOND GRID SEARCH #######################\n",
    "#      col_sample_rate_per_tree max_depth min_rows ntrees sample_rate  \n",
    "# 1                        0.65        10      2.0    150         0.9   \n",
    "# 2                         0.6        10      5.0    200         0.9   \n",
    "# 3                        0.95        10      5.0    200         0.7   \n",
    "# 4                        0.95         8      5.0    150         0.6   \n",
    "# 5                         0.7        10     40.0    150         0.6   \n",
    "# 6                        0.75         8      5.0    100         0.5   \n",
    "# 7                         0.5         6      2.0    100         0.9   \n",
    "# 8                         0.8        10     10.0     50         0.5   \n",
    "# 9                         0.9         6     10.0    150         0.6   \n",
    "# 10                       0.85         6     10.0    100         0.6   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of hyper-parameter tuning\n",
    "<br>\n",
    "\n",
    "The first thing I notice from the above results is that the supposedly 'top' models according to validation AUC (as sorted by the gridsearch algo) tend to be the most overfitted and therefore the worst performers on unseen data. Sorting the models myself, this time on the difference between test and train error (after having used each to predict on unseen data), I can get a better idea of the true predictive performance of each model. \n",
    "\n",
    "<b> First grid search: </b> <br>\n",
    "Parameter sets for the best three models 8, 6, and 9 (in the same order):\n",
    "- max_depth: 6, 10, 10\n",
    "- min no. of rows: 2, 40, 40\n",
    "- sample rate: 0.6, 0.8, 0.6\n",
    "- column sampling rate: 0.75, 0.65, 0.75\n",
    "- no. of trees: 50, 50, 150\n",
    "- learn rate: 0.3, 0.25, 0.65\n",
    "\n",
    "<b> Second grid search: </b> <br>\n",
    "Parameter sets for the best three models 10, 7, and 9:\n",
    "- max_depth: 6, 6, 6\n",
    "- min no. of rows: 10, 2, 10\n",
    "- sample rate: 0.6, 0.9, 0.6\n",
    "- column sampling rate: 0.85, 0.5, 0.9\n",
    "- no. of trees: 100, 100, 150\n",
    "\n",
    "\n",
    "Looking at the results from the <b>first</b> gridsearch, I notice a drastic change in the level of overfitting between the model 1 and the next two, with a drop from 0.2 (model 1) to 0.04 (model 2) and 0.06 (model 3). The corresponding learn rates for these models are 0.7, 0.2 and 0.4, respectively. Then looking at our sorted models, the 2 best models (8 and 6) have learn rates 0.25 and 0.3, respectively. Clearly, there is a sweet-spot around 0.3. Models taking such step sizes tend to have less over-fitted results (all other params kept constant or at least close to each other). In retrospect, I guess this is why the XGBoost algorithm uses 0.3 as default.  However this makes clear the effect that dynamic optimisation has on hyper-parameter tuning. That is, when keeping the learn rate at its default of 0.3 in the second gridsearch, the model still finds ways to overfit to the data... although not as badly as when values of learn rate are high. For ex, in grid 1, the 2 models with the highest learn rates are also the most overfitted. This does not seem coincidental. An exception to this, though, is model 9 which has a relatively high learn rate of 0.55. Again, this shows how the dynamics of HP tuning change as HP values change. \n",
    "\n",
    "<b>Predictive performance:</b>\n",
    "<br>\n",
    "The models with the highest validation AUC are far from being the best predictors. Note that models with validation AUCs of 0.9 and above have train AUCs of 0.99+ (!!). Best approach seems to be to sort the models according to how much they overfit the data, and then inspect the top few more closely.<br>\n",
    "\n",
    "Looking at the 'best' of the grid1 models, it is unclear which of the two are best, as one has a lower test AUC (0.871488), but is less overfitted (0.013909),  while the other has a (slightly) higher test AUC (0.875430), but is a little more overfitted (0.019552).  Moving forward, I will conserve both of these models' 'optimal' hp values, train two new models on the ENTIRE training set (using these models' hp values). I will then compare their validation vs train error to get an idea of how much they overfit. Could also submit predictions for both, and see which performs best, although knowing that the final Kaggle scores are based on a different subset of the private data, if the difference is negligible, I should pick the one with the smallest amount of overfitting, as it has the best chance to perform better on a different set of data. Will see.\n",
    "\n",
    "Looking at the 'best' of the grid2 models, again, it is unclear which is best, for the same reasons. However both are more overfitted than the top 2 grid1 models, with not much difference in their test AUC, so I will consider only those grom grid 1. That is, models 8 and 6 from grid 1:\n",
    "\n",
    "<b>Model 8, Gridsearch 1</b> <br>\n",
    "Performance:\n",
    "- train error 0.275601\n",
    "- val error 0.280268\n",
    "- test error 0.289510\n",
    "- train AUROC computed via scikitlearn roc_auc_score:  0.893928\n",
    "- validation AUCROC computed via scikitlearn roc_auc_score: 0.886819\n",
    "- test AURCROC computed via scikitlearn roc_auc_score: 0.871488\n",
    "- Overfitting metric (difference between VAL and TRAIN error): 0.004666\n",
    "- Overfitting metric (difference between TEST and TRAIN error) 0.013909\n",
    "\n",
    "<b>Model 6, Gridsearch 1</b> <br>\n",
    "Performance:\n",
    "- train error 0.263930\n",
    "- val error 0.272216\n",
    "- test error 0.283482\n",
    "- train AUROC computed via scikitlearn roc_auc_score:  0.906359\n",
    "- validation AUCROC computed via scikitlearn roc_auc_score: 0.893465\n",
    "- test AURCROC computed via scikitlearn roc_auc_score: 0.871488\n",
    "- Overfitting metric (difference between VAL and TRAIN error): 0.008286\n",
    "- Overfitting metric (difference between TEST and TRAIN error) 0.019552"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get models 8 and 6 from sorted grid 1\n",
    "best_xgbs = [xgboost_grid_performance[7], xgboost_grid_performance[5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for best_xgb in best_xgbs:\n",
    "    ### plot scoring history from best model, accross number of trees used\n",
    "    df_scoring_history = best_xgb.scoring_history()\n",
    "    # df_scoring_history\n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    ax1 = fig.add_subplot(1, 1, 1)\n",
    "    ax1.plot(df_scoring_history['number_of_trees'],\n",
    "             df_scoring_history['training_auc'],\n",
    "             'k-',\n",
    "             label='training')\n",
    "    ax1.plot(df_scoring_history['number_of_trees'],\n",
    "             df_scoring_history['validation_auc'],\n",
    "             'r-',\n",
    "             label='validation')\n",
    "    ax1.set_xlabel('number of trees', fontsize=20)\n",
    "    ax1.set_ylabel('mae', fontsize=20)\n",
    "    _ = ax1.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above chart helps us to understand 1) the <code>ntrees</code> value of 50, and 2) the optimal number of trees if constrained in time/cpu power. \n",
    "- For this both of these models in particular, validation 'auc' seems to to start to remain pretty constant at 50 trees (whereas training auc continues to increase, but this is only an indication of increased overfitting). \n",
    "- In terms of performance vs computation time/power tradeoff, it seems from the above chart that 20 would be an ideal number of trees to build. That is, there are huge performance increases throughout the 0-20 range, after which the increases are relatively minimal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Anyway, let's move on**\n",
    "- I now train my best models (model 8,6, search grid 1) over the entire train data set, and use them to predict over the test set.\n",
    "- Reminder: the previous models were trained on a 125k subsample of df_train, and tested on the remaining 125k samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the entire train and test set to h2o\n",
    "h2o_df_train = h2o.H2OFrame(df_train[vars_ind+var_dep], destination_frame='df_train')\n",
    "h2o_df_test = h2o.H2OFrame(df_test[vars_ind], destination_frame='df_test')\n",
    "\n",
    "### Set target type to enum\n",
    "h2o_df_train[var_dep] = h2o_df_train[var_dep].asfactor()\n",
    "### Set numerics type to numeric\n",
    "h2o_df_train[vars_ind_numeric] = h2o_df_train[vars_ind_numeric].asnumeric()\n",
    "h2o_df_test[vars_ind_numeric] = h2o_df_test[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_model8_params = {\n",
    "          'seed':2020\n",
    "        , 'distribution':'bernoulli'  \n",
    "        , 'categorical_encoding':'AUTO' \n",
    "        , 'keep_cross_validation_models':False\n",
    "        , 'keep_cross_validation_predictions':False\n",
    "        , 'keep_cross_validation_fold_assignment':False\n",
    "        , 'score_tree_interval': 20\n",
    "        , 'tree_method': 'auto'\n",
    "        ### keep best values of tuned hyper-parameters\n",
    "        , 'col_sample_rate_per_tree':0.75\n",
    "        , 'max_depth':6\n",
    "        , 'min_rows': 2.0\n",
    "        , 'ntrees': 50\n",
    "        , 'sample_rate': 0.6\n",
    "        , 'learn_rate': 0.3\n",
    "    }\n",
    "xgboost_model6_params = {\n",
    "          'seed':2020\n",
    "        , 'distribution':'bernoulli'  \n",
    "        , 'categorical_encoding':'AUTO' \n",
    "        , 'keep_cross_validation_models':False\n",
    "        , 'keep_cross_validation_predictions':False\n",
    "        , 'keep_cross_validation_fold_assignment':False\n",
    "        , 'score_tree_interval': 20\n",
    "        , 'tree_method': 'auto'\n",
    "        ### keep best values of tuned hyper-parameters\n",
    "        , 'col_sample_rate_per_tree':0.65\n",
    "        , 'max_depth':10\n",
    "        , 'min_rows': 40\n",
    "        , 'ntrees': 50\n",
    "        , 'sample_rate': 0.8\n",
    "        , 'learn_rate': 0.25\n",
    "    }\n",
    "model8 = H2OXGBoostEstimator(**xgboost_model8_params)\n",
    "model6 = H2OXGBoostEstimator(**xgboost_model6_params)\n",
    "\n",
    "## Train models over entire train set\n",
    "# model8.train(x=features, \n",
    "#                 y='target',\n",
    "#                 training_frame=h2o_df_train[idx_h2o_train, :])\n",
    "# model6.train(x=features, \n",
    "#                 y='target',\n",
    "#                 training_frame=h2o_df_train[idx_h2o_train, :])\n",
    "model8.train(x=features, \n",
    "                y='target',\n",
    "                training_frame=h2o_df_train)\n",
    "model6.train(x=features, \n",
    "                y='target',\n",
    "                training_frame=h2o_df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 8\n",
      "train error 0.2822660460155192\n",
      "train AUROC computed via scikitlearn roc_auc_score:  0.8855883907267241\n"
     ]
    }
   ],
   "source": [
    "#use each model to predict over entire dataset\n",
    "\n",
    "pred_train = model8.predict(h2o_df_train)\n",
    "pred_train = pred_train['p1'].as_data_frame().values.ravel()\n",
    "train_error = fn_MAE(y, pred_train)\n",
    "\n",
    "print('model 8')\n",
    "print('train error', train_error)\n",
    "print('train AUROC computed via scikitlearn roc_auc_score: ', roc_auc_score(y, pred_train))\n",
    "\n",
    "preds = model8.predict(h2o_df_test)\n",
    "df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "df_preds = df_test[['unique_id', 'Predicted']].copy()\n",
    "df_preds.to_csv(dirPOutput + 'part2_preds_xgboost_model8_250k.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 6\n",
      "train error 0.27063677917260864\n",
      "train AUROC computed via scikitlearn roc_auc_score:  0.8986053570058855\n"
     ]
    }
   ],
   "source": [
    "#use each model to predict over entire dataset\n",
    "\n",
    "pred_train = model6.predict(h2o_df_train)\n",
    "pred_train = pred_train['p1'].as_data_frame().values.ravel()\n",
    "train_error = fn_MAE(y, pred_train)\n",
    "\n",
    "print('model 6')\n",
    "print('train error', train_error)\n",
    "print('train AUROC computed via scikitlearn roc_auc_score: ', roc_auc_score(y, pred_train))\n",
    "\n",
    "preds = model6.predict(h2o_df_test)\n",
    "df_test['Predicted'] = np.round(preds[2].as_data_frame(), 5)\n",
    "df_preds = df_test[['unique_id', 'Predicted']].copy()\n",
    "df_preds.to_csv(dirPOutput + 'part2_preds_xgboost_model6_250k.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model 8:\n",
    "# train score: 0.8856\n",
    "# kaggle score: 0.86450\n",
    "\n",
    "# model 6:\n",
    "# train score: 0.89860\n",
    "# kaggle score: 0.86849"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 6 seems to be the best....\n",
    "- One last attempt at trying to improve it - will train it on the entire 1m example dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "####import 1 million example dataset \n",
    "\n",
    "#### Load data via pickle\n",
    "f_name = dirPData + '01_df_1m.pickle'\n",
    "\n",
    "with (open(f_name, \"rb\")) as f:\n",
    "    dict_ = pickle.load(f)\n",
    "\n",
    "df_train_1m = dict_['df_train']\n",
    "df_test_1m  = dict_['df_test']\n",
    "\n",
    "del f_name, dict_\n",
    "\n",
    "# h2o_df_test = h2o.H2OFrame(df_test[vars_ind],\n",
    "#                            destination_frame='df_test')\n",
    "preds = best_xgb.predict(h2o_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the entire train and test set to h2o\n",
    "h2o_df_train_1m = h2o.H2OFrame(df_train_1m[vars_ind+var_dep], destination_frame='df_train_1m')\n",
    "h2o_df_test_1m = h2o.H2OFrame(df_test_1m[vars_ind], destination_frame='df_test_1m')\n",
    "\n",
    "### Set target type to enum\n",
    "h2o_df_train_1m[var_dep] = h2o_df_train_1m[var_dep].asfactor()\n",
    "### Set numerics type to numeric\n",
    "h2o_df_train_1m[vars_ind_numeric] = h2o_df_train_1m[vars_ind_numeric].asnumeric()\n",
    "h2o_df_test_1m[vars_ind_numeric] = h2o_df_test_1m[vars_ind_numeric].asnumeric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "model6_1m = H2OXGBoostEstimator(**xgboost_model6_params)\n",
    "\n",
    "model6_1m.train(x=features, \n",
    "                y='target',\n",
    "                training_frame=h2o_df_train_1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model 6: 1m examples\n",
      "train error 0.36312986580309736\n",
      "train AUROC computed via scikitlearn roc_auc_score:  0.8402897164749876\n"
     ]
    }
   ],
   "source": [
    "#use each model to predict over entire dataset\n",
    "y_1m = df_train_1m[var_dep].values.ravel()\n",
    "pred_train_1m = model6_1m.predict(h2o_df_train_1m)\n",
    "pred_train_1m = pred_train_1m['p1'].as_data_frame().values.ravel()\n",
    "train_error_1m = fn_MAE(y_1m, pred_train_1m)\n",
    "\n",
    "print('model 6: 1m examples')\n",
    "print('train error', train_error_1m)\n",
    "print('train AUROC computed via scikitlearn roc_auc_score: ', roc_auc_score(y_1m, pred_train_1m))\n",
    "\n",
    "preds_1m = model6_1m.predict(h2o_df_test_1m)\n",
    "df_test_1m['Predicted'] = np.round(preds_1m[2].as_data_frame(), 5)\n",
    "df_preds_1m = df_test_1m[['unique_id', 'Predicted']].copy()\n",
    "df_preds_1m.to_csv(dirPOutput + 'part2_preds_xgboost_model6_1m.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Didn't work.. <br>\n",
    "**BEST MODEL: MODEL 6, ON 250K DATASET**\n",
    "- Train AUCROC score of 0.89860\n",
    "- Kaggle AUCROC score of 0.86849"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Projects/AmesHousing/PData/XGBoost_model_python_1594936985348_1228\n"
     ]
    }
   ],
   "source": [
    "### SAVE MODEL\n",
    "best_xgb = model6\n",
    "best_xgb_path = h2o.save_model(model=best_xgb, path=dirPData, force=True)\n",
    "print(best_xgb_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 528,
     "status": "ok",
     "timestamp": 1562674795290,
     "user": {
      "displayName": "K a l o u",
      "photoUrl": "https://lh4.googleusercontent.com/-EYTbYeNdLqk/AAAAAAAAAAI/AAAAAAAAADk/OD6CDp5FiG4/s64/photo.jpg",
      "userId": "10262331298445208932"
     },
     "user_tz": -60
    },
    "id": "HOP3NaYBPwOO",
    "outputId": "24b4d559-3307-4b24-caef-72a637790fe0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_a9dc closed.\n"
     ]
    }
   ],
   "source": [
    "### SHUT DOWN H2O CLUSTER\n",
    "h2o.cluster().shutdown()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3smVMkl0PwND",
    "H8sn8pF4PwNm"
   ],
   "name": "*Decision Tree.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
