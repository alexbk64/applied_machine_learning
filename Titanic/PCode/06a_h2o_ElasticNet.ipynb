{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic: penalised logistic regression (elastic net) with h2o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contents\n",
    " - Start\n",
    " - Import data\n",
    " - logistic regression with h2o\n",
    " \n",
    "Notes\n",
    " - **h2o does not need onehot for hccv's**\n",
    " - This booklet is useful reference.  http://docs.h2o.ai/h2o/latest-stable/h2o-docs/booklets/GLMBooklet.pdf\n",
    " - From within Python you can see some quite detailed examples and all the parameters if you type:\n",
    "    - help(H2OGeneralizedLinearEstimator)\n",
    "    - help(H2OGridSearch)     \n",
    "\n",
    "\n",
    "Sources:\n",
    " - \n",
    "\n",
    "Copyright (C) 2018 Alan Chalk  \n",
    "Please do not distribute or publish without permission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "**packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import h2o\n",
    "from h2o.estimators.glm import H2OGeneralizedLinearEstimator\n",
    "from h2o.grid.grid_search import H2OGridSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Directories and paths**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/Projects/Titanic/PCode\n"
     ]
    }
   ],
   "source": [
    "# Set directories\n",
    "print(os.getcwd())\n",
    "dirRawData = \"../input/\"\n",
    "dirPData =   \"../PData/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_accuracy(actuals, predictions):\n",
    "    return np.mean(actuals == predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('../input/train.csv')\n",
    "df_test  = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deal with missings**\n",
    "\n",
    "Since the booklet (see comments below, says they will be excluded from training otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  5,  17,  19,  26,  28,  29,  31,  32,  36,  42,\n",
       "            ...\n",
       "            832, 837, 839, 846, 849, 859, 863, 868, 878, 888],\n",
       "           dtype='int64', length=177)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.index[df_train['Age'].isna().values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['age_m'] = 0\n",
    "df_test['age_m']  = 0\n",
    "mean_miss = np.mean(df_train['Age'])\n",
    "\n",
    "idx_miss = df_train.index[df_train['Age'].isna().values]\n",
    "df_train.loc[idx_miss, 'age_m'] = 1\n",
    "df_train.loc[idx_miss, 'Age'] = mean_miss\n",
    "del idx_miss\n",
    "\n",
    "idx_miss = df_test.index[df_test['Age'].isna().values]\n",
    "df_test.loc[idx_miss, 'age_m'] = 1\n",
    "df_test.loc[idx_miss, 'Age'] = mean_miss\n",
    "del idx_miss, mean_miss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['fare_m'] = 0\n",
    "df_test['fare_m']  = 0\n",
    "mean_miss = np.mean(df_train['Fare'])\n",
    "\n",
    "idx_miss = df_train.index[df_train['Fare'].isna().values]\n",
    "df_train.loc[idx_miss, 'fare_m'] = 1\n",
    "df_train.loc[idx_miss, 'Fare'] = mean_miss\n",
    "del idx_miss\n",
    "\n",
    "idx_miss = df_test.index[df_test['Fare'].isna().values]\n",
    "df_test.loc[idx_miss, 'fare_m'] = 1\n",
    "df_test.loc[idx_miss, 'Fare'] = mean_miss\n",
    "del idx_miss, mean_miss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "age_m            0\n",
       "fare_m           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age              0\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          327\n",
       "Embarked         0\n",
       "age_m            0\n",
       "fare_m           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.isna().sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define variables we will use**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dep = ['Survived']\n",
    "vars_ind_numeric = ['Age', 'Fare']\n",
    "vars_ind_cat = ['Pclass', 'Sex', 'SibSp', 'Parch', 'Embarked', 'age_m', 'fare_m']\n",
    "vars_ind = vars_ind_cat + vars_ind_numeric\n",
    "\n",
    "# for convenience store dependent variable as y\n",
    "y_train = df_train[var_dep].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalised logistic regression \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments about generalised linear models (of which logistic regression is one example) in H2O:\n",
    " - Read as much of the booklet if as you have time for.  But probably skip the section on GLM Model Families (pages 15 to 38).  And unless you like that sort of stuff, probably skip the section on Solver Selection (pages 43-44).\n",
    " - From the booklet: \n",
    "  - *We strongly recommend avoiding one-hot encoding categorical columns withmany  levels  into  many  binary  columns,  as  this  is  very  inefficient.   This  is especially true for Python users who are used to expanding their categoricalvariables manually for other frameworks.*\n",
    " - From the booklet. \n",
    "  - *The recommended way to find optimal regularization settings on H2O is to do a grid search over a few $\\alpha$ values with an automatic lambda search for each $\\alpha$.*  \n",
    "  - This is exactly what we do below.\n",
    " - To enable k-fold cross validation, simple set nfolds>1. This is quite different from some of the sklearn packages where you typically need to use a different class.\n",
    " - Note the comment in the booklet in the section: Grid Search Over Alpha.  In our code here we search over many values just to demonstrate early stopping (which for some reason does not seem to work).\n",
    " - What about missing values? The booklet says:\n",
    "  - What if the training data contains NA values? The rows with missing response are ignored during model training andvalidation.\n",
    "  - What if the testing data contains NA values? If the missing value handling is set to skip and you are generating predictions, skipped rows will have NA (missing) prediction.\n",
    "  - This is a bit if a puzzle because the booklet also says later on, regarding the parameter \"missing_values_handling\": Handling  of  missing  values.    Either Skip or MeanImputation (default).  When I manually mean imputed values here, my results were very similar to not imputing them - so I think missing_values_handling by default mean imputes missing even in the train data.\n",
    "  - Note: I think we have seen that missing objects (strings) are OK...\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**start h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321 ..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_212\"; OpenJDK Runtime Environment (build 1.8.0_212-8u212-b03-0ubuntu1.18.04.1-b03); OpenJDK 64-Bit Server VM (build 25.212-b03, mixed mode)\n",
      "  Starting server from /opt/conda/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp9boip6tg\n",
      "  JVM stdout: /tmp/tmp9boip6tg/h2o_jovyan_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp9boip6tg/h2o_jovyan_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321 ... successful.\n",
      "Warning: Your H2O cluster version is too old (1 year, 2 months and 6 days)! Please download and install the latest version from http://h2o.ai/download/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>03 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>Etc/UTC</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.24.0.3</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>1 year, 2 months and 6 days !!!</td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_jovyan_9tbumu</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>891 Mb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.7 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ---------------------------------------------------\n",
       "H2O cluster uptime:         03 secs\n",
       "H2O cluster timezone:       Etc/UTC\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.24.0.3\n",
       "H2O cluster version age:    1 year, 2 months and 6 days !!!\n",
       "H2O cluster name:           H2O_from_python_jovyan_9tbumu\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    891 Mb\n",
       "H2O cluster total cores:    1\n",
       "H2O cluster allowed cores:  1\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         Amazon S3, XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.7 final\n",
       "--------------------------  ---------------------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "h2o.init(port=54321)\n",
    "#h2o.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load data into h2o**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "h2o_df_train = h2o.H2OFrame(df_train[vars_ind + var_dep], destination_frame = 'df_train') \n",
    "h2o_df_test  = h2o.H2OFrame(df_test[vars_ind], destination_frame = 'df_test') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(h2o.estimators.glm.H2OGeneralizedLinearEstimator)\n",
    "#help(H2OGridSearch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the hyper parameters to search over\n",
    "# We will (stupidly) search over 100 values for alpha - just so we can demonstrate random \n",
    "# search with early stopping\n",
    "\n",
    "#alpha_opts = [0.0, 0.25, 0.5, 0.75, 0.99]\n",
    "\n",
    "alpha_opts = np.arange(0, 1, 0.01).tolist()\n",
    "\n",
    "hyper_parameters = {\"alpha\":alpha_opts}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm Grid Build progress: |████████████████████████████████████████████████| 100%\n"
     ]
    }
   ],
   "source": [
    "# I've included the criteria dictionary from the help file, just \n",
    "# to demonstrate it\n",
    "criteria = {# The default strategy, \"Cartesian\", covers the entire space of h-p combinations. \n",
    "            \"strategy\": \"RandomDiscrete\", \n",
    "            \"max_runtime_secs\": 600,\n",
    "            \"max_models\": 30,\n",
    "            \"stopping_metric\": \"AUTO\",\n",
    "            \"seed\": 2020}\n",
    "\n",
    "grid = H2OGridSearch(H2OGeneralizedLinearEstimator(family=\"binomial\",\n",
    "                                                   nfolds = 20,\n",
    "                                                   lambda_search=True),\n",
    "                     hyper_params=hyper_parameters,\n",
    "                     grid_id='g1',\n",
    "                     search_criteria=criteria)\n",
    "\n",
    "grid.train(y = \"Survived\",\n",
    "           x = vars_ind, \n",
    "           training_frame = h2o_df_train\n",
    "           )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      alpha    model_ids            accuracy\n",
      "0                    [0.29]  g1_model_28  0.8148148148148149\n",
      "1                    [0.36]  g1_model_12   0.813692480359147\n",
      "2                    [0.22]  g1_model_20  0.8125701459034792\n",
      "3                    [0.73]  g1_model_10  0.8125701459034792\n",
      "4                    [0.58]  g1_model_18  0.7991021324354658\n",
      "5                    [0.31]  g1_model_30  0.7946127946127945\n",
      "6                    [0.16]   g1_model_6  0.7878787878787878\n",
      "7                    [0.18]  g1_model_26  0.7867564534231201\n",
      "8                    [0.27]  g1_model_29  0.7867564534231201\n",
      "9                    [0.12]   g1_model_5  0.7845117845117845\n",
      "10                   [0.05]  g1_model_11  0.7845117845117845\n",
      "11                   [0.84]  g1_model_15  0.7833894500561167\n",
      "12    [0.35000000000000003]  g1_model_21  0.7822671156004489\n",
      "13     [0.5700000000000001]   g1_model_8  0.7822671156004489\n",
      "14                    [0.0]  g1_model_16  0.7811447811447811\n",
      "15                   [0.34]  g1_model_17  0.7811447811447811\n",
      "16                   [0.01]   g1_model_3  0.7811447811447811\n",
      "17                   [0.92]   g1_model_1  0.7800224466891134\n",
      "18                   [0.32]  g1_model_27  0.7800224466891134\n",
      "19                   [0.33]  g1_model_24  0.7800224466891134\n",
      "20                   [0.62]   g1_model_4  0.7744107744107744\n",
      "21                   [0.26]  g1_model_25  0.7732884399551067\n",
      "22                    [0.2]   g1_model_7  0.7732884399551067\n",
      "23                   [0.19]  g1_model_19  0.7732884399551067\n",
      "24                   [0.64]  g1_model_13  0.7732884399551067\n",
      "25                   [0.08]  g1_model_22  0.7721661054994389\n",
      "26                   [0.66]  g1_model_14   0.771043771043771\n",
      "27                   [0.49]   g1_model_2   0.771043771043771\n",
      "28                   [0.98]   g1_model_9  0.7699214365881033\n",
      "29                   [0.77]  g1_model_23  0.7699214365881033\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many models are in the grid - should be as defined in grid search params\n",
    "# I have NO IDEA why there are more!  If you do, please post on moodle!\n",
    "grid = grid.get_grid(sort_by='accuracy', decreasing=True)\n",
    "grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extract the best model from the grid**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bst = grid.models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Print the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Details\n",
      "=============\n",
      "H2OGeneralizedLinearEstimator :  Generalized Linear Modeling\n",
      "Model Key:  g1_model_28\n",
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 0.13924654352250787\n",
      "RMSE: 0.3731575317778108\n",
      "LogLoss: 0.4401070376731717\n",
      "Null degrees of freedom: 890\n",
      "Residual degrees of freedom: 880\n",
      "Null deviance: 1186.6551368246774\n",
      "Residual deviance: 784.270741133592\n",
      "AIC: 806.270741133592\n",
      "AUC: 0.8581019184269112\n",
      "pr_auc: 0.8239562993949332\n",
      "Gini: 0.7162038368538224\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.351969291622021: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>432.0</td>\n",
       "<td>117.0</td>\n",
       "<td>0.2131</td>\n",
       "<td> (117.0/549.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>68.0</td>\n",
       "<td>274.0</td>\n",
       "<td>0.1988</td>\n",
       "<td> (68.0/342.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>500.0</td>\n",
       "<td>391.0</td>\n",
       "<td>0.2076</td>\n",
       "<td> (185.0/891.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      432  117  0.2131   (117.0/549.0)\n",
       "1      68   274  0.1988   (68.0/342.0)\n",
       "Total  500  391  0.2076   (185.0/891.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.3519693</td>\n",
       "<td>0.7476126</td>\n",
       "<td>229.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1451578</td>\n",
       "<td>0.7985574</td>\n",
       "<td>318.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.6177104</td>\n",
       "<td>0.8034380</td>\n",
       "<td>140.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5681341</td>\n",
       "<td>0.8204265</td>\n",
       "<td>159.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9744825</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0625173</td>\n",
       "<td>1.0</td>\n",
       "<td>381.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9744825</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.6177104</td>\n",
       "<td>0.6141915</td>\n",
       "<td>140.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3581662</td>\n",
       "<td>0.7894737</td>\n",
       "<td>226.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.3519693</td>\n",
       "<td>0.7940274</td>\n",
       "<td>229.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.351969     0.747613  229\n",
       "max f2                       0.145158     0.798557  318\n",
       "max f0point5                 0.61771      0.803438  140\n",
       "max accuracy                 0.568134     0.820426  159\n",
       "max precision                0.974482     1         0\n",
       "max recall                   0.0625173    1         381\n",
       "max specificity              0.974482     1         0\n",
       "max absolute_mcc             0.61771      0.614191  140\n",
       "max min_per_class_accuracy   0.358166     0.789474  226\n",
       "max mean_per_class_accuracy  0.351969     0.794027  229"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.38 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101010</td>\n",
       "<td>0.9576187</td>\n",
       "<td>2.3157895</td>\n",
       "<td>2.3157895</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9635225</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9635225</td>\n",
       "<td>0.0233918</td>\n",
       "<td>0.0233918</td>\n",
       "<td>131.5789474</td>\n",
       "<td>131.5789474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0202020</td>\n",
       "<td>0.9489849</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.4605263</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9536262</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.9585743</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.0497076</td>\n",
       "<td>160.5263158</td>\n",
       "<td>146.0526316</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0303030</td>\n",
       "<td>0.9442479</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.5087719</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9464679</td>\n",
       "<td>0.9629630</td>\n",
       "<td>0.9545388</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.0760234</td>\n",
       "<td>160.5263158</td>\n",
       "<td>150.8771930</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0404040</td>\n",
       "<td>0.9355471</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.5328947</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9392302</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.9507117</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.1023392</td>\n",
       "<td>160.5263158</td>\n",
       "<td>153.2894737</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505051</td>\n",
       "<td>0.9242421</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.5473684</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9302744</td>\n",
       "<td>0.9777778</td>\n",
       "<td>0.9466242</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.1286550</td>\n",
       "<td>160.5263158</td>\n",
       "<td>154.7368421</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1010101</td>\n",
       "<td>0.8661479</td>\n",
       "<td>2.4894737</td>\n",
       "<td>2.5184211</td>\n",
       "<td>0.9555556</td>\n",
       "<td>0.8950018</td>\n",
       "<td>0.9666667</td>\n",
       "<td>0.9208130</td>\n",
       "<td>0.1257310</td>\n",
       "<td>0.2543860</td>\n",
       "<td>148.9473684</td>\n",
       "<td>151.8421053</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1503928</td>\n",
       "<td>0.7875765</td>\n",
       "<td>2.5460526</td>\n",
       "<td>2.5274941</td>\n",
       "<td>0.9772727</td>\n",
       "<td>0.8284970</td>\n",
       "<td>0.9701493</td>\n",
       "<td>0.8905003</td>\n",
       "<td>0.1257310</td>\n",
       "<td>0.3801170</td>\n",
       "<td>154.6052632</td>\n",
       "<td>152.7494108</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008979</td>\n",
       "<td>0.7253275</td>\n",
       "<td>2.0263158</td>\n",
       "<td>2.4014996</td>\n",
       "<td>0.7777778</td>\n",
       "<td>0.7601401</td>\n",
       "<td>0.9217877</td>\n",
       "<td>0.8577282</td>\n",
       "<td>0.1023392</td>\n",
       "<td>0.4824561</td>\n",
       "<td>102.6315789</td>\n",
       "<td>140.1499559</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3007856</td>\n",
       "<td>0.5849532</td>\n",
       "<td>1.6685393</td>\n",
       "<td>2.1580911</td>\n",
       "<td>0.6404494</td>\n",
       "<td>0.6461982</td>\n",
       "<td>0.8283582</td>\n",
       "<td>0.7874813</td>\n",
       "<td>0.1666667</td>\n",
       "<td>0.6491228</td>\n",
       "<td>66.8539326</td>\n",
       "<td>115.8091123</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4017957</td>\n",
       "<td>0.4160599</td>\n",
       "<td>1.0421053</td>\n",
       "<td>1.8775360</td>\n",
       "<td>0.4</td>\n",
       "<td>0.4991815</td>\n",
       "<td>0.7206704</td>\n",
       "<td>0.7150037</td>\n",
       "<td>0.1052632</td>\n",
       "<td>0.7543860</td>\n",
       "<td>4.2105263</td>\n",
       "<td>87.7536019</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5005612</td>\n",
       "<td>0.2761488</td>\n",
       "<td>0.7993421</td>\n",
       "<td>1.6647982</td>\n",
       "<td>0.3068182</td>\n",
       "<td>0.3386724</td>\n",
       "<td>0.6390135</td>\n",
       "<td>0.6407500</td>\n",
       "<td>0.0789474</td>\n",
       "<td>0.8333333</td>\n",
       "<td>-20.0657895</td>\n",
       "<td>66.4798206</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6004489</td>\n",
       "<td>0.1677985</td>\n",
       "<td>0.4976345</td>\n",
       "<td>1.4706345</td>\n",
       "<td>0.1910112</td>\n",
       "<td>0.2175295</td>\n",
       "<td>0.5644860</td>\n",
       "<td>0.5703451</td>\n",
       "<td>0.0497076</td>\n",
       "<td>0.8830409</td>\n",
       "<td>-50.2365464</td>\n",
       "<td>47.0634530</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7003367</td>\n",
       "<td>0.1272336</td>\n",
       "<td>0.2927262</td>\n",
       "<td>1.3026316</td>\n",
       "<td>0.1123596</td>\n",
       "<td>0.1424521</td>\n",
       "<td>0.5</td>\n",
       "<td>0.5093155</td>\n",
       "<td>0.0292398</td>\n",
       "<td>0.9122807</td>\n",
       "<td>-70.7273802</td>\n",
       "<td>30.2631579</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8002245</td>\n",
       "<td>0.1059754</td>\n",
       "<td>0.2927262</td>\n",
       "<td>1.1765705</td>\n",
       "<td>0.1123596</td>\n",
       "<td>0.1178705</td>\n",
       "<td>0.4516129</td>\n",
       "<td>0.4604535</td>\n",
       "<td>0.0292398</td>\n",
       "<td>0.9415205</td>\n",
       "<td>-70.7273802</td>\n",
       "<td>17.6570458</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9057239</td>\n",
       "<td>0.0827112</td>\n",
       "<td>0.3880179</td>\n",
       "<td>1.0847192</td>\n",
       "<td>0.1489362</td>\n",
       "<td>0.0907334</td>\n",
       "<td>0.4163569</td>\n",
       "<td>0.4173882</td>\n",
       "<td>0.0409357</td>\n",
       "<td>0.9824561</td>\n",
       "<td>-61.1982083</td>\n",
       "<td>8.4719233</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0081654</td>\n",
       "<td>0.1860902</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0714286</td>\n",
       "<td>0.0615227</td>\n",
       "<td>0.3838384</td>\n",
       "<td>0.3838386</td>\n",
       "<td>0.0175439</td>\n",
       "<td>1.0</td>\n",
       "<td>-81.3909774</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010101                    0.957619           2.31579   2.31579            0.888889         0.963522   0.888889                    0.963522            0.0233918       0.0233918                  131.579   131.579\n",
       "    2        0.020202                    0.948985           2.60526   2.46053            1                0.953626   0.944444                    0.958574            0.0263158       0.0497076                  160.526   146.053\n",
       "    3        0.030303                    0.944248           2.60526   2.50877            1                0.946468   0.962963                    0.954539            0.0263158       0.0760234                  160.526   150.877\n",
       "    4        0.040404                    0.935547           2.60526   2.53289            1                0.93923    0.972222                    0.950712            0.0263158       0.102339                   160.526   153.289\n",
       "    5        0.0505051                   0.924242           2.60526   2.54737            1                0.930274   0.977778                    0.946624            0.0263158       0.128655                   160.526   154.737\n",
       "    6        0.10101                     0.866148           2.48947   2.51842            0.955556         0.895002   0.966667                    0.920813            0.125731        0.254386                   148.947   151.842\n",
       "    7        0.150393                    0.787577           2.54605   2.52749            0.977273         0.828497   0.970149                    0.8905              0.125731        0.380117                   154.605   152.749\n",
       "    8        0.200898                    0.725328           2.02632   2.4015             0.777778         0.76014    0.921788                    0.857728            0.102339        0.482456                   102.632   140.15\n",
       "    9        0.300786                    0.584953           1.66854   2.15809            0.640449         0.646198   0.828358                    0.787481            0.166667        0.649123                   66.8539   115.809\n",
       "    10       0.401796                    0.41606            1.04211   1.87754            0.4              0.499182   0.72067                     0.715004            0.105263        0.754386                   4.21053   87.7536\n",
       "    11       0.500561                    0.276149           0.799342  1.6648             0.306818         0.338672   0.639013                    0.64075             0.0789474       0.833333                   -20.0658  66.4798\n",
       "    12       0.600449                    0.167798           0.497635  1.47063            0.191011         0.217529   0.564486                    0.570345            0.0497076       0.883041                   -50.2365  47.0635\n",
       "    13       0.700337                    0.127234           0.292726  1.30263            0.11236          0.142452   0.5                         0.509315            0.0292398       0.912281                   -70.7274  30.2632\n",
       "    14       0.800224                    0.105975           0.292726  1.17657            0.11236          0.117871   0.451613                    0.460453            0.0292398       0.94152                    -70.7274  17.657\n",
       "    15       0.905724                    0.0827112          0.388018  1.08472            0.148936         0.0907334  0.416357                    0.417388            0.0409357       0.982456                   -61.1982  8.47192\n",
       "    16       1                           0.00816539         0.18609   1                  0.0714286        0.0615227  0.383838                    0.383839            0.0175439       1                          -81.391   0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ModelMetricsBinomialGLM: glm\n",
      "** Reported on cross-validation data. **\n",
      "\n",
      "MSE: 0.14339340936042502\n",
      "RMSE: 0.37867322239686424\n",
      "LogLoss: 0.4512327401195468\n",
      "Null degrees of freedom: 890\n",
      "Residual degrees of freedom: 880\n",
      "Null deviance: 1188.5380025173836\n",
      "Residual deviance: 804.0967428930323\n",
      "AIC: 826.0967428930323\n",
      "AUC: 0.8487388020750114\n",
      "pr_auc: 0.8110099622800865\n",
      "Gini: 0.6974776041500228\n",
      "Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.5598416888138862: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>0</b></td>\n",
       "<td><b>1</b></td>\n",
       "<td><b>Error</b></td>\n",
       "<td><b>Rate</b></td></tr>\n",
       "<tr><td>0</td>\n",
       "<td>494.0</td>\n",
       "<td>55.0</td>\n",
       "<td>0.1002</td>\n",
       "<td> (55.0/549.0)</td></tr>\n",
       "<tr><td>1</td>\n",
       "<td>110.0</td>\n",
       "<td>232.0</td>\n",
       "<td>0.3216</td>\n",
       "<td> (110.0/342.0)</td></tr>\n",
       "<tr><td>Total</td>\n",
       "<td>604.0</td>\n",
       "<td>287.0</td>\n",
       "<td>0.1852</td>\n",
       "<td> (165.0/891.0)</td></tr></table></div>"
      ],
      "text/plain": [
       "       0    1    Error    Rate\n",
       "-----  ---  ---  -------  -------------\n",
       "0      494  55   0.1002   (55.0/549.0)\n",
       "1      110  232  0.3216   (110.0/342.0)\n",
       "Total  604  287  0.1852   (165.0/891.0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Metrics: Maximum metrics at their respective thresholds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>metric</b></td>\n",
       "<td><b>threshold</b></td>\n",
       "<td><b>value</b></td>\n",
       "<td><b>idx</b></td></tr>\n",
       "<tr><td>max f1</td>\n",
       "<td>0.5598417</td>\n",
       "<td>0.7376789</td>\n",
       "<td>155.0</td></tr>\n",
       "<tr><td>max f2</td>\n",
       "<td>0.1479958</td>\n",
       "<td>0.7945135</td>\n",
       "<td>310.0</td></tr>\n",
       "<tr><td>max f0point5</td>\n",
       "<td>0.5964728</td>\n",
       "<td>0.7863188</td>\n",
       "<td>142.0</td></tr>\n",
       "<tr><td>max accuracy</td>\n",
       "<td>0.5964728</td>\n",
       "<td>0.8148148</td>\n",
       "<td>142.0</td></tr>\n",
       "<tr><td>max precision</td>\n",
       "<td>0.9280216</td>\n",
       "<td>0.9772727</td>\n",
       "<td>20.0</td></tr>\n",
       "<tr><td>max recall</td>\n",
       "<td>0.0623785</td>\n",
       "<td>1.0</td>\n",
       "<td>380.0</td></tr>\n",
       "<tr><td>max specificity</td>\n",
       "<td>0.9689140</td>\n",
       "<td>0.9981785</td>\n",
       "<td>0.0</td></tr>\n",
       "<tr><td>max absolute_mcc</td>\n",
       "<td>0.5964728</td>\n",
       "<td>0.6019681</td>\n",
       "<td>142.0</td></tr>\n",
       "<tr><td>max min_per_class_accuracy</td>\n",
       "<td>0.3575581</td>\n",
       "<td>0.7777778</td>\n",
       "<td>221.0</td></tr>\n",
       "<tr><td>max mean_per_class_accuracy</td>\n",
       "<td>0.5598417</td>\n",
       "<td>0.7890902</td>\n",
       "<td>155.0</td></tr></table></div>"
      ],
      "text/plain": [
       "metric                       threshold    value     idx\n",
       "---------------------------  -----------  --------  -----\n",
       "max f1                       0.559842     0.737679  155\n",
       "max f2                       0.147996     0.794513  310\n",
       "max f0point5                 0.596473     0.786319  142\n",
       "max accuracy                 0.596473     0.814815  142\n",
       "max precision                0.928022     0.977273  20\n",
       "max recall                   0.0623785    1         380\n",
       "max specificity              0.968914     0.998179  0\n",
       "max absolute_mcc             0.596473     0.601968  142\n",
       "max min_per_class_accuracy   0.357558     0.777778  221\n",
       "max mean_per_class_accuracy  0.559842     0.78909   155"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gains/Lift Table: Avg response rate: 38.38 %, avg score: 38.45 %\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>group</b></td>\n",
       "<td><b>cumulative_data_fraction</b></td>\n",
       "<td><b>lower_threshold</b></td>\n",
       "<td><b>lift</b></td>\n",
       "<td><b>cumulative_lift</b></td>\n",
       "<td><b>response_rate</b></td>\n",
       "<td><b>score</b></td>\n",
       "<td><b>cumulative_response_rate</b></td>\n",
       "<td><b>cumulative_score</b></td>\n",
       "<td><b>capture_rate</b></td>\n",
       "<td><b>cumulative_capture_rate</b></td>\n",
       "<td><b>gain</b></td>\n",
       "<td><b>cumulative_gain</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>1</td>\n",
       "<td>0.0101010</td>\n",
       "<td>0.9580857</td>\n",
       "<td>2.3157895</td>\n",
       "<td>2.3157895</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9628894</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9628894</td>\n",
       "<td>0.0233918</td>\n",
       "<td>0.0233918</td>\n",
       "<td>131.5789474</td>\n",
       "<td>131.5789474</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2</td>\n",
       "<td>0.0202020</td>\n",
       "<td>0.9506234</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.4605263</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9539471</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.9584182</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.0497076</td>\n",
       "<td>160.5263158</td>\n",
       "<td>146.0526316</td></tr>\n",
       "<tr><td></td>\n",
       "<td>3</td>\n",
       "<td>0.0303030</td>\n",
       "<td>0.9418633</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.5087719</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9464789</td>\n",
       "<td>0.9629630</td>\n",
       "<td>0.9544384</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.0760234</td>\n",
       "<td>160.5263158</td>\n",
       "<td>150.8771930</td></tr>\n",
       "<tr><td></td>\n",
       "<td>4</td>\n",
       "<td>0.0404040</td>\n",
       "<td>0.9341469</td>\n",
       "<td>2.6052632</td>\n",
       "<td>2.5328947</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9383431</td>\n",
       "<td>0.9722222</td>\n",
       "<td>0.9504146</td>\n",
       "<td>0.0263158</td>\n",
       "<td>0.1023392</td>\n",
       "<td>160.5263158</td>\n",
       "<td>153.2894737</td></tr>\n",
       "<tr><td></td>\n",
       "<td>5</td>\n",
       "<td>0.0505051</td>\n",
       "<td>0.9243787</td>\n",
       "<td>2.3157895</td>\n",
       "<td>2.4894737</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9296433</td>\n",
       "<td>0.9555556</td>\n",
       "<td>0.9462603</td>\n",
       "<td>0.0233918</td>\n",
       "<td>0.1257310</td>\n",
       "<td>131.5789474</td>\n",
       "<td>148.9473684</td></tr>\n",
       "<tr><td></td>\n",
       "<td>6</td>\n",
       "<td>0.1010101</td>\n",
       "<td>0.8665491</td>\n",
       "<td>2.5473684</td>\n",
       "<td>2.5184211</td>\n",
       "<td>0.9777778</td>\n",
       "<td>0.8948537</td>\n",
       "<td>0.9666667</td>\n",
       "<td>0.9205570</td>\n",
       "<td>0.1286550</td>\n",
       "<td>0.2543860</td>\n",
       "<td>154.7368421</td>\n",
       "<td>151.8421053</td></tr>\n",
       "<tr><td></td>\n",
       "<td>7</td>\n",
       "<td>0.1503928</td>\n",
       "<td>0.7870585</td>\n",
       "<td>2.4276316</td>\n",
       "<td>2.4886096</td>\n",
       "<td>0.9318182</td>\n",
       "<td>0.8268695</td>\n",
       "<td>0.9552239</td>\n",
       "<td>0.8897940</td>\n",
       "<td>0.1198830</td>\n",
       "<td>0.3742690</td>\n",
       "<td>142.7631579</td>\n",
       "<td>148.8609584</td></tr>\n",
       "<tr><td></td>\n",
       "<td>8</td>\n",
       "<td>0.2008979</td>\n",
       "<td>0.7173924</td>\n",
       "<td>1.9684211</td>\n",
       "<td>2.3578359</td>\n",
       "<td>0.7555556</td>\n",
       "<td>0.7608305</td>\n",
       "<td>0.9050279</td>\n",
       "<td>0.8573730</td>\n",
       "<td>0.0994152</td>\n",
       "<td>0.4736842</td>\n",
       "<td>96.8421053</td>\n",
       "<td>135.7835931</td></tr>\n",
       "<tr><td></td>\n",
       "<td>9</td>\n",
       "<td>0.3007856</td>\n",
       "<td>0.5964234</td>\n",
       "<td>1.7563572</td>\n",
       "<td>2.1580911</td>\n",
       "<td>0.6741573</td>\n",
       "<td>0.6482714</td>\n",
       "<td>0.8283582</td>\n",
       "<td>0.7879325</td>\n",
       "<td>0.1754386</td>\n",
       "<td>0.6491228</td>\n",
       "<td>75.6357185</td>\n",
       "<td>115.8091123</td></tr>\n",
       "<tr><td></td>\n",
       "<td>10</td>\n",
       "<td>0.4006734</td>\n",
       "<td>0.4115753</td>\n",
       "<td>0.9367238</td>\n",
       "<td>1.8536046</td>\n",
       "<td>0.3595506</td>\n",
       "<td>0.5037413</td>\n",
       "<td>0.7114846</td>\n",
       "<td>0.7170837</td>\n",
       "<td>0.0935673</td>\n",
       "<td>0.7426901</td>\n",
       "<td>-6.3276168</td>\n",
       "<td>85.3604600</td></tr>\n",
       "<tr><td></td>\n",
       "<td>11</td>\n",
       "<td>0.5005612</td>\n",
       "<td>0.2739006</td>\n",
       "<td>0.8781786</td>\n",
       "<td>1.6589568</td>\n",
       "<td>0.3370787</td>\n",
       "<td>0.3381768</td>\n",
       "<td>0.6367713</td>\n",
       "<td>0.6414723</td>\n",
       "<td>0.0877193</td>\n",
       "<td>0.8304094</td>\n",
       "<td>-12.1821407</td>\n",
       "<td>65.8956809</td></tr>\n",
       "<tr><td></td>\n",
       "<td>12</td>\n",
       "<td>0.6004489</td>\n",
       "<td>0.1682170</td>\n",
       "<td>0.5269072</td>\n",
       "<td>1.4706345</td>\n",
       "<td>0.2022472</td>\n",
       "<td>0.2185855</td>\n",
       "<td>0.5644860</td>\n",
       "<td>0.5711229</td>\n",
       "<td>0.0526316</td>\n",
       "<td>0.8830409</td>\n",
       "<td>-47.3092844</td>\n",
       "<td>47.0634530</td></tr>\n",
       "<tr><td></td>\n",
       "<td>13</td>\n",
       "<td>0.7003367</td>\n",
       "<td>0.1303240</td>\n",
       "<td>0.2341810</td>\n",
       "<td>1.2942814</td>\n",
       "<td>0.0898876</td>\n",
       "<td>0.1431747</td>\n",
       "<td>0.4967949</td>\n",
       "<td>0.5100854</td>\n",
       "<td>0.0233918</td>\n",
       "<td>0.9064327</td>\n",
       "<td>-76.5819042</td>\n",
       "<td>29.4281377</td></tr>\n",
       "<tr><td></td>\n",
       "<td>14</td>\n",
       "<td>0.8002245</td>\n",
       "<td>0.1071034</td>\n",
       "<td>0.2927262</td>\n",
       "<td>1.1692626</td>\n",
       "<td>0.1123596</td>\n",
       "<td>0.1180645</td>\n",
       "<td>0.4488079</td>\n",
       "<td>0.4611515</td>\n",
       "<td>0.0292398</td>\n",
       "<td>0.9356725</td>\n",
       "<td>-70.7273802</td>\n",
       "<td>16.9262567</td></tr>\n",
       "<tr><td></td>\n",
       "<td>15</td>\n",
       "<td>0.9001122</td>\n",
       "<td>0.0806643</td>\n",
       "<td>0.4098167</td>\n",
       "<td>1.0849849</td>\n",
       "<td>0.1573034</td>\n",
       "<td>0.0926713</td>\n",
       "<td>0.4164589</td>\n",
       "<td>0.4202603</td>\n",
       "<td>0.0409357</td>\n",
       "<td>0.9766082</td>\n",
       "<td>-59.0183323</td>\n",
       "<td>8.4984906</td></tr>\n",
       "<tr><td></td>\n",
       "<td>16</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0062409</td>\n",
       "<td>0.2341810</td>\n",
       "<td>1.0</td>\n",
       "<td>0.0898876</td>\n",
       "<td>0.0620779</td>\n",
       "<td>0.3838384</td>\n",
       "<td>0.3844823</td>\n",
       "<td>0.0233918</td>\n",
       "<td>1.0</td>\n",
       "<td>-76.5819042</td>\n",
       "<td>0.0</td></tr></table></div>"
      ],
      "text/plain": [
       "    group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain\n",
       "--  -------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------\n",
       "    1        0.010101                    0.958086           2.31579   2.31579            0.888889         0.962889   0.888889                    0.962889            0.0233918       0.0233918                  131.579   131.579\n",
       "    2        0.020202                    0.950623           2.60526   2.46053            1                0.953947   0.944444                    0.958418            0.0263158       0.0497076                  160.526   146.053\n",
       "    3        0.030303                    0.941863           2.60526   2.50877            1                0.946479   0.962963                    0.954438            0.0263158       0.0760234                  160.526   150.877\n",
       "    4        0.040404                    0.934147           2.60526   2.53289            1                0.938343   0.972222                    0.950415            0.0263158       0.102339                   160.526   153.289\n",
       "    5        0.0505051                   0.924379           2.31579   2.48947            0.888889         0.929643   0.955556                    0.94626             0.0233918       0.125731                   131.579   148.947\n",
       "    6        0.10101                     0.866549           2.54737   2.51842            0.977778         0.894854   0.966667                    0.920557            0.128655        0.254386                   154.737   151.842\n",
       "    7        0.150393                    0.787059           2.42763   2.48861            0.931818         0.82687    0.955224                    0.889794            0.119883        0.374269                   142.763   148.861\n",
       "    8        0.200898                    0.717392           1.96842   2.35784            0.755556         0.76083    0.905028                    0.857373            0.0994152       0.473684                   96.8421   135.784\n",
       "    9        0.300786                    0.596423           1.75636   2.15809            0.674157         0.648271   0.828358                    0.787933            0.175439        0.649123                   75.6357   115.809\n",
       "    10       0.400673                    0.411575           0.936724  1.8536             0.359551         0.503741   0.711485                    0.717084            0.0935673       0.74269                    -6.32762  85.3605\n",
       "    11       0.500561                    0.273901           0.878179  1.65896            0.337079         0.338177   0.636771                    0.641472            0.0877193       0.830409                   -12.1821  65.8957\n",
       "    12       0.600449                    0.168217           0.526907  1.47063            0.202247         0.218585   0.564486                    0.571123            0.0526316       0.883041                   -47.3093  47.0635\n",
       "    13       0.700337                    0.130324           0.234181  1.29428            0.0898876        0.143175   0.496795                    0.510085            0.0233918       0.906433                   -76.5819  29.4281\n",
       "    14       0.800224                    0.107103           0.292726  1.16926            0.11236          0.118065   0.448808                    0.461152            0.0292398       0.935673                   -70.7274  16.9263\n",
       "    15       0.900112                    0.0806643          0.409817  1.08498            0.157303         0.0926713  0.416459                    0.42026             0.0409357       0.976608                   -59.0183  8.49849\n",
       "    16       1                           0.00624087         0.234181  1                  0.0898876        0.0620779  0.383838                    0.384482            0.0233918       1                          -76.5819  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cross-Validation Metrics Summary: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>mean</b></td>\n",
       "<td><b>sd</b></td>\n",
       "<td><b>cv_1_valid</b></td>\n",
       "<td><b>cv_2_valid</b></td>\n",
       "<td><b>cv_3_valid</b></td>\n",
       "<td><b>cv_4_valid</b></td>\n",
       "<td><b>cv_5_valid</b></td>\n",
       "<td><b>cv_6_valid</b></td>\n",
       "<td><b>cv_7_valid</b></td>\n",
       "<td><b>cv_8_valid</b></td>\n",
       "<td><b>cv_9_valid</b></td>\n",
       "<td><b>cv_10_valid</b></td>\n",
       "<td><b>cv_11_valid</b></td>\n",
       "<td><b>cv_12_valid</b></td>\n",
       "<td><b>cv_13_valid</b></td>\n",
       "<td><b>cv_14_valid</b></td>\n",
       "<td><b>cv_15_valid</b></td>\n",
       "<td><b>cv_16_valid</b></td>\n",
       "<td><b>cv_17_valid</b></td>\n",
       "<td><b>cv_18_valid</b></td>\n",
       "<td><b>cv_19_valid</b></td>\n",
       "<td><b>cv_20_valid</b></td></tr>\n",
       "<tr><td>accuracy</td>\n",
       "<td>0.8351695</td>\n",
       "<td>0.0434734</td>\n",
       "<td>0.8235294</td>\n",
       "<td>0.8076923</td>\n",
       "<td>0.7555556</td>\n",
       "<td>0.7608696</td>\n",
       "<td>0.8</td>\n",
       "<td>0.7179487</td>\n",
       "<td>0.9444444</td>\n",
       "<td>0.902439</td>\n",
       "<td>0.7727272</td>\n",
       "<td>0.9111111</td>\n",
       "<td>0.8461539</td>\n",
       "<td>0.7884616</td>\n",
       "<td>0.8297872</td>\n",
       "<td>0.8461539</td>\n",
       "<td>0.7826087</td>\n",
       "<td>0.8478261</td>\n",
       "<td>0.8478261</td>\n",
       "<td>0.8936170</td>\n",
       "<td>0.8913044</td>\n",
       "<td>0.9333333</td></tr>\n",
       "<tr><td>auc</td>\n",
       "<td>0.8456320</td>\n",
       "<td>0.0431665</td>\n",
       "<td>0.8285714</td>\n",
       "<td>0.8851675</td>\n",
       "<td>0.7874494</td>\n",
       "<td>0.8570076</td>\n",
       "<td>0.75</td>\n",
       "<td>0.8280423</td>\n",
       "<td>0.9581530</td>\n",
       "<td>0.8818182</td>\n",
       "<td>0.7933884</td>\n",
       "<td>0.8739496</td>\n",
       "<td>0.8463902</td>\n",
       "<td>0.8341308</td>\n",
       "<td>0.8481482</td>\n",
       "<td>0.7056451</td>\n",
       "<td>0.80625</td>\n",
       "<td>0.8296146</td>\n",
       "<td>0.9083821</td>\n",
       "<td>0.8511905</td>\n",
       "<td>0.8661258</td>\n",
       "<td>0.9732143</td></tr>\n",
       "<tr><td>err</td>\n",
       "<td>0.1648305</td>\n",
       "<td>0.0434734</td>\n",
       "<td>0.1764706</td>\n",
       "<td>0.1923077</td>\n",
       "<td>0.2444444</td>\n",
       "<td>0.2391304</td>\n",
       "<td>0.2</td>\n",
       "<td>0.2820513</td>\n",
       "<td>0.0555556</td>\n",
       "<td>0.0975610</td>\n",
       "<td>0.2272727</td>\n",
       "<td>0.0888889</td>\n",
       "<td>0.1538462</td>\n",
       "<td>0.2115385</td>\n",
       "<td>0.1702128</td>\n",
       "<td>0.1538462</td>\n",
       "<td>0.2173913</td>\n",
       "<td>0.1521739</td>\n",
       "<td>0.1521739</td>\n",
       "<td>0.1063830</td>\n",
       "<td>0.1086957</td>\n",
       "<td>0.0666667</td></tr>\n",
       "<tr><td>err_count</td>\n",
       "<td>7.35</td>\n",
       "<td>2.0034347</td>\n",
       "<td>6.0</td>\n",
       "<td>10.0</td>\n",
       "<td>11.0</td>\n",
       "<td>11.0</td>\n",
       "<td>8.0</td>\n",
       "<td>11.0</td>\n",
       "<td>3.0</td>\n",
       "<td>4.0</td>\n",
       "<td>10.0</td>\n",
       "<td>4.0</td>\n",
       "<td>8.0</td>\n",
       "<td>11.0</td>\n",
       "<td>8.0</td>\n",
       "<td>6.0</td>\n",
       "<td>10.0</td>\n",
       "<td>7.0</td>\n",
       "<td>7.0</td>\n",
       "<td>5.0</td>\n",
       "<td>5.0</td>\n",
       "<td>2.0</td></tr>\n",
       "<tr><td>f0point5</td>\n",
       "<td>0.7852688</td>\n",
       "<td>0.0620630</td>\n",
       "<td>0.7692308</td>\n",
       "<td>0.7142857</td>\n",
       "<td>0.7009346</td>\n",
       "<td>0.7142857</td>\n",
       "<td>0.7638889</td>\n",
       "<td>0.6716418</td>\n",
       "<td>0.8974359</td>\n",
       "<td>0.8510638</td>\n",
       "<td>0.7627119</td>\n",
       "<td>0.942029</td>\n",
       "<td>0.8247423</td>\n",
       "<td>0.7009346</td>\n",
       "<td>0.8152174</td>\n",
       "<td>0.625</td>\n",
       "<td>0.6818182</td>\n",
       "<td>0.8219178</td>\n",
       "<td>0.7943925</td>\n",
       "<td>0.8333333</td>\n",
       "<td>0.9230769</td>\n",
       "<td>0.8974359</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td>r2</td>\n",
       "<td>0.3826166</td>\n",
       "<td>0.0896052</td>\n",
       "<td>0.3345959</td>\n",
       "<td>0.4462042</td>\n",
       "<td>0.3085620</td>\n",
       "<td>0.3702689</td>\n",
       "<td>0.2289873</td>\n",
       "<td>0.2428366</td>\n",
       "<td>0.5724444</td>\n",
       "<td>0.434025</td>\n",
       "<td>0.2710866</td>\n",
       "<td>0.5052289</td>\n",
       "<td>0.4177876</td>\n",
       "<td>0.3324979</td>\n",
       "<td>0.3779057</td>\n",
       "<td>0.1074321</td>\n",
       "<td>0.2657411</td>\n",
       "<td>0.3670202</td>\n",
       "<td>0.4488615</td>\n",
       "<td>0.4404869</td>\n",
       "<td>0.5108556</td>\n",
       "<td>0.6695035</td></tr>\n",
       "<tr><td>recall</td>\n",
       "<td>0.803178</td>\n",
       "<td>0.0912006</td>\n",
       "<td>0.8571429</td>\n",
       "<td>0.8947368</td>\n",
       "<td>0.7894737</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6875</td>\n",
       "<td>1.0</td>\n",
       "<td>1.0</td>\n",
       "<td>0.7272728</td>\n",
       "<td>0.8181818</td>\n",
       "<td>0.7647059</td>\n",
       "<td>0.7619048</td>\n",
       "<td>0.7894737</td>\n",
       "<td>0.75</td>\n",
       "<td>0.5</td>\n",
       "<td>0.75</td>\n",
       "<td>0.7058824</td>\n",
       "<td>0.8947368</td>\n",
       "<td>0.6666667</td>\n",
       "<td>0.7058824</td>\n",
       "<td>1.0</td></tr>\n",
       "<tr><td>residual_deviance</td>\n",
       "<td>39.761208</td>\n",
       "<td>5.3400207</td>\n",
       "<td>34.07286</td>\n",
       "<td>41.485016</td>\n",
       "<td>47.299812</td>\n",
       "<td>43.416603</td>\n",
       "<td>44.748272</td>\n",
       "<td>42.96615</td>\n",
       "<td>36.162697</td>\n",
       "<td>29.799288</td>\n",
       "<td>49.231136</td>\n",
       "<td>35.582245</td>\n",
       "<td>46.442528</td>\n",
       "<td>49.505062</td>\n",
       "<td>45.397144</td>\n",
       "<td>36.153057</td>\n",
       "<td>45.91638</td>\n",
       "<td>42.603706</td>\n",
       "<td>37.61356</td>\n",
       "<td>33.7803</td>\n",
       "<td>35.453953</td>\n",
       "<td>17.594355</td></tr>\n",
       "<tr><td>rmse</td>\n",
       "<td>0.3749187</td>\n",
       "<td>0.0280139</td>\n",
       "<td>0.4014605</td>\n",
       "<td>0.3583482</td>\n",
       "<td>0.4107027</td>\n",
       "<td>0.3964028</td>\n",
       "<td>0.4301663</td>\n",
       "<td>0.4337865</td>\n",
       "<td>0.3187639</td>\n",
       "<td>0.3333279</td>\n",
       "<td>0.4268821</td>\n",
       "<td>0.3410304</td>\n",
       "<td>0.3743931</td>\n",
       "<td>0.3934203</td>\n",
       "<td>0.3899663</td>\n",
       "<td>0.3814889</td>\n",
       "<td>0.4081195</td>\n",
       "<td>0.3840259</td>\n",
       "<td>0.3655369</td>\n",
       "<td>0.3261609</td>\n",
       "<td>0.3375858</td>\n",
       "<td>0.2868046</td></tr>\n",
       "<tr><td>specificity</td>\n",
       "<td>0.8345994</td>\n",
       "<td>0.0967334</td>\n",
       "<td>0.8</td>\n",
       "<td>0.7575757</td>\n",
       "<td>0.7307692</td>\n",
       "<td>0.5416667</td>\n",
       "<td>0.875</td>\n",
       "<td>0.4761905</td>\n",
       "<td>0.9090909</td>\n",
       "<td>0.9666666</td>\n",
       "<td>0.7272728</td>\n",
       "<td>1.0</td>\n",
       "<td>0.9032258</td>\n",
       "<td>0.7878788</td>\n",
       "<td>0.8888889</td>\n",
       "<td>0.9354839</td>\n",
       "<td>0.8</td>\n",
       "<td>0.9310345</td>\n",
       "<td>0.8148148</td>\n",
       "<td>0.9714286</td>\n",
       "<td>1.0</td>\n",
       "<td>0.875</td></tr></table></div>"
      ],
      "text/plain": [
       "                   mean        sd           cv_1_valid    cv_2_valid    cv_3_valid    cv_4_valid    cv_5_valid    cv_6_valid    cv_7_valid    cv_8_valid    cv_9_valid    cv_10_valid    cv_11_valid    cv_12_valid    cv_13_valid    cv_14_valid    cv_15_valid    cv_16_valid    cv_17_valid    cv_18_valid    cv_19_valid    cv_20_valid\n",
       "-----------------  ----------  -----------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  -------------  -------------  -------------  -------------  -------------  -------------  -------------  -------------  -------------  -------------  -------------\n",
       "accuracy           0.8351695   0.043473445  0.8235294     0.8076923     0.75555557    0.76086956    0.8           0.71794873    0.9444444     0.902439      0.77272725    0.9111111      0.84615386     0.78846157     0.82978725     0.84615386     0.7826087      0.84782606     0.84782606     0.89361703     0.8913044      0.93333334\n",
       "auc                0.84563196  0.043166537  0.82857144    0.8851675     0.7874494     0.85700756    0.75          0.8280423     0.95815295    0.8818182     0.7933884     0.8739496      0.8463902      0.83413076     0.84814817     0.70564514     0.80625        0.8296146      0.90838206     0.85119045     0.86612576     0.97321427\n",
       "err                0.16483052  0.043473445  0.1764706     0.1923077     0.24444444    0.23913044    0.2           0.2820513     0.055555556   0.09756097    0.22727273    0.08888889     0.15384616     0.21153846     0.17021276     0.15384616     0.2173913      0.1521739      0.1521739      0.10638298     0.10869565     0.06666667\n",
       "err_count          7.35        2.0034347    6.0           10.0          11.0          11.0          8.0           11.0          3.0           4.0           10.0          4.0            8.0            11.0           8.0            6.0            10.0           7.0            7.0            5.0            5.0            2.0\n",
       "f0point5           0.78526884  0.062062968  0.7692308     0.71428573    0.7009346     0.71428573    0.7638889     0.67164177    0.8974359     0.85106385    0.7627119     0.942029       0.82474226     0.7009346      0.8152174      0.625          0.6818182      0.82191783     0.7943925      0.8333333      0.9230769      0.8974359\n",
       "---                ---         ---          ---           ---           ---           ---           ---           ---           ---           ---           ---           ---            ---            ---            ---            ---            ---            ---            ---            ---            ---            ---\n",
       "r2                 0.3826166   0.08960521   0.33459595    0.44620416    0.30856204    0.3702689     0.22898726    0.24283665    0.57244444    0.434025      0.2710866     0.5052289      0.41778755     0.33249792     0.3779057      0.10743214     0.26574114     0.36702025     0.44886154     0.44048688     0.51085556     0.6695035\n",
       "recall             0.803178    0.09120062   0.85714287    0.8947368     0.7894737     1.0           0.6875        1.0           1.0           0.72727275    0.8181818     0.7647059      0.7619048      0.7894737      0.75           0.5            0.75           0.7058824      0.8947368      0.6666667      0.7058824      1.0\n",
       "residual_deviance  39.761208   5.3400207    34.07286      41.485016     47.299812     43.416603     44.748272     42.96615      36.162697     29.799288     49.231136     35.582245      46.442528      49.505062      45.397144      36.153057      45.91638       42.603706      37.61356       33.7803        35.453953      17.594355\n",
       "rmse               0.3749187   0.028013917  0.4014605     0.35834825    0.4107027     0.39640278    0.4301663     0.43378654    0.3187639     0.33332795    0.42688212    0.34103042     0.37439314     0.39342028     0.38996634     0.38148886     0.40811953     0.3840259      0.3655369      0.32616094     0.33758584     0.28680465\n",
       "specificity        0.8345994   0.09673338   0.8           0.75757575    0.7307692     0.5416667     0.875         0.47619048    0.90909094    0.96666664    0.72727275    1.0            0.9032258      0.7878788      0.8888889      0.9354839      0.8            0.9310345      0.8148148      0.9714286      1.0            0.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>iteration</b></td>\n",
       "<td><b>lambda</b></td>\n",
       "<td><b>predictors</b></td>\n",
       "<td><b>deviance_train</b></td>\n",
       "<td><b>deviance_test</b></td>\n",
       "<td><b>deviance_xval</b></td>\n",
       "<td><b>deviance_se</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.000 sec</td>\n",
       "<td>1</td>\n",
       "<td>.57E0</td>\n",
       "<td>1</td>\n",
       "<td>1.3318239</td>\n",
       "<td>nan</td>\n",
       "<td>1.3349657</td>\n",
       "<td>0.0153653</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.001 sec</td>\n",
       "<td>3</td>\n",
       "<td>.52E0</td>\n",
       "<td>2</td>\n",
       "<td>1.3239910</td>\n",
       "<td>nan</td>\n",
       "<td>1.3320964</td>\n",
       "<td>0.0153047</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.003 sec</td>\n",
       "<td>5</td>\n",
       "<td>.47E0</td>\n",
       "<td>2</td>\n",
       "<td>1.3163008</td>\n",
       "<td>nan</td>\n",
       "<td>1.3246696</td>\n",
       "<td>0.0153058</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.005 sec</td>\n",
       "<td>7</td>\n",
       "<td>.43E0</td>\n",
       "<td>4</td>\n",
       "<td>1.3079938</td>\n",
       "<td>nan</td>\n",
       "<td>1.3172391</td>\n",
       "<td>0.0153585</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.006 sec</td>\n",
       "<td>9</td>\n",
       "<td>.39E0</td>\n",
       "<td>4</td>\n",
       "<td>1.2874538</td>\n",
       "<td>nan</td>\n",
       "<td>1.3037951</td>\n",
       "<td>0.0154369</td></tr>\n",
       "<tr><td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td>\n",
       "<td>---</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.081 sec</td>\n",
       "<td>93</td>\n",
       "<td>.41E-2</td>\n",
       "<td>10</td>\n",
       "<td>0.8804696</td>\n",
       "<td>nan</td>\n",
       "<td>0.9044317</td>\n",
       "<td>0.0355895</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.083 sec</td>\n",
       "<td>94</td>\n",
       "<td>.37E-2</td>\n",
       "<td>11</td>\n",
       "<td>0.8802141</td>\n",
       "<td>nan</td>\n",
       "<td>0.9044285</td>\n",
       "<td>0.0358043</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.084 sec</td>\n",
       "<td>95</td>\n",
       "<td>.34E-2</td>\n",
       "<td>11</td>\n",
       "<td>0.8799959</td>\n",
       "<td>nan</td>\n",
       "<td>0.9044487</td>\n",
       "<td>0.0360046</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.085 sec</td>\n",
       "<td>96</td>\n",
       "<td>.31E-2</td>\n",
       "<td>11</td>\n",
       "<td>0.8798108</td>\n",
       "<td>nan</td>\n",
       "<td>0.9044898</td>\n",
       "<td>0.0361907</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2020-07-13 13:00:31</td>\n",
       "<td> 0.086 sec</td>\n",
       "<td>97</td>\n",
       "<td>.28E-2</td>\n",
       "<td>11</td>\n",
       "<td>0.8796538</td>\n",
       "<td>nan</td>\n",
       "<td>0.9045456</td>\n",
       "<td>0.0363643</td></tr></table></div>"
      ],
      "text/plain": [
       "     timestamp            duration    iteration    lambda    predictors    deviance_train      deviance_test    deviance_xval       deviance_se\n",
       "---  -------------------  ----------  -----------  --------  ------------  ------------------  ---------------  ------------------  --------------------\n",
       "     2020-07-13 13:00:31  0.000 sec   1            .57E0     1             1.3318239470535393  nan              1.3349656727499988  0.01536531524684296\n",
       "     2020-07-13 13:00:31  0.001 sec   3            .52E0     2             1.323990967269915   nan              1.3320964225773877  0.015304713698923108\n",
       "     2020-07-13 13:00:31  0.003 sec   5            .47E0     2             1.3163008455570704  nan              1.3246696302620218  0.015305776519215022\n",
       "     2020-07-13 13:00:31  0.005 sec   7            .43E0     4             1.3079938181290665  nan              1.3172391186301085  0.015358458947930032\n",
       "     2020-07-13 13:00:31  0.006 sec   9            .39E0     4             1.287453791228754   nan              1.3037950562031333  0.015436880697562529\n",
       "---  ---                  ---         ---          ---       ---           ---                 ---              ---                 ---\n",
       "     2020-07-13 13:00:31  0.081 sec   93           .41E-2    10            0.8804696199606892  nan              0.9044317090100842  0.035589516429662425\n",
       "     2020-07-13 13:00:31  0.083 sec   94           .37E-2    11            0.8802140753463435  nan              0.9044285247480317  0.03580425557969815\n",
       "     2020-07-13 13:00:31  0.084 sec   95           .34E-2    11            0.8799959303981123  nan              0.9044486810408994  0.036004595244727294\n",
       "     2020-07-13 13:00:31  0.085 sec   96           .31E-2    11            0.8798107784322469  nan              0.904489802756778   0.03619066372806028\n",
       "     2020-07-13 13:00:31  0.086 sec   97           .28E-2    11            0.8796537742225069  nan              0.9045456443601451  0.03636432917824771"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "See the whole table with table.as_data_frame()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(bst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**View performance metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE {'train': 0.13924654352250787, 'valid': None}\n",
      "logloss {'train': 0.4401070376731717, 'valid': None}\n",
      "Accuracy {'train': [[0.5681340602318017, 0.8204264870931538]], 'valid': None}\n"
     ]
    }
   ],
   "source": [
    "print(\"MSE\", bst.mse(train=True, valid=True))\n",
    "print(\"logloss\", bst.logloss(train=True, valid=True))\n",
    "print(\"Accuracy\", bst.accuracy(train=True, valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Is performance consistent over the various folds?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "g1_model_28_cv_1  training auc: 0.8598425469131818  validation auc:  0.8285714285714286\n",
      "g1_model_28_cv_2  training auc: 0.8570511435908512  validation auc:  0.8851674641148325\n",
      "g1_model_28_cv_3  training auc: 0.8610333335306548  validation auc:  0.7874493927125505\n",
      "g1_model_28_cv_4  training auc: 0.8575416666666668  validation auc:  0.8570075757575757\n",
      "g1_model_28_cv_5  training auc: 0.8597808939526731  validation auc:  0.75\n",
      "g1_model_28_cv_6  training auc: 0.8601933688739244  validation auc:  0.828042328042328\n",
      "g1_model_28_cv_7  training auc: 0.8518800260812867  validation auc:  0.9581529581529582\n",
      "g1_model_28_cv_8  training auc: 0.8570804882734051  validation auc:  0.8818181818181817\n",
      "g1_model_28_cv_9  training auc: 0.8591051944971537  validation auc:  0.793388429752066\n",
      "g1_model_28_cv_10  training auc: 0.8575697622914514  validation auc:  0.8739495798319327\n",
      "g1_model_28_cv_11  training auc: 0.8593560182345229  validation auc:  0.8463901689708141\n",
      "g1_model_28_cv_12  training auc: 0.8591991264069887  validation auc:  0.8341307814992026\n",
      "g1_model_28_cv_13  training auc: 0.8596148354394231  validation auc:  0.8481481481481481\n",
      "g1_model_28_cv_14  training auc: 0.8608160127621205  validation auc:  0.7056451612903226\n",
      "g1_model_28_cv_15  training auc: 0.8596433679681312  validation auc:  0.80625\n",
      "g1_model_28_cv_16  training auc: 0.8592692307692308  validation auc:  0.8296146044624746\n",
      "g1_model_28_cv_17  training auc: 0.856054944663891  validation auc:  0.9083820662768032\n",
      "g1_model_28_cv_18  training auc: 0.8567739653342766  validation auc:  0.8511904761904763\n",
      "g1_model_28_cv_19  training auc: 0.8567396449704141  validation auc:  0.8661257606490872\n",
      "g1_model_28_cv_20  training auc: 0.8534325950670388  validation auc:  0.9732142857142857\n"
     ]
    }
   ],
   "source": [
    "for model_ in bst.get_xval_models():\n",
    "    print (model_.model_id, \" training auc:\",model_.auc(), \" validation auc: \", model_.auc(valid=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What are the coefficients?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intercept': 0.030754517118264824,\n",
       " 'Embarked.C': 0.0,\n",
       " 'Embarked.Q': 0.004811886641967384,\n",
       " 'Embarked.S': -0.40757358583184583,\n",
       " 'Embarked.nan': 0.0,\n",
       " 'Sex.female': 1.2801001815215087,\n",
       " 'Sex.male': -1.2658869323045447,\n",
       " 'Pclass': -0.8485466848031566,\n",
       " 'SibSp': -0.32337928758898565,\n",
       " 'Parch': -0.06880445938515553,\n",
       " 'age_m': -0.09917867303863424,\n",
       " 'Age': -0.46764608281557757,\n",
       " 'Fare': 0.0992449548569269}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalised coefficients\n",
    "bst_coef = bst.coef_norm()\n",
    "bst_coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Intercept': 3.613019893515755,\n",
       " 'Embarked.C': 0.0,\n",
       " 'Embarked.Q': 0.004811886641967384,\n",
       " 'Embarked.S': -0.40757358583184583,\n",
       " 'Embarked.nan': 0.0,\n",
       " 'Sex.female': 1.2801001815215087,\n",
       " 'Sex.male': -1.2658869323045447,\n",
       " 'Pclass': -1.0149215081379053,\n",
       " 'SibSp': -0.2932497969327619,\n",
       " 'Parch': -0.08535927423205009,\n",
       " 'age_m': -0.24843707627542505,\n",
       " 'Age': -0.03596720005990505,\n",
       " 'Fare': 0.0019971444446189226}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Coefficients for original variables\n",
    "bst_coef = bst.coef()\n",
    "bst_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculate one prediction by hand**\n",
    "\n",
    "Let's try to calculate the prediction for the first train example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId                          1\n",
       "Survived                             0\n",
       "Pclass                               3\n",
       "Name           Braund, Mr. Owen Harris\n",
       "Sex                               male\n",
       "Age                                 22\n",
       "SibSp                                1\n",
       "Parch                                0\n",
       "Ticket                       A/5 21171\n",
       "Fare                              7.25\n",
       "Cabin                              NaN\n",
       "Embarked                             S\n",
       "age_m                                0\n",
       "fare_m                               0\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    " - 'Embarked = \"S\"\n",
    " - 'Sex' = 'male'\n",
    " - 'Pclass': 3\n",
    " - 'SibSp': 1\n",
    " - 'Parch': 0\n",
    " - 'Age': 22\n",
    " - 'Fare': 7.25 \n",
    "\n",
    "So our linear predictor is (something like - depending on what models you are running...):\n",
    " - +3.6977083826221824  (intercept)\n",
    " - -0.36966806191950596 ('Embarked = \"S\")\n",
    " - -1.2936519932945865 ('Sex' = 'male')\n",
    " - -1.0668360883558596 * 3 (Pclass = 3: did it make ANY sense to fit this as a numeric feature???)\n",
    " - -0.3054373949219665 * 1 ('SibSp' = 1)\n",
    " - -0.07687389625171188 * 0 ('Parch' = 0)\n",
    " - -0.0375122337702396 * 22 (Age 22)\n",
    " - +0.0019047574242136308 * 7.25 (Fare 7.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "lpred_example1 = +bst_coef['Intercept'] + \\\n",
    "                 bst_coef['Embarked.S'] + \\\n",
    "                 bst_coef['Sex.male'] + \\\n",
    "                 bst_coef['Pclass'] * 3 + \\\n",
    "                 bst_coef['SibSp'] * 1 + \\\n",
    "                 bst_coef['Parch'] * 0 + \\\n",
    "                 bst_coef['Age'] * 22 + \\\n",
    "                 bst_coef['Fare'] * 7.25\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10199479791877711"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_example1 = np.exp(lpred_example1) / (1 + np.exp(lpred_example1))\n",
    "pred_example1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now find all predictions using the .predict method of the model class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n",
      "glm prediction progress: |████████████████████████████████████████████████| 100%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predict</th>\n",
       "      <th>p0</th>\n",
       "      <th>p1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.898005</td>\n",
       "      <td>0.101995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.086230</td>\n",
       "      <td>0.913770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.372510</td>\n",
       "      <td>0.627490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116647</td>\n",
       "      <td>0.883353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.912774</td>\n",
       "      <td>0.087226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   predict        p0        p1\n",
       "0        0  0.898005  0.101995\n",
       "1        1  0.086230  0.913770\n",
       "2        1  0.372510  0.627490\n",
       "3        1  0.116647  0.883353\n",
       "4        0  0.912774  0.087226"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bst_preds_train = bst.predict(h2o_df_train).as_data_frame()\n",
    "df_bst_preds_test = bst.predict(h2o_df_test).as_data_frame()\n",
    "df_bst_preds_train.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did we get the first prediction right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(df_bst_preds_train.iloc[0]['p1'], 5) == np.round(pred_example1, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Histogram of predictions**\n",
    "\n",
    "Do they lie between zero and one??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff5f5e49978>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_bst_preds_train['p1'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What was the linear predictor before the logistic transform?**\n",
    "\n",
    "I cannot see any parameter to pass to .predict in order to return the linear predictor, so we will calclate ourselves.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7ff5f5f2e160>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = df_bst_preds_train['p1'].values\n",
    "eta = np.log(p / (1 - p))\n",
    "df_bst_preds_train['eta'] = eta\n",
    "df_bst_preds_train['eta'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix a threshold based on train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize_scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: -0.819304152637486\n",
       " message: 'Solution found.'\n",
       "    nfev: 21\n",
       "  status: 0\n",
       " success: True\n",
       "       x: 0.6105957329009364"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fn_accuracy_th(x):\n",
    "    return -fn_accuracy(df_bst_preds_train['p1'].values > x, y_train)\n",
    "\n",
    "res = minimize_scalar(fn_accuracy_th, method='Bounded', bounds = [0, 1])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6105957329009364"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = res['x']\n",
    "th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create submission for Kaggle**\n",
    "\n",
    "One thing I tried was, rather than finding ourselves the threshold that maximises accuracy, we will use the threshold that h2o uses - which is chosen to maximise the F1 score (https://en.wikipedia.org/wiki/F1_score) on the training data.  However this gave a remarkably poor result.  So back to finding the threshold ourselves..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_2 = pd.DataFrame({\n",
    "        \"PassengerId\": df_test[\"PassengerId\"],\n",
    "        \"Survived\": 1 * (df_bst_preds_test['p1'].values > th)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_2.to_csv('../POutput/06a_pd_sub_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2O session _sid_a1ca closed.\n"
     ]
    }
   ],
   "source": [
    "h2o.cluster().shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on which model I ran and how I chose the th, I tend to get Kaggle results between 0.74 and 0.77 on test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
